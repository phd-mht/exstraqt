{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8f1259-7f64-4919-93ab-93357d171d49",
   "metadata": {},
   "source": [
    "### https://arxiv.org/pdf/2402.08593"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5102f290-c8e0-4064-86e8-a89a42e1628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score, recall_score, RocCurveDisplay\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "import settings as s\n",
    "\n",
    "assert s.FILE_SIZE == \"Large\"\n",
    "assert s.HIGH_ILLICIT == False\n",
    "\n",
    "os.environ[\"EXT_DATA_TYPE_FOLDER\"] = s.OUTPUT_POSTFIX.lstrip(\"-\")\n",
    "\n",
    "from common import get_weights, delete_large_vars, MULTI_PROC_STAGING_LOCATION\n",
    "from communities import get_communities_spark\n",
    "from features import (\n",
    "    generate_features_spark, generate_features_udf_wrapper, get_edge_features_udf,\n",
    "    SCHEMA_FEAT_UDF, CURRENCY_RATES\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdfc52ce-1f9d-49d2-81f7-366fcdab6f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_script = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3cbd846-2921-4802-b8b8-38d64092cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    sys.version_info.major,\n",
    "    sys.version_info.minor,\n",
    "    sys.version_info.micro,\n",
    ") != (3, 11, 8):\n",
    "    raise EnvironmentError(\n",
    "        \"Only runs efficiently on Python 3.11.8 (Tested on: Conda 24.1.2 | Apple M3 Pro)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adb8939c-e2fc-4bfb-ae18-ef1eec4eafc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/01 13:39:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "SPARK_CONF = [\n",
    "    (\"spark.driver.memory\", \"32g\"),\n",
    "    (\"spark.worker.memory\", \"32g\"),\n",
    "    (\"spark.driver.maxResultSize\", \"32g\"),\n",
    "    (\"spark.driver.bindAddress\", \"127.0.0.1\"),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"true\"),\n",
    "    (\"spark.network.timeout\", \"600s\"),\n",
    "    (\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "]\n",
    "\n",
    "shutil.rmtree(\"artifacts\", ignore_errors=True)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"testing\")\n",
    "    .config(conf=SparkConf().setAll(SPARK_CONF))\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "320b1fa7-53c8-4dbb-b25f-795aaca78a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PERC = 0.6\n",
    "VALIDATION_PERC = 0.2\n",
    "TEST_PERC = 0.2\n",
    "\n",
    "KEEP_TOP_N = 100\n",
    "\n",
    "assert(sum([TRAIN_PERC, VALIDATION_PERC, TEST_PERC]) == 1)\n",
    "\n",
    "location_main = os.path.join(\"features\", os.environ[\"EXT_DATA_TYPE_FOLDER\"])\n",
    "# shutil.rmtree(location_main, ignore_errors=True)\n",
    "\n",
    "location_communities_leiden = f\"{location_main}{os.sep}communities_leiden.parquet\"\n",
    "\n",
    "location_features_leiden = f\"{location_main}{os.sep}features_leiden.parquet\"\n",
    "location_features_ego = f\"{location_main}{os.sep}features_ego.parquet\"\n",
    "location_features_2_hop = f\"{location_main}{os.sep}features_2_hop.parquet\"\n",
    "location_features_2_hop_out = f\"{location_main}{os.sep}features_2_hop_out.parquet\"\n",
    "location_features_2_hop_in = f\"{location_main}{os.sep}features_2_hop_in.parquet\"\n",
    "location_features_2_hop_combined = f\"{location_main}{os.sep}features_2_hop_combined.parquet\"\n",
    "location_features_source = f\"{location_main}{os.sep}features_source.parquet\"\n",
    "location_features_target = f\"{location_main}{os.sep}features_target.parquet\"\n",
    "\n",
    "location_flow_dispense = f\"{location_main}{os.sep}location_flow_dispense.parquet\"\n",
    "location_flow_passthrough = f\"{location_main}{os.sep}location_flow_passthrough.parquet\"\n",
    "location_flow_sink = f\"{location_main}{os.sep}location_flow_sink.parquet\"\n",
    "\n",
    "location_comm_as_source_features = f\"{location_main}{os.sep}comm_as_source_features.parquet\"\n",
    "location_comm_as_target_features = f\"{location_main}{os.sep}comm_as_target_features.parquet\"\n",
    "location_comm_as_passthrough_features = f\"{location_main}{os.sep}comm_as_passthrough_features.parquet\"\n",
    "location_comm_as_passthrough_features_reverse = f\"{location_main}{os.sep}comm_as_passthrough_features_reverse.parquet\"\n",
    "\n",
    "location_features_node_level = f\"{location_main}{os.sep}features_node_level.parquet\"\n",
    "location_features_edges = f\"{location_main}{os.sep}features_edges.parquet\"\n",
    "\n",
    "location_features_edges_train = f\"{location_main}{os.sep}features_edges_train.parquet\"\n",
    "location_features_edges_valid = f\"{location_main}{os.sep}features_edges_valid.parquet\"\n",
    "location_features_edges_test = f\"{location_main}{os.sep}features_edges_test.parquet\"\n",
    "\n",
    "location_train_trx_features = f\"{location_main}{os.sep}train_trx_features.parquet\"\n",
    "location_valid_trx_features = f\"{location_main}{os.sep}valid_trx_features.parquet\"\n",
    "location_test_trx_features = f\"{location_main}{os.sep}test_trx_features.parquet\"\n",
    "\n",
    "location_train_features = f\"{location_main}{os.sep}train_features.parquet\"\n",
    "location_valid_features = f\"{location_main}{os.sep}valid_features.parquet\"\n",
    "location_test_features = f\"{location_main}{os.sep}test_features.parquet\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(location_main)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82546745-6577-4e38-bf44-1a6114c3a398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "data = spark.read.parquet(s.STAGED_DATA_LOCATION)\n",
    "data = data.withColumn(\"is_laundering\", sf.col(\"is_laundering\").cast(\"boolean\"))\n",
    "data_count_original = data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57b5d7e4-1b66-4e93-b67b-a947d8c8e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "trx_ids_sorted = data.sort(\"timestamp\").select(\"transaction_id\").toPandas()[\"transaction_id\"].values\n",
    "trx_count = len(trx_ids_sorted)\n",
    "\n",
    "last_train_index = int(np.floor(trx_count * TRAIN_PERC))\n",
    "last_validation_index = last_train_index + int(np.floor(trx_count * VALIDATION_PERC))\n",
    "train_indexes = trx_ids_sorted[:last_train_index]\n",
    "validation_indexes = trx_ids_sorted[last_train_index:last_validation_index]\n",
    "test_indexes = trx_ids_sorted[last_validation_index:]\n",
    "\n",
    "train_indexes_loc = os.path.join(location_main, \"temp_train_indexes.parquet\")\n",
    "validation_indexes_loc = os.path.join(location_main, \"temp_validation_indexes.parquet\")\n",
    "test_indexes_loc = os.path.join(location_main, \"temp_test_indexes.parquet\")\n",
    "\n",
    "pd.DataFrame(train_indexes, columns=[\"transaction_id\"]).to_parquet(train_indexes_loc)\n",
    "pd.DataFrame(validation_indexes, columns=[\"transaction_id\"]).to_parquet(validation_indexes_loc)\n",
    "pd.DataFrame(test_indexes, columns=[\"transaction_id\"]).to_parquet(test_indexes_loc)\n",
    "\n",
    "train_indexes = spark.read.parquet(train_indexes_loc)\n",
    "validation_indexes = spark.read.parquet(validation_indexes_loc)\n",
    "test_indexes = spark.read.parquet(test_indexes_loc)\n",
    "\n",
    "train = train_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "validation = validation_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "test = test_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "train_count, validation_count, test_count = train.count(), validation.count(), test.count()\n",
    "print()\n",
    "print(trx_count, train_count, validation_count, test_count)\n",
    "print()\n",
    "\n",
    "os.remove(train_indexes_loc)\n",
    "os.remove(validation_indexes_loc)\n",
    "os.remove(test_indexes_loc)\n",
    "\n",
    "train.write.parquet(\"train-temp\", mode=\"overwrite\")\n",
    "validation.write.parquet(\"validation-temp\", mode=\"overwrite\")\n",
    "test.write.parquet(\"test-temp\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "478fab4c-1cb5-4fd0-991a-8263a3ed2bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.parquet(\"train-temp\")\n",
    "validation = spark.read.parquet(\"validation-temp\")\n",
    "test = spark.read.parquet(\"test-temp\")\n",
    "test_count = test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6cc277c-cf84-487e-bcbd-0784a304cc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# edges = data.groupby([\"source\", \"target\"]).agg(\n",
    "#     sf.sum(\"amount\").alias(\"amount\")\n",
    "# ).toPandas()\n",
    "# weights = get_weights(edges)\n",
    "# edges_agg = edges.set_index([\"source\", \"target\"]).join(\n",
    "#     weights.set_index([\"source\", \"target\"]), how=\"left\"\n",
    "# ).reset_index()\n",
    "# edges_agg.loc[:, \"amount_weighted\"] = (\n",
    "#     edges_agg.loc[:, \"amount\"] * \n",
    "#     (edges_agg.loc[:, \"weight\"] / edges_agg.loc[:, \"weight\"].max())\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "199fa227-2ea8-4639-a5ce-fe198af819ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later on, we will reset the variables (to free up memory), while still keeping these intact\n",
    "to_keep = %who_ls\n",
    "to_keep = list(to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26195fe6-5b17-4525-bf16-83c63098e7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing comm_as_source\n",
      "\n",
      "Processed hop #1 | 6,546,146 | 1,667,461\n",
      "Processed hop #2 | 11,743,312 | 1,469,375\n",
      "Processed hop #3 | 21,126,341 | 1,409,144\n",
      "Processed hop #4 | 28,378,044 | 1,391,266\n",
      "Processed hop #5 | 31,710,015 | 1,386,115\n",
      "\n",
      "Processing comm_as_target\n",
      "\n",
      "Processed hop #1 | 6,558,360 | 1,314,480\n",
      "Processed hop #2 | 18,557,822 | 1,194,763\n",
      "Processed hop #3 | 31,991,564 | 1,162,193\n",
      "Processed hop #4 | 40,375,545 | 1,141,523\n",
      "Processed hop #5 | 42,541,507 | 1,128,392\n",
      "\n",
      "Processing comm_as_passthrough\n",
      "\n",
      "Processed hop #1 | 6,037,558 | 1,296,091\n",
      "Processed hop #2 | 10,075,945 | 1,116,547\n",
      "Processed hop #3 | 17,959,726 | 1,070,066\n",
      "Processed hop #4 | 22,473,287 | 1,055,391\n",
      "Processed hop #5 | 24,581,094 | 1,051,132\n",
      "\n",
      "Processing comm_as_passthrough_reverse\n",
      "\n",
      "Processed hop #1 | 6,430,169 | 1,286,192\n",
      "Processed hop #2 | 18,024,540 | 1,166,579\n",
      "Processed hop #3 | 31,227,006 | 1,135,566\n",
      "Processed hop #4 | 39,698,968 | 1,116,228\n",
      "Processed hop #5 | 42,104,754 | 1,104,409\n",
      "\n",
      "\n",
      "comm_as_source_features\n",
      "\n",
      "CPU times: user 3min 33s, sys: 2.18 s, total: 3min 35s\n",
      "Wall time: 3min 35s\n",
      "\n",
      "comm_as_target_features\n",
      "\n",
      "CPU times: user 3min 7s, sys: 2.4 s, total: 3min 10s\n",
      "Wall time: 3min 10s\n",
      "\n",
      "comm_as_passthrough_features\n",
      "\n",
      "CPU times: user 2min 44s, sys: 1.47 s, total: 2min 46s\n",
      "Wall time: 2min 46s\n",
      "\n",
      "comm_as_passthrough_features_reverse\n",
      "\n",
      "CPU times: user 3min 2s, sys: 1.83 s, total: 3min 4s\n",
      "Wall time: 3min 4s\n",
      "\n",
      "\n",
      "CPU times: user 38min 2s, sys: 1min 25s, total: 39min 27s\n",
      "Wall time: 39min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TOP_N = 50\n",
    "NUM_HOPS = 5\n",
    "\n",
    "data_input = spark.createDataFrame(edges_agg)\n",
    "nodes_source = set(edges_agg[\"source\"].unique())\n",
    "nodes_target = set(edges_agg[\"target\"].unique())\n",
    "nodes_passthrough = nodes_source.intersection(nodes_target)\n",
    "\n",
    "%run generate_flow_features.ipynb\n",
    "\n",
    "comm_as_source_features.to_parquet(location_comm_as_source_features)\n",
    "comm_as_target_features.to_parquet(location_comm_as_target_features)\n",
    "comm_as_passthrough_features.to_parquet(location_comm_as_passthrough_features)\n",
    "comm_as_passthrough_features_reverse.to_parquet(location_comm_as_passthrough_features_reverse)\n",
    "\n",
    "del comm_as_source_features\n",
    "del comm_as_target_features\n",
    "del comm_as_passthrough_features\n",
    "del comm_as_passthrough_features_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "986d0df2-1737-4f95-ad2b-a73c93bfd259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Leiden communities\n",
      "CPU times: user 1h 7min 25s, sys: 46.7 s, total: 1h 8min 12s\n",
      "Wall time: 1h 8min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Constructing Leiden communities\")\n",
    "\n",
    "graph = ig.Graph.DataFrame(edges_agg.loc[:, [\"source\", \"target\", \"amount_weighted\"]], use_vids=False, directed=True)\n",
    "nodes_mapping = {x.index: x[\"name\"] for x in graph.vs()}\n",
    "communities_leiden = la.find_partition(\n",
    "    graph, la.ModularityVertexPartition, n_iterations=100, weights=\"amount_weighted\"\n",
    ")\n",
    "communities_leiden = [[nodes_mapping[_] for _ in x] for x in communities_leiden]\n",
    "communities_leiden = [(str(uuid.uuid4()), set(x)) for x in communities_leiden]\n",
    "sizes_leiden = [len(x[1]) for x in communities_leiden]\n",
    "\n",
    "with open(location_communities_leiden, \"wb\") as fl:\n",
    "    pickle.dump(communities_leiden, fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26e3f6b2-1caa-4e11-b6ef-65d19f8cc156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(location_communities_leiden, \"rb\") as fl:\n",
    "#     communities_leiden = pickle.load(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0998de24-a007-40f3-9cad-0703940530c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2047791\n",
      "250000 2047791\n",
      "500000 2047791\n",
      "750000 2047791\n",
      "1000000 2047791\n",
      "1250000 2047791\n",
      "1500000 2047791\n",
      "1750000 2047791\n",
      "2000000 2047791\n",
      "CPU times: user 2min 58s, sys: 3.37 s, total: 3min 1s\n",
      "Wall time: 3min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_agg_weights = get_weights(\n",
    "    data.groupby([\"source\", \"target\"])\n",
    "    .agg(\n",
    "        sf.sum(\"amount\").alias(\"amount\")\n",
    "    ).toPandas()\n",
    ")\n",
    "\n",
    "data_agg_weights_rev = data_agg_weights.rename(\n",
    "    columns={\"target\": \"source\", \"source\": \"target\"}\n",
    ").loc[:, [\"source\", \"target\", \"weight\"]]\n",
    "data_agg_weights_ud = pd.concat([data_agg_weights, data_agg_weights_rev], ignore_index=True)\n",
    "data_agg_weights_ud = data_agg_weights_ud.groupby([\"source\", \"target\"]).agg(weight=(\"weight\", \"sum\")).reset_index()\n",
    "\n",
    "data_agg_weights_ud.sort_values(\"weight\", ascending=False, inplace=True)\n",
    "grouped_ud = data_agg_weights_ud.groupby(\"source\").head(KEEP_TOP_N).reset_index(drop=True)\n",
    "grouped_ud = grouped_ud.groupby(\"source\").agg(targets=(\"target\", set))\n",
    "\n",
    "total = grouped_ud.index.nunique()\n",
    "nodes_neighborhoods = {}\n",
    "for index, (source, targets) in enumerate(grouped_ud.iterrows()):\n",
    "    community_candidates = {source}\n",
    "    for target in targets[\"targets\"]:\n",
    "        community_candidates |= (grouped_ud.loc[target, \"targets\"] | {target})\n",
    "    nodes_neighborhoods[source] = set(community_candidates)\n",
    "    if not (index % 250_000):\n",
    "        print(index, total)\n",
    "\n",
    "del data_agg_weights_rev\n",
    "del data_agg_weights_ud\n",
    "del grouped_ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e37941e-b6c3-478c-bd54-a16572719410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing 2-hop communities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.5 s, sys: 3.01 s, total: 53.5 s\n",
      "Wall time: 6min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Constructing 2-hop communities\")\n",
    "\n",
    "communities_2_hop = get_communities_spark(\n",
    "    nodes_neighborhoods,\n",
    "    ig.Graph.DataFrame(edges_agg.loc[:, [\"source\", \"target\", \"amount_weighted\"]], use_vids=False, directed=True), \n",
    "    os.cpu_count(), spark, 2, \"all\", 0.01, \"amount_weighted\"\n",
    ")\n",
    "sizes_2_hop = [len(x[1]) for x in communities_2_hop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d03955cf-3674-4fa6-bc72-90939be394e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8177437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 24s, sys: 28.4 s, total: 1min 53s\n",
      "Wall time: 4min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ts_min = data.select(sf.min(\"timestamp\").alias(\"x\")).collect()[0][\"x\"] - timedelta(minutes=1)\n",
    "data_graph_agg = data.groupby([\"source\", \"target\", \"source_bank\", \"target_bank\", \"source_currency\"]).agg(\n",
    "    sf.count(\"source\").alias(\"num_transactions\"),\n",
    "    sf.sum(\"amount\").alias(\"amount\"),\n",
    "    sf.sum(\"source_amount\").alias(\"source_amount\"),\n",
    "    sf.collect_list(sf.array((sf.col(\"timestamp\") - ts_min).cast(\"long\"), sf.col(\"amount\"))).alias(\"timestamps_amounts\"),\n",
    ")\n",
    "data_graph_agg_sdf = data_graph_agg.persist(StorageLevel.DISK_ONLY)\n",
    "print(data_graph_agg_sdf.count())\n",
    "data_graph_agg = data_graph_agg_sdf.toPandas()\n",
    "index = [\"source\", \"target\"]\n",
    "edges_agg.loc[:, index + [\"weight\"]].set_index(index)\n",
    "data_graph_agg = data_graph_agg.set_index(index).join(\n",
    "    edges_agg.loc[:, index + [\"weight\"]].set_index(index), how=\"left\"\n",
    ").reset_index()\n",
    "data_graph_agg.loc[:, \"amount_weighted\"] = (\n",
    "    data_graph_agg.loc[:, \"amount\"] * \n",
    "    (data_graph_agg.loc[:, \"weight\"] / data_graph_agg.loc[:, \"weight\"].max())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea905a1c-23e7-4f88-b340-59867f2b4bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = ig.Graph.DataFrame(data_graph_agg, use_vids=False, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74dce0c8-f8e0-446d-9bfe-e456911434f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leiden communitites features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 59s, sys: 42 s, total: 3min 41s\n",
      "Wall time: 10min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Leiden communitites features creation\")\n",
    "\n",
    "features_leiden = generate_features_spark(communities_leiden, graph, spark)\n",
    "features_leiden = features_leiden.rename(columns={\"key\": \"key_fake\"})\n",
    "communities_leiden_dict = dict(communities_leiden)\n",
    "features_leiden.loc[:, \"key\"] = features_leiden.loc[:, \"key_fake\"].apply(lambda x: communities_leiden_dict[x])\n",
    "features_leiden = features_leiden.explode(\"key\")\n",
    "del features_leiden[\"key_fake\"]\n",
    "features_leiden.set_index(\"key\").to_parquet(location_features_leiden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66b1390f-e28d-4def-ae4a-f57766bcf83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-hop communitites features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28min 12s, sys: 13min 2s, total: 41min 14s\n",
      "Wall time: 1h 16min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"2-hop communitites features creation\")\n",
    "\n",
    "features_2_hop = generate_features_spark(communities_2_hop, graph, spark)\n",
    "features_2_hop.set_index(\"key\").to_parquet(location_features_2_hop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d51ba7de-7131-43c7-990e-03a97f4318a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4c4b527-dab6-4dee-9e19-64c9a74dc15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal flows features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175660810 159434964\n",
      "dispense\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passthrough\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sink\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.8 s, sys: 3.77 s, total: 23.6 s\n",
      "Wall time: 31min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Temporal flows features creation\")\n",
    "\n",
    "edges_totals = data.select(\"source\", \"target\", \"amount\").groupby(\n",
    "    [\"source\", \"target\"]\n",
    ").agg(sf.count(\"amount\").alias(\"amount\")).toPandas()\n",
    "edges_totals = edges_totals.sort_values(\"amount\", ascending=False).reset_index(drop=True)\n",
    "left_edges = spark.createDataFrame(edges_totals.groupby(\"target\").head(TOP_N).loc[:, [\"source\", \"target\"]])\n",
    "right_edges = spark.createDataFrame(edges_totals.groupby(\"source\").head(TOP_N).loc[:, [\"source\", \"target\"]])\n",
    "\n",
    "columns = [\"source\", \"target\", \"timestamp\", \"amount\"]\n",
    "\n",
    "left = left_edges.select(sf.col(\"source\").alias(\"src\"), sf.col(\"target\").alias(\"tgt\")).join(\n",
    "    data.select(*columns),\n",
    "    on=(sf.col(\"src\") == sf.col(\"source\")) & (sf.col(\"tgt\") == sf.col(\"target\")),\n",
    "    how=\"left\"\n",
    ").drop(\"src\", \"tgt\").persist(StorageLevel.DISK_ONLY)\n",
    "select = []\n",
    "for column in left.columns:\n",
    "    select.append(sf.col(column).alias(f\"left_{column}\"))\n",
    "left = left.select(*select)\n",
    "right = right_edges.select(sf.col(\"source\").alias(\"src\"), sf.col(\"target\").alias(\"tgt\")).join(\n",
    "    data.select(*columns),\n",
    "    on=(sf.col(\"src\") == sf.col(\"source\")) & (sf.col(\"tgt\") == sf.col(\"target\")),\n",
    "    how=\"left\"\n",
    ").drop(\"src\", \"tgt\").persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "print(left.count(), right.count())\n",
    "\n",
    "flows_temporal = left.join(\n",
    "    right,\n",
    "    (left[\"left_target\"] == right[\"source\"]) &\n",
    "    (left[\"left_timestamp\"] <= right[\"timestamp\"]),\n",
    "    how=\"inner\"\n",
    ").groupby([\"left_source\", \"left_target\", \"source\", \"target\"]).agg(\n",
    "    sf.sum(\"left_amount\").alias(\"left_amount\"),\n",
    "    sf.sum(\"amount\").alias(\"amount\"),\n",
    ").drop(\"left_target\").select(\n",
    "    sf.col(\"left_source\").alias(\"dispense\"),\n",
    "    sf.col(\"source\").alias(\"passthrough\"),\n",
    "    sf.col(\"target\").alias(\"sink\"),\n",
    "    sf.least(\"left_amount\", \"amount\").alias(\"amount\"),\n",
    ")\n",
    "\n",
    "aggregate = [\n",
    "    sf.sum(\"amount\").alias(\"amount_sum\"),\n",
    "    sf.mean(\"amount\").alias(\"amount_mean\"),\n",
    "    sf.median(\"amount\").alias(\"amount_median\"),\n",
    "    sf.max(\"amount\").alias(\"amount_max\"),\n",
    "    sf.stddev(\"amount\").alias(\"amount_std\"),\n",
    "    sf.countDistinct(\"dispense\").alias(\"dispense_count\"),\n",
    "    sf.countDistinct(\"passthrough\").alias(\"passthrough_count\"),\n",
    "    sf.countDistinct(\"sink\").alias(\"sink_count\"),\n",
    "]\n",
    "for flow_location, flow_type in [\n",
    "    (location_flow_dispense, \"dispense\"), (location_flow_passthrough, \"passthrough\"), (location_flow_sink, \"sink\")\n",
    "]:\n",
    "    print(flow_type)\n",
    "    flows_temporal_stats = flows_temporal.groupby(flow_type).agg(*aggregate).toPandas()\n",
    "    flows_temporal_cyclic_stats = flows_temporal.where(\n",
    "        (sf.col(\"dispense\") == sf.col(\"sink\"))\n",
    "    ).groupby(flow_type).agg(*aggregate).toPandas()\n",
    "    flows_temporal_stats = flows_temporal_stats.set_index(flow_type).join(\n",
    "        flows_temporal_cyclic_stats.set_index(flow_type),\n",
    "        how=\"left\", rsuffix=\"_cycle\"\n",
    "    )\n",
    "    flows_temporal_stats.index.name = \"key\"\n",
    "    flows_temporal_stats.to_parquet(flow_location)\n",
    "    del flows_temporal_stats\n",
    "    del flows_temporal_cyclic_stats\n",
    "\n",
    "left.unpersist()\n",
    "right.unpersist()\n",
    "\n",
    "del edges_totals\n",
    "del left_edges\n",
    "del right_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6839964f-cb70-4f50-97be-96b0acdd8355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-hop-source features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/01 04:16:59 WARN TaskSetManager: Stage 285 contains a task of very large size (8870 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 38s, sys: 46.6 s, total: 11min 24s\n",
      "Wall time: 44min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"1-hop-source features creation\")\n",
    "\n",
    "features_source = spark.createDataFrame(data_graph_agg).withColumn(\n",
    "    \"key\", sf.col(\"source\")\n",
    ").repartition(os.cpu_count() * 5, \"key\").groupby(\"key\").applyInPandas(\n",
    "    generate_features_udf_wrapper(False), schema=SCHEMA_FEAT_UDF\n",
    ").toPandas()\n",
    "features_source = pd.DataFrame(features_source[\"features\"].apply(json.loads).tolist())\n",
    "features_source.columns = [f\"{s.G_1HOP_PREFIX}{x}\" if x != \"key\" else x for x in features_source.columns]\n",
    "features_source.set_index(\"key\").to_parquet(location_features_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb81ceb5-dcdd-4c9e-bb47-1429de489490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-hop-target features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/01 05:09:07 WARN TaskSetManager: Stage 288 contains a task of very large size (8870 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 38s, sys: 4min 57s, total: 15min 35s\n",
      "Wall time: 40min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"1-hop-target features creation\")\n",
    "\n",
    "features_target = spark.createDataFrame(data_graph_agg).withColumn(\n",
    "    \"key\", sf.col(\"target\")\n",
    ").repartition(os.cpu_count() * 5, \"key\").groupby(\"key\").applyInPandas(\n",
    "    generate_features_udf_wrapper(False), schema=SCHEMA_FEAT_UDF\n",
    ").toPandas()\n",
    "features_target = pd.DataFrame(features_target[\"features\"].apply(json.loads).tolist())\n",
    "features_target.columns = [f\"{s.G_1HOP_PREFIX}{x}\" if x != \"key\" else x for x in features_target.columns]\n",
    "features_target.set_index(\"key\").to_parquet(location_features_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f335ce88-2ddb-4a5e-82ef-1f2295494bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_graph_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5be6ab9-3a2f-4d27-ad4b-3be73d99f71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLED_FEATURES = [\n",
    "    (\"leiden\", location_features_leiden),\n",
    "    (\"2_hop\", location_features_2_hop),\n",
    "    (\"as_source\", location_features_source),\n",
    "    (\"as_target\", location_features_target),\n",
    "    (\"comm_as_source_features\", location_comm_as_source_features),\n",
    "    (\"comm_as_target_features\", location_comm_as_target_features),\n",
    "    (\"comm_as_passthrough_features\", location_comm_as_passthrough_features),\n",
    "    (\"comm_as_passthrough_features_reverse\", location_comm_as_passthrough_features_reverse),\n",
    "    (\"flow_dispense\", location_flow_dispense),\n",
    "    (\"flow_passthrough\", location_flow_passthrough),\n",
    "    (\"flow_sink\", location_flow_sink),   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74b28c47-b23c-44e2-b53e-a80b07eb971f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: (2047791, 344)\n"
     ]
    }
   ],
   "source": [
    "all_features = pd.DataFrame()\n",
    "all_features.index.name = \"key\"\n",
    "\n",
    "for feature_group, location in ENABLED_FEATURES:\n",
    "    all_features = all_features.join(\n",
    "        pd.read_parquet(location), how=\"outer\", rsuffix=f\"_{feature_group}\"\n",
    "    )\n",
    "\n",
    "all_features.to_parquet(location_features_node_level)\n",
    "print(\"Features:\", all_features.shape)\n",
    "del all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85f3470e-c691-4aa7-b273-b4b0041f1dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.read_parquet(location_features_node_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09a76fe3-abb0-44b4-ac8a-b3b60eb55f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 14 constant columns\n"
     ]
    }
   ],
   "source": [
    "constants = []\n",
    "for column in all_features.columns:\n",
    "    if all_features[column].nunique(dropna=True) <= 1:\n",
    "        del all_features[column]\n",
    "        constants.append(column)\n",
    "print(f\"Deleted {len(constants)} constant columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1963559f-3817-41f4-8ea8-2869faf99e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = {}\n",
    "for column in all_features.columns:\n",
    "    medians[column] = np.nanmedian(all_features[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "322714b7-7063-4485-81b1-6d541d25bb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script executed in 0:00:32\n"
     ]
    }
   ],
   "source": [
    "delta = round(time.time() - start_script)\n",
    "print(f\"Script executed in {timedelta(seconds=delta)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6da3dca-7750-4520-9c78-4fa51c674804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the anomaly detection model\n",
      "CPU times: user 39.8 s, sys: 5.67 s, total: 45.5 s\n",
      "Wall time: 45.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Training the anomaly detection model\")\n",
    "\n",
    "anomalies = all_features.loc[:, []]\n",
    "model_ad = IsolationForest(n_estimators=10_000)\n",
    "anomalies.loc[:, \"anomaly_score\"] = -model_ad.fit(\n",
    "    all_features.fillna(medians)\n",
    ").decision_function(all_features.fillna(medians))\n",
    "anomalies.loc[:, \"anomaly_score\"] += abs(anomalies.loc[:, \"anomaly_score\"].min()) + 1e-10\n",
    "anomalies.loc[:, \"anomaly_score\"] /= anomalies.loc[:, \"anomaly_score\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11bc54f1-c238-43fa-9b1b-b5394cd55af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 98.4\n"
     ]
    }
   ],
   "source": [
    "n_components = 10\n",
    "pca = PCA(n_components=n_components)\n",
    "all_features_dim_reduced = pd.DataFrame(\n",
    "    pca.fit_transform(normalize(all_features.fillna(0), norm=\"l1\", axis=1)),\n",
    "    index=all_features.index\n",
    ")\n",
    "explained_variance_ratio = round(sum(pca.explained_variance_ratio_) * 100, 2)\n",
    "assert explained_variance_ratio > 95\n",
    "print(n_components, explained_variance_ratio)\n",
    "all_features_dim_reduced.columns = [\n",
    "    f\"pca_{x + 1}\" for x in all_features_dim_reduced.columns\n",
    "]\n",
    "all_features_dim_reduced = all_features_dim_reduced.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e30b7013-254c-4f97-9894-71af0b6c4587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# print(f\"Generating edge features\")\n",
    "\n",
    "# to_select = [\"source\", \"target\", \"format\", \"source_currency\", \"source_amount\", \"amount\", \"timestamp\"]\n",
    "\n",
    "# edges_features_input = data.select(to_select).groupby(\n",
    "#     [\"source\", \"target\", \"format\", \"source_currency\"]\n",
    "# ).agg(\n",
    "#     sf.sum(\"source_amount\").alias(\"source_amount\"), \n",
    "#     sf.sum(\"amount\").alias(\"amount\"),\n",
    "#     sf.unix_timestamp(sf.min(\"timestamp\")).alias(\"min_ts\"),\n",
    "#     sf.unix_timestamp(sf.max(\"timestamp\")).alias(\"max_ts\"),\n",
    "# ).repartition(os.cpu_count() * 2, \"source\", \"target\").persist(StorageLevel.DISK_ONLY)\n",
    "# _ = edges_features_input.count()\n",
    "\n",
    "# edge_features = edges_features_input.groupby([\"source\", \"target\"]).applyInPandas(\n",
    "#     get_edge_features_udf, schema=SCHEMA_FEAT_UDF\n",
    "# ).toPandas()\n",
    "# edge_features = pd.DataFrame(edge_features[\"features\"].apply(json.loads).tolist())\n",
    "\n",
    "# edge_features.to_parquet(location_features_edges)\n",
    "# del edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b80a20e-5859-4af4-93b5-09c47c2e3516",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_features = pd.read_parquet(location_features_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de5e1408-4d7a-4547-baaa-0cac6952106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.5 s, sys: 3.08 s, total: 39.6 s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_edges = train.select(\"source\", \"target\").drop_duplicates().toPandas().set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "valid_edges = validation.select(\"source\", \"target\").drop_duplicates().toPandas().set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "test_edges = test.select(\"source\", \"target\").drop_duplicates().toPandas().set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "\n",
    "train_features = train_edges.join(\n",
    "    edge_features.set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "validation_features = valid_edges.join(\n",
    "    edge_features.set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "test_features = test_edges.join(\n",
    "    edge_features.set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf277502-3d7e-40d5-b4f5-133bd7d7d079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 6 constant columns\n",
      "Features: (2047791, 1)\n"
     ]
    }
   ],
   "source": [
    "# Disable some features for final training\n",
    "# NOTE: PCA components for all features are already there!\n",
    "\n",
    "ENABLED_FEATURES_TRAINING = [\n",
    "    # (\"leiden\", location_features_leiden),\n",
    "    # (\"2_hop\", location_features_2_hop),\n",
    "    (\"as_source\", location_features_source),\n",
    "    (\"as_target\", location_features_target),\n",
    "    # (\"comm_as_source_features\", location_comm_as_source_features),\n",
    "    # (\"comm_as_target_features\", location_comm_as_target_features),\n",
    "    # (\"comm_as_passthrough_features\", location_comm_as_passthrough_features),\n",
    "    # (\"comm_as_passthrough_features_reverse\", location_comm_as_passthrough_features_reverse),\n",
    "    # (\"flow_dispense\", location_flow_dispense),\n",
    "    # (\"flow_passthrough\", location_flow_passthrough),\n",
    "    # (\"flow_sink\", location_flow_sink),   \n",
    "]\n",
    "\n",
    "all_features = pd.DataFrame()\n",
    "all_features.index.name = \"key\"\n",
    "\n",
    "for feature_group, location in ENABLED_FEATURES_TRAINING:\n",
    "    all_features = all_features.join(\n",
    "        pd.read_parquet(location), how=\"outer\", rsuffix=f\"_{feature_group}\"\n",
    "    )\n",
    "\n",
    "constants = []\n",
    "for column in all_features.columns:\n",
    "    if all_features[column].nunique(dropna=True) <= 1:\n",
    "        del all_features[column]\n",
    "        constants.append(column)\n",
    "print(f\"Deleted {len(constants)} constant columns\")\n",
    "\n",
    "# Disabling all features\n",
    "all_features = all_features.loc[:, list(all_features.columns[0:1])]\n",
    "print(\"Features:\", all_features.shape)\n",
    "\n",
    "all_features_spark = spark.createDataFrame(all_features.reset_index())\n",
    "for col in all_features_spark.columns:\n",
    "    all_features_spark = all_features_spark.withColumnRenamed(col, f\"node_{col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce68e646-67f2-4158-ba72-528c94207d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_edge_features(features_in, location):\n",
    "    initial_node_features = list(all_features_spark.columns)\n",
    "\n",
    "    features_in = features_in.rename(\n",
    "        columns={x: f\"edge_{x}\" for x in features_in.columns}\n",
    "    )\n",
    "    features_in = features_in.set_index(\"edge_target\").join(\n",
    "        anomalies, how=\"left\"\n",
    "    ).reset_index().set_index(\"edge_source\").join(\n",
    "        anomalies, how=\"left\", rsuffix=\"_source\"\n",
    "    ).reset_index().set_index(\"edge_target\").join(\n",
    "        all_features_dim_reduced, how=\"left\", rsuffix=\"_target\"\n",
    "    ).reset_index().set_index(\"edge_source\").join(\n",
    "        all_features_dim_reduced, how=\"left\", rsuffix=\"_source\"\n",
    "    ).reset_index()\n",
    "    \n",
    "    features_in = spark.createDataFrame(features_in)\n",
    "    features_in = features_in.withColumnRenamed(\"anomaly_score\", \"anomaly_score_target\")\n",
    "    \n",
    "    features_in = features_in.join(\n",
    "        all_features_spark,\n",
    "        features_in[\"edge_source\"] == all_features_spark[\"node_key\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    all_features_spark_target = all_features_spark.select(*initial_node_features)\n",
    "    for col in all_features_spark_target.columns:\n",
    "        all_features_spark_target = all_features_spark_target.withColumnRenamed(col, f\"{col}_target\")\n",
    "\n",
    "    features_in = features_in.join(\n",
    "        all_features_spark_target,\n",
    "        features_in[\"edge_target\"] == all_features_spark_target[\"node_key_target\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    features_in = features_in.withColumnRenamed(\"edge_source\", \"source\")\n",
    "    features_in = features_in.withColumnRenamed(\"edge_target\", \"target\").drop(\"node_key\", \"node_key_target\")\n",
    "\n",
    "    features_in = features_in.withColumn(\n",
    "        \"anom_scores_diff\", sf.col(\"anomaly_score_source\") - sf.col(\"anomaly_score_target\")\n",
    "    )\n",
    "    features_in = features_in.withColumn(\n",
    "        \"anom_scores_min\", sf.least(sf.col(\"anomaly_score_source\"), sf.col(\"anomaly_score_target\"))\n",
    "    )\n",
    "    features_in = features_in.withColumn(\n",
    "        \"anom_scores_max\", sf.greatest(sf.col(\"anomaly_score_source\"), sf.col(\"anomaly_score_target\"))\n",
    "    )\n",
    "    features_in = features_in.withColumn(\n",
    "        \"anom_scores_mean\", (sf.col(\"anomaly_score_source\") + sf.col(\"anomaly_score_target\")) / 2\n",
    "    )\n",
    "    features_in.write.parquet(location, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5af6d41c-660c-4bc2-87a8-73660fe48a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/01 13:43:02 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/08/01 13:43:03 WARN TaskSetManager: Stage 19 contains a task of very large size (3085 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/08/01 13:43:41 WARN TaskSetManager: Stage 20 contains a task of very large size (8175 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 27:==========================================>             (13 + 4) / 17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.1 s, sys: 3.07 s, total: 22.2 s\n",
      "Wall time: 1min 24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "save_edge_features(train_features, location_features_edges_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7adbd114-df75-4bca-b697-7fcd08ae36c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/01 13:44:22 WARN TaskSetManager: Stage 28 contains a task of very large size (3085 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/08/01 13:44:47 WARN TaskSetManager: Stage 29 contains a task of very large size (8175 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 36:===================================================>    (12 + 1) / 13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.1 s, sys: 2.66 s, total: 16.7 s\n",
      "Wall time: 56.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "save_edge_features(validation_features, location_features_edges_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40d4fed8-cc76-44e0-809a-5d6ae82539c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/01 13:45:18 WARN TaskSetManager: Stage 37 contains a task of very large size (3085 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/08/01 13:45:43 WARN TaskSetManager: Stage 38 contains a task of very large size (8175 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 45:===================================================>    (12 + 1) / 13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 s, sys: 2.63 s, total: 16.6 s\n",
      "Wall time: 55.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "save_edge_features(test_features, location_features_edges_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a824e164-79e5-41bd-9e00-2646593a643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trx_features(data_in, location):\n",
    "    columns = [\"source\", \"target\", \"source_currency\", \"target_currency\", \"format\"]\n",
    "\n",
    "    # trx_features = data_in.groupby(columns).agg(\n",
    "    #     sf.sum(\"amount\").alias(\"amount\"),\n",
    "    #     sf.count(\"amount\").alias(\"trx_count\"),\n",
    "    #     sf.max(\"is_laundering\").alias(\"is_laundering\"),\n",
    "    # ).toPandas()\n",
    "\n",
    "    trx_features = data_in.select(*(columns + [\"amount\", \"is_laundering\"])).toPandas()\n",
    "    \n",
    "    trx_features.loc[:, \"inter_currency\"] = trx_features[\"source_currency\"] != trx_features[\"target_currency\"]\n",
    "\n",
    "    trx_features.to_parquet(location)\n",
    "    del trx_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c783f612-4c76-4ea5-aed2-68eeaec1191d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "save_trx_features(train, location_train_trx_features)\n",
    "save_trx_features(validation, location_valid_trx_features)\n",
    "save_trx_features(test, location_test_trx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e6eaaec-8af4-43d7-8127-a306b855f430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To free up memory for training\n",
    "\n",
    "to_reset = %who_ls\n",
    "to_reset = list(to_reset)\n",
    "if \"to_keep\" in to_reset:\n",
    "    to_reset.remove(\"to_keep\")\n",
    "to_reset = set(to_reset) - set(to_keep)\n",
    "for var_to_reset in list(to_reset):\n",
    "    var_to_reset = f\"^{var_to_reset}$\"\n",
    "    %reset_selective -f {var_to_reset}\n",
    "\n",
    "delete_large_vars(globals(), locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13c3a17d-14f9-4d34-9103-0c7e2e8c064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(location_features_trx, location_features_edges, location_features, is_test_ds=False):\n",
    "    features_input = spark.read.parquet(location_features_edges)\n",
    "    trx_features_input = spark.read.parquet(location_features_trx).withColumnRenamed(\n",
    "        \"source\", \"source_trx\"\n",
    "    ).withColumnRenamed(\n",
    "        \"target\", \"target_trx\"\n",
    "    )\n",
    "    drop = [\"source_trx\", \"target_trx\"]\n",
    "    if not is_test_ds:\n",
    "        drop += [\"source\", \"target\"]\n",
    "    features_input = trx_features_input.join(\n",
    "        features_input,\n",
    "        (trx_features_input[\"source_trx\"] == features_input[\"source\"]) &\n",
    "        (trx_features_input[\"target_trx\"] == features_input[\"target\"]),\n",
    "        how=\"left\"\n",
    "    ).drop(*drop)\n",
    "    features_input = features_input.write.parquet(location_features, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96867e2d-ceed-4176-b2f2-cb407c592ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:===============================================>        (29 + 5) / 34]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 71.2 ms, sys: 35.3 ms, total: 106 ms\n",
      "Wall time: 1min 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "combine_features(location_train_trx_features, location_features_edges_train, location_train_features)\n",
    "combine_features(location_valid_trx_features, location_features_edges_valid, location_valid_features)\n",
    "combine_features(\n",
    "    location_test_trx_features, location_features_edges_test, location_test_features,\n",
    "    is_test_ds=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8924af3-4f88-48dc-9e82-ab345e77f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(MULTI_PROC_STAGING_LOCATION, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc30b26a-7d61-44b0-ab1a-88b0a789ea0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "category_features = [\"source_currency\", \"target_currency\", \"format\"]\n",
    "category_features_map = {}\n",
    "for feat in category_features:\n",
    "    dist_vals = data.select(feat).distinct().toPandas()\n",
    "    dist_vals = dist_vals.sort_values(feat).reset_index(drop=True)\n",
    "    category_features_map[feat] = dict(zip(dist_vals[feat], dist_vals.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5e1587b-86bf-472f-9f28-38114fee3c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 54.7 s, total: 1min 57s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_f = pd.read_parquet(location_train_features)\n",
    "category_features_new = []\n",
    "cat_data = train_f.loc[:, []].copy(deep=True)\n",
    "for col in category_features:\n",
    "    mapping = category_features_map[col]\n",
    "    new_col = f\"{col}_cat\"\n",
    "    cat_data.loc[:, new_col] = train_f.loc[:, col].apply(lambda x: mapping[x])\n",
    "    del train_f[col]\n",
    "    category_features_new.append(new_col)\n",
    "\n",
    "train_f = pd.concat([train_f, cat_data], axis=1)\n",
    "train_labels = train_f[\"is_laundering\"]\n",
    "del train_f[\"is_laundering\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15a569ff-605f-4a4c-894c-ef57aee429d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_types = []\n",
    "for col, dtype in zip(train_f.columns, train_f.dtypes):\n",
    "    if col in category_features_new:\n",
    "        feature_types.append(\"c\")\n",
    "    elif dtype == bool:\n",
    "        feature_types.append(\"i\")\n",
    "    elif str(dtype).startswith(\"float\"):\n",
    "        feature_types.append(\"q\")\n",
    "    elif str(dtype).startswith(\"int\"):\n",
    "        feature_types.append(\"int\")\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc1ff96b-2c72-45a9-9da9-207463a2fedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.2 s, sys: 25.5 s, total: 46.6 s\n",
      "Wall time: 31.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "valid_f = pd.read_parquet(location_valid_features)\n",
    "cat_data = valid_f.loc[:, []].copy(deep=True)\n",
    "for col in category_features:\n",
    "    mapping = category_features_map[col]\n",
    "    new_col = f\"{col}_cat\"\n",
    "    cat_data.loc[:, new_col] = valid_f.loc[:, col].apply(lambda x: mapping[x])\n",
    "    del valid_f[col]\n",
    "\n",
    "valid_f = pd.concat([valid_f, cat_data], axis=1)\n",
    "valid_labels = valid_f[\"is_laundering\"]\n",
    "del valid_f[\"is_laundering\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d11bf0c-81cb-42e5-a903-0b6566c0290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_eval(y, y_):\n",
    "    return 1 - f1_score(y, np.round(y_))\n",
    "\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    scale_pos_weight=5, early_stopping_rounds=10,\n",
    "    eval_metric=f1_eval, disable_default_eval_metric=True, \n",
    "    num_parallel_tree=1, max_depth=6,\n",
    "    colsample_bytree=1, subsample=1,\n",
    "    n_estimators=100, enable_categorical=True,\n",
    "    feature_types=feature_types,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0a1a639-912f-49ba-8e5c-f4f95d066f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-f1_eval:0.93890\n",
      "[1]\tvalidation_0-f1_eval:0.93866\n",
      "[2]\tvalidation_0-f1_eval:0.93813\n",
      "[3]\tvalidation_0-f1_eval:0.93211\n",
      "[4]\tvalidation_0-f1_eval:0.92519\n",
      "[5]\tvalidation_0-f1_eval:0.91134\n",
      "[6]\tvalidation_0-f1_eval:0.91027\n",
      "[7]\tvalidation_0-f1_eval:0.90816\n",
      "[8]\tvalidation_0-f1_eval:0.88959\n",
      "[9]\tvalidation_0-f1_eval:0.88032\n",
      "[10]\tvalidation_0-f1_eval:0.87640\n",
      "[11]\tvalidation_0-f1_eval:0.87285\n",
      "[12]\tvalidation_0-f1_eval:0.86977\n",
      "[13]\tvalidation_0-f1_eval:0.86666\n",
      "[14]\tvalidation_0-f1_eval:0.85460\n",
      "[15]\tvalidation_0-f1_eval:0.85287\n",
      "[16]\tvalidation_0-f1_eval:0.85170\n",
      "[17]\tvalidation_0-f1_eval:0.84299\n",
      "[18]\tvalidation_0-f1_eval:0.84148\n",
      "[19]\tvalidation_0-f1_eval:0.83980\n",
      "[20]\tvalidation_0-f1_eval:0.83128\n",
      "[21]\tvalidation_0-f1_eval:0.83023\n",
      "[22]\tvalidation_0-f1_eval:0.82949\n",
      "[23]\tvalidation_0-f1_eval:0.82970\n",
      "[24]\tvalidation_0-f1_eval:0.82932\n",
      "[25]\tvalidation_0-f1_eval:0.82920\n",
      "[26]\tvalidation_0-f1_eval:0.82928\n",
      "[27]\tvalidation_0-f1_eval:0.82892\n",
      "[28]\tvalidation_0-f1_eval:0.82872\n",
      "[29]\tvalidation_0-f1_eval:0.82794\n",
      "[30]\tvalidation_0-f1_eval:0.82588\n",
      "[31]\tvalidation_0-f1_eval:0.82501\n",
      "[32]\tvalidation_0-f1_eval:0.82340\n",
      "[33]\tvalidation_0-f1_eval:0.82345\n",
      "[34]\tvalidation_0-f1_eval:0.82176\n",
      "[35]\tvalidation_0-f1_eval:0.82105\n",
      "[36]\tvalidation_0-f1_eval:0.82013\n",
      "[37]\tvalidation_0-f1_eval:0.81989\n",
      "[38]\tvalidation_0-f1_eval:0.81735\n",
      "[39]\tvalidation_0-f1_eval:0.81634\n",
      "[40]\tvalidation_0-f1_eval:0.81623\n",
      "[41]\tvalidation_0-f1_eval:0.81623\n",
      "[42]\tvalidation_0-f1_eval:0.81484\n",
      "[43]\tvalidation_0-f1_eval:0.81278\n",
      "[44]\tvalidation_0-f1_eval:0.81274\n",
      "[45]\tvalidation_0-f1_eval:0.81254\n",
      "[46]\tvalidation_0-f1_eval:0.81192\n",
      "[47]\tvalidation_0-f1_eval:0.80975\n",
      "[48]\tvalidation_0-f1_eval:0.80831\n",
      "[49]\tvalidation_0-f1_eval:0.80800\n",
      "[50]\tvalidation_0-f1_eval:0.80751\n",
      "[51]\tvalidation_0-f1_eval:0.80429\n",
      "[52]\tvalidation_0-f1_eval:0.80164\n",
      "[53]\tvalidation_0-f1_eval:0.80176\n",
      "[54]\tvalidation_0-f1_eval:0.80170\n",
      "[55]\tvalidation_0-f1_eval:0.80188\n",
      "[56]\tvalidation_0-f1_eval:0.80185\n",
      "[57]\tvalidation_0-f1_eval:0.80181\n",
      "[58]\tvalidation_0-f1_eval:0.80151\n",
      "[59]\tvalidation_0-f1_eval:0.80152\n",
      "[60]\tvalidation_0-f1_eval:0.80176\n",
      "[61]\tvalidation_0-f1_eval:0.79981\n",
      "[62]\tvalidation_0-f1_eval:0.79958\n",
      "[63]\tvalidation_0-f1_eval:0.79898\n",
      "[64]\tvalidation_0-f1_eval:0.79728\n",
      "[65]\tvalidation_0-f1_eval:0.79717\n",
      "[66]\tvalidation_0-f1_eval:0.79756\n",
      "[67]\tvalidation_0-f1_eval:0.79743\n",
      "[68]\tvalidation_0-f1_eval:0.79627\n",
      "[69]\tvalidation_0-f1_eval:0.79670\n",
      "[70]\tvalidation_0-f1_eval:0.79668\n",
      "[71]\tvalidation_0-f1_eval:0.79519\n",
      "[72]\tvalidation_0-f1_eval:0.79489\n",
      "[73]\tvalidation_0-f1_eval:0.79499\n",
      "[74]\tvalidation_0-f1_eval:0.79492\n",
      "[75]\tvalidation_0-f1_eval:0.79496\n",
      "[76]\tvalidation_0-f1_eval:0.79411\n",
      "[77]\tvalidation_0-f1_eval:0.79275\n",
      "[78]\tvalidation_0-f1_eval:0.79271\n",
      "[79]\tvalidation_0-f1_eval:0.79307\n",
      "[80]\tvalidation_0-f1_eval:0.79300\n",
      "[81]\tvalidation_0-f1_eval:0.79154\n",
      "[82]\tvalidation_0-f1_eval:0.79142\n",
      "[83]\tvalidation_0-f1_eval:0.79115\n",
      "[84]\tvalidation_0-f1_eval:0.79075\n",
      "[85]\tvalidation_0-f1_eval:0.79051\n",
      "[86]\tvalidation_0-f1_eval:0.79077\n",
      "[87]\tvalidation_0-f1_eval:0.79090\n",
      "[88]\tvalidation_0-f1_eval:0.79110\n",
      "[89]\tvalidation_0-f1_eval:0.79004\n",
      "[90]\tvalidation_0-f1_eval:0.79009\n",
      "[91]\tvalidation_0-f1_eval:0.79042\n",
      "[92]\tvalidation_0-f1_eval:0.79048\n",
      "[93]\tvalidation_0-f1_eval:0.79048\n",
      "[94]\tvalidation_0-f1_eval:0.79015\n",
      "[95]\tvalidation_0-f1_eval:0.79059\n",
      "[96]\tvalidation_0-f1_eval:0.79080\n",
      "[97]\tvalidation_0-f1_eval:0.79035\n",
      "[98]\tvalidation_0-f1_eval:0.79031\n",
      "[99]\tvalidation_0-f1_eval:0.79047\n",
      "0.790035 89\n",
      "CPU times: user 1h 1min 16s, sys: 45min 12s, total: 1h 46min 29s\n",
      "Wall time: 22min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(\n",
    "    train_f, train_labels, verbose=True, eval_set=[(valid_f, valid_labels)],\n",
    ")\n",
    "print(model.best_score, model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77203a87-bc55-4a00-a312-e415b5cbaf3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.790035, 89)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_score, model.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9fbaac60-0e7c-40c3-955c-6a5cfe81ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_f\n",
    "del valid_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2038bd32-b21a-4c10-b9cf-cf0f6cb0b2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_f = pd.read_parquet(location_test_features)\n",
    "join_columns = [\"source\", \"target\", \"source_currency\", \"target_currency\", \"format\"]\n",
    "test_predictions = test_f.loc[:, join_columns].copy(deep=True)\n",
    "del test_f[\"source\"]\n",
    "del test_f[\"target\"]\n",
    "\n",
    "cat_data = test_f.loc[:, []].copy(deep=True)\n",
    "for col in category_features:\n",
    "    mapping = category_features_map[col]\n",
    "    new_col = f\"{col}_cat\"\n",
    "    cat_data.loc[:, new_col] = test_f.loc[:, col].apply(lambda x: mapping[x])\n",
    "    del test_f[col]\n",
    "\n",
    "test_f = pd.concat([test_f, cat_data], axis=1)\n",
    "test_labels = test_f[\"is_laundering\"]\n",
    "del test_f[\"is_laundering\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53de8b82-12d3-441f-80b7-63be8bfd2e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions.loc[:, \"prediction\"] = model.predict(test_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ac71b697-c1ba-4e00-af59-c19c793171d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.64 20.65\n"
     ]
    }
   ],
   "source": [
    "f1_final = f1_score(test_labels, test_predictions.loc[:, \"prediction\"]) * 100\n",
    "recall = recall_score(test_labels, test_predictions.loc[:, \"prediction\"]) * 100\n",
    "print(round(f1_score_final, 2), round(recall, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4ad6e354-7ce0-4ddd-84a4-3bceae270544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.2725 33.4261\n"
     ]
    }
   ],
   "source": [
    "# test_predictions_final = spark.createDataFrame(test_predictions).alias(\"pred\")\n",
    "# test_predictions_final = test.join(\n",
    "#     test_predictions_final,\n",
    "#     (test[\"source\"] == test_predictions_final[\"pred.source\"]) &\n",
    "#     (test[\"target\"] == test_predictions_final[\"pred.target\"]) &\n",
    "#     (test[\"source_currency\"] == test_predictions_final[\"pred.source_currency\"]) &\n",
    "#     (test[\"target_currency\"] == test_predictions_final[\"pred.target_currency\"]) &\n",
    "#     (test[\"format\"] == test_predictions_final[\"pred.format\"]),\n",
    "#     how=\"left\"\n",
    "# ).select(\"transaction_id\", \"is_laundering\", \"prediction\").persist(storageLevel=StorageLevel.DISK_ONLY)\n",
    "\n",
    "# assert test_predictions_final.count() == test_count\n",
    "\n",
    "# test_predictions_final = test_predictions_final.toPandas()\n",
    "# f1_final = f1_score(test_predictions_final[\"is_laundering\"], test_predictions_final[\"prediction\"])\n",
    "# f1_final = round(f1_final * 100, 4)\n",
    "# recall = round(recall_score(test_predictions_final[\"is_laundering\"], test_predictions_final[\"prediction\"]) * 100, 4)\n",
    "# print(f1_final, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c18819c8-a15e-40da-b154-01c7c52cd7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gfp_best = 24.23\n",
    "gfp_std = 0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "998bda30-ec09-4839-84b3-bebdbb79bea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFP best: 24.23 ± 0.12\n"
     ]
    }
   ],
   "source": [
    "print(f\"GFP best: {gfp_best} ± {gfp_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb6bbb2c-8383-43e7-91f2-447c9b013683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.64 ± NA\n"
     ]
    }
   ],
   "source": [
    "# print(f\"{round(f1_final, 2)} ±{round(np.std(f1_scores), 2)}\")\n",
    "print(f\"{round(f1_final, 2)} ± NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0f8243a3-75d5-46e5-81c7-a5418ec30623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uplift of 30.6%\n"
     ]
    }
   ],
   "source": [
    "uplift = round(((f1_final - gfp_best) / gfp_best) * 100, 2)\n",
    "print(f\"Uplift of {uplift}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf189cae-0f78-4d68-a62d-8c08f7d0abc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19cb5afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from typing import Iterator, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import types as st\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "import settings as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d25de0ba-74f0-479e-af22-3d6265b6e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert s.FILE_SIZE == \"Small\", \"Script suitable for `small` datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf9769f9-0403-4c53-9610-d4f3b85fe6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRIP_IDS = True\n",
    "CALCULATE_USD_AMOUNT = True\n",
    "\n",
    "TRX_PARTITIONS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd40b21f-0f55-40dc-93eb-e449cd1cf5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    sys.version_info.major,\n",
    "    sys.version_info.minor,\n",
    "    sys.version_info.micro,\n",
    ") != (3, 9, 19):\n",
    "    raise EnvironmentError(\n",
    "        \"Only runs efficiently on Python 3.9.19 (Tested on: Conda 24.1.2 | Apple M3 Pro)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b85caee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/16 19:54:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/10/16 19:54:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "config = [\n",
    "    (\"spark.driver.memory\", \"32g\"),\n",
    "    (\"spark.worker.memory\", \"32g\"),\n",
    "    (\"spark.driver.maxResultSize\", \"32g\"),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"true\"),\n",
    "]\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"testing\")\n",
    "    .config(conf=SparkConf().setAll(config))\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d9503bf-bc5e-4e10-87d1-dae179e8af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_account = pd.read_csv(s.ACCOUNT_FILE).set_index(\"Account Number\")[\"Entity ID\"].to_dict()\n",
    "data_account_func = sf.udf(lambda x: data_account[x[:9]], st.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a2d4cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(value):\n",
    "    return f\"id-{hash(value)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa090168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.91 s, sys: 315 ms, total: 3.23 s\n",
      "Wall time: 3.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    os.remove(s.STAGED_DATA_CSV_LOCATION)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "\n",
    "mapping = {}\n",
    "with open(s.DATA_FILE) as in_file:\n",
    "    cnt = -1\n",
    "    lines = \"\"\n",
    "    for line in in_file:\n",
    "        cnt += 1\n",
    "        if cnt == 0:\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        line_id = get_id(line)\n",
    "        mapping[line_id] = cnt\n",
    "        lines += f\"{cnt},{line}\\n\"\n",
    "        if not (cnt % 2e7):\n",
    "            print(cnt)\n",
    "            with open(s.STAGED_DATA_CSV_LOCATION, \"a\") as out_file:\n",
    "                out_file.write(lines)\n",
    "                lines = \"\"\n",
    "if lines:\n",
    "    lines = lines.strip()\n",
    "    with open(s.STAGED_DATA_CSV_LOCATION, \"a\") as out_file:\n",
    "        out_file.write(lines)\n",
    "        del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "467df787",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.remove(s.STAGED_PATTERNS_CSV_LOCATION)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "lines = \"\"\n",
    "with open(s.PATTERNS_FILE) as in_file:\n",
    "    for line in in_file:\n",
    "        line = line.strip()\n",
    "        if line[:4].isnumeric():\n",
    "            line_id = get_id(line)\n",
    "            cnt = mapping[line_id]\n",
    "            lines += f\"{cnt},{line}\\n\"\n",
    "        else:\n",
    "            lines += f\"{line}\\n\"\n",
    "\n",
    "lines = lines.strip()\n",
    "with open(s.STAGED_PATTERNS_CSV_LOCATION, \"a\") as out_file:\n",
    "    out_file.write(lines)\n",
    "    del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "304986b5-67dd-431d-b77a-8987b4fcd8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcd1cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = st.StructType(\n",
    "    [\n",
    "        st.StructField(\"transaction_id\", st.IntegerType(), False),\n",
    "        st.StructField(\"timestamp\", st.TimestampType(), False),\n",
    "        st.StructField(\"source_bank\", st.StringType(), False),\n",
    "        st.StructField(\"source\", st.StringType(), False),\n",
    "        st.StructField(\"target_bank\", st.StringType(), False),\n",
    "        st.StructField(\"target\", st.StringType(), False),\n",
    "        st.StructField(\"received_amount\", st.FloatType(), False),\n",
    "        st.StructField(\"receiving_currency\", st.StringType(), False),\n",
    "        st.StructField(\"sent_amount\", st.FloatType(), False),\n",
    "        st.StructField(\"sending_currency\", st.StringType(), False),\n",
    "        st.StructField(\"format\", st.StringType(), False),\n",
    "        st.StructField(\"is_laundering\", st.IntegerType(), False),\n",
    "    ]\n",
    ")\n",
    "columns = [x.name for x in schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c2ee4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(s.STAGED_PATTERNS_CSV_LOCATION, \"r\") as fl:\n",
    "    patterns = fl.read()\n",
    "\n",
    "cases = []\n",
    "case_id = 0\n",
    "for pattern in patterns.split(\"\\n\\n\"):\n",
    "    case_id += 1\n",
    "    if not pattern.strip():\n",
    "        continue\n",
    "    pattern = pattern.split(\"\\n\")\n",
    "    name = pattern.pop(0).split(\" - \")[1]\n",
    "    category, sub_category = name, name\n",
    "    if \": \" in name:\n",
    "        category, sub_category = name.split(\": \")\n",
    "    pattern.pop()\n",
    "    case = pd.DataFrame([x.split(\",\") for x in pattern], columns=columns)\n",
    "    case.loc[:, \"id\"] = case_id\n",
    "    case.loc[:, \"type\"] = category.strip().lower()\n",
    "    case.loc[:, \"sub_type\"] = sub_category.strip().lower()\n",
    "    cases.append(case)\n",
    "cases = pd.concat(cases, ignore_index=True)\n",
    "cases = spark.createDataFrame(cases)\n",
    "cases = cases.withColumn(\"timestamp\", sf.to_timestamp(\"timestamp\", s.TIMESTAMP_FORMAT))\n",
    "cases = cases.select(\"transaction_id\", \"id\", \"type\", \"sub_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45180d75-a337-468b-89e3-38f1b82646f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENCY_MAPPING = {\n",
    "    \"Australian Dollar\": \"aud\",\n",
    "    \"Bitcoin\": \"btc\",\n",
    "    \"Brazil Real\": \"brl\",\n",
    "    \"Canadian Dollar\": \"cad\",\n",
    "    \"Euro\": \"eur\",\n",
    "    \"Mexican Peso\": \"mxn\",\n",
    "    \"Ruble\": \"rub\",\n",
    "    \"Rupee\": \"inr\",\n",
    "    \"Saudi Riyal\": \"sar\",\n",
    "    \"Shekel\": \"ils\",\n",
    "    \"Swiss Franc\": \"chf\",\n",
    "    \"UK Pound\": \"gbp\",\n",
    "    \"US Dollar\": \"usd\",\n",
    "    \"Yen\": \"jpy\",\n",
    "    \"Yuan\": \"cny\",\n",
    "}\n",
    "\n",
    "currency_code = sf.udf(lambda x: CURRENCY_MAPPING[x], st.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edb394c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:==========================================>              (12 + 4) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 83.6 ms, sys: 24.5 ms, total: 108 ms\n",
      "Wall time: 15.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "5072693"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data = spark.read.csv(\n",
    "    s.STAGED_DATA_CSV_LOCATION,\n",
    "    header=False,\n",
    "    schema=schema,\n",
    "    timestampFormat=s.TIMESTAMP_FORMAT,\n",
    ")\n",
    "group_by = [\n",
    "    \"timestamp\",\n",
    "    \"source_bank\",\n",
    "    \"source\",\n",
    "    \"target_bank\",\n",
    "    \"target\",\n",
    "    \"receiving_currency\",\n",
    "    \"sending_currency\",\n",
    "    \"format\",\n",
    "]\n",
    "data = data.groupby(group_by).agg(\n",
    "    sf.first(\"transaction_id\").alias(\"transaction_id\"),\n",
    "    sf.collect_set(\"transaction_id\").alias(\"transaction_ids\"),\n",
    "    sf.sum(\"received_amount\").alias(\"received_amount\"),\n",
    "    sf.sum(\"sent_amount\").alias(\"sent_amount\"),\n",
    "    sf.max(\"is_laundering\").alias(\"is_laundering\"),\n",
    ")\n",
    "data = data.withColumn(\n",
    "    \"source_currency\", currency_code(sf.col(\"sending_currency\"))\n",
    ").withColumn(\n",
    "    \"target_currency\",\n",
    "    currency_code(sf.col(\"receiving_currency\")),\n",
    ")\n",
    "data = data.join(cases, on=\"transaction_id\", how=\"left\").repartition(\n",
    "    TRX_PARTITIONS, \"transaction_id\"\n",
    ")\n",
    "data = data.select(\n",
    "    \"transaction_id\",\n",
    "    \"transaction_ids\",\n",
    "    \"timestamp\",\n",
    "    sf.concat(sf.col(\"source\"), sf.lit(\"-\"), sf.col(\"source_currency\")).alias(\"source\"),\n",
    "    sf.concat(sf.col(\"target\"), sf.lit(\"-\"), sf.col(\"target_currency\")).alias(\"target\"),\n",
    "    \"source_bank\",\n",
    "    \"target_bank\",\n",
    "    \"source_currency\",\n",
    "    \"target_currency\",\n",
    "    sf.col(\"sent_amount\").alias(\"source_amount\"),\n",
    "    sf.col(\"received_amount\").alias(\"target_amount\"),\n",
    "    \"format\",\n",
    "    \"is_laundering\",\n",
    ")\n",
    "\n",
    "data = data.withColumn(\"source_entity\", data_account_func(sf.col(\"source\")))\n",
    "data = data.withColumn(\"target_entity\", data_account_func(sf.col(\"target\")))\n",
    "\n",
    "data = data.persist(StorageLevel.DISK_ONLY)\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72d4b7c6-8740-48ac-9314-25ebf3e56862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5652"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select(sf.explode(\"transaction_ids\")).count() - data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0364dec-3ed8-4213-b7a8-4f6f9eba67dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "cases_data = (\n",
    "    cases.join(\n",
    "        data.withColumnRenamed(\"transaction_id\", \"x\")\n",
    "        .drop(*cases.columns)\n",
    "        .select(sf.explode(\"transaction_ids\").alias(\"transaction_id\"), \"*\"),\n",
    "        on=\"transaction_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .drop(\"is_laundering\", \"transaction_id\", \"transaction_ids\")\n",
    "    .withColumnRenamed(\"x\", \"transaction_id\")\n",
    ")\n",
    "cases_data.toPandas().to_parquet(s.STAGED_CASES_DATA_LOCATION)\n",
    "cases_data = pd.read_parquet(s.STAGED_CASES_DATA_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9958c88-72c6-46c2-89fa-cea3635b7ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_rates = {\n",
    "    \"jpy\": np.float32(0.009487665410827868),\n",
    "    \"cny\": np.float32(0.14930721887033868),\n",
    "    \"cad\": np.float32(0.7579775434031815),\n",
    "    \"sar\": np.float32(0.2665884611958837),\n",
    "    \"aud\": np.float32(0.7078143121927827),\n",
    "    \"ils\": np.float32(0.29612081311363503),\n",
    "    \"chf\": np.float32(1.0928961554056371),\n",
    "    \"usd\": np.float32(1.0),\n",
    "    \"eur\": np.float32(1.171783425225877),\n",
    "    \"rub\": np.float32(0.012852809604990688),\n",
    "    \"gbp\": np.float32(1.2916554735187644),\n",
    "    \"btc\": np.float32(11879.132698717296),\n",
    "    \"inr\": np.float32(0.013615817231245796),\n",
    "    \"mxn\": np.float32(0.047296753463246695),\n",
    "    \"brl\": np.float32(0.1771008654705292),\n",
    "}\n",
    "\n",
    "@sf.pandas_udf(st.FloatType())\n",
    "def get_usd_amount(iterator: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    for a, b in iterator:\n",
    "        yield [currency_rates[x] for x in a] * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8483dd03-18e9-4a29-8f9a-b9e751fcf4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if STRIP_IDS:\n",
    "    data = data.withColumn(\"source\", sf.substring(\"source\", 1, 8))\n",
    "    data = data.withColumn(\"target\", sf.substring(\"target\", 1, 8))\n",
    "if CALCULATE_USD_AMOUNT:\n",
    "    data = data.withColumn(\"amount\", get_usd_amount(\"source_currency\", \"source_amount\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4abd5602-0afa-4f16-93bb-c467c7c44018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "data.repartition(1).write.parquet(\"temp.parquet\", mode=\"overwrite\")\n",
    "data_pd = pd.read_parquet(\"temp.parquet\").sort_values(\"timestamp\")\n",
    "shutil.rmtree(\"temp.parquet\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c675d919-3139-40be-b3f2-efd49c56480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_main = os.path.abspath(f\".{os.sep}data\")\n",
    "\n",
    "location_source_dispensation = os.path.join(location_main, \"source_dispensation.parquet\")\n",
    "location_target_accumulation = os.path.join(location_main, \"target_accumulation.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ed4ab78-8473-4051-8192-949d3c07fa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 496086\n",
      "200000 496086\n",
      "400000 496086\n",
      "CPU times: user 1min 9s, sys: 766 ms, total: 1min 10s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_unique = data_pd[\"source\"].nunique()\n",
    "source_dispensation = []\n",
    "for index, (_, group) in enumerate(data_pd[[\"source\", \"amount\"]].groupby(\"source\")):\n",
    "    group.loc[:, \"source_dispensation\"] = group[\"amount\"].cumsum()\n",
    "    source_dispensation.append(group)\n",
    "    if not (index % 200_000):\n",
    "        print(index, num_unique)\n",
    "source_dispensation = pd.concat(source_dispensation, ignore_index=False)\n",
    "source_dispensation.to_parquet(location_source_dispensation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f106c4e2-768a-4356-b0f0-4f58406e03ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dispensation = pd.read_parquet(location_source_dispensation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "569d1c21-15bc-4858-9dcb-73aaf93d1a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 419952\n",
      "200000 419952\n",
      "400000 419952\n",
      "CPU times: user 58.4 s, sys: 642 ms, total: 59.1 s\n",
      "Wall time: 59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_unique = data_pd[\"target\"].nunique()\n",
    "target_accumulation = []\n",
    "for index, (_, group) in enumerate(data_pd[[\"target\", \"amount\"]].groupby(\"target\")):\n",
    "    group.loc[:, \"target_accumulation\"] = group[\"amount\"].cumsum()\n",
    "    target_accumulation.append(group)\n",
    "    if not (index % 200_000):\n",
    "        print(index, num_unique)\n",
    "target_accumulation = pd.concat(target_accumulation, ignore_index=False)\n",
    "target_accumulation.to_parquet(location_target_accumulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a89af77b-0ce6-4208-a24f-f0cd74ebe859",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_accumulation = pd.read_parquet(location_target_accumulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2ee7449-5276-483f-a3a0-ed93396a260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd = source_dispensation[[\"source_dispensation\"]].join(\n",
    "    target_accumulation[[\"target_accumulation\"]], how=\"outer\"\n",
    ").join(data_pd)\n",
    "data_pd.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15ceca15-3334-4695-86ff-f93b980c18bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.1 s, sys: 207 ms, total: 12.3 s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dispensation_mapping = {}\n",
    "for source, group in data_pd[[\"source\", \"source_dispensation\"]].groupby(\"source\"):\n",
    "    dispensation_mapping[source] = (group.index.tolist(), group[\"source_dispensation\"].tolist())\n",
    "\n",
    "accumulation_mapping = {}\n",
    "for target, group in data_pd[[\"target\", \"target_accumulation\"]].groupby(\"target\"):\n",
    "    accumulation_mapping[target] = (group.index.tolist(), group[\"target_accumulation\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "105ba11e-acd0-4d6d-88e8-54cec8a457e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dis_acc_data(node, mapping_dis, mapping_acc, trx_id):\n",
    "    data_dis = mapping_dis.get(node)\n",
    "    if data_dis is None:\n",
    "        data_acc = mapping_acc[node]\n",
    "        index_acc = bisect.bisect_right(data_acc[0], trx_id)\n",
    "        if index_acc:\n",
    "            index_acc -= 1\n",
    "        else:\n",
    "            return 0, 0\n",
    "        return 0, data_acc[1][index_acc]\n",
    "    data_acc = mapping_acc.get(node)\n",
    "    if data_acc is None:\n",
    "        data_dis = mapping_dis[node]\n",
    "        index_dis = bisect.bisect_right(data_dis[0], trx_id)\n",
    "        if index_dis:\n",
    "            index_dis -= 1\n",
    "        else:\n",
    "            return 0, 0\n",
    "        return data_dis[1][index_dis], 0\n",
    "    index_dis = bisect.bisect_right(data_dis[0], trx_id)\n",
    "    index_acc = bisect.bisect_right(data_acc[0], trx_id)\n",
    "    so_far_dispensed = 0\n",
    "    if index_dis:\n",
    "        index_dis -= 1\n",
    "        so_far_dispensed = data_dis[1][index_dis]\n",
    "    so_far_accumulated = 0\n",
    "    if index_acc:\n",
    "        index_acc -= 1\n",
    "        so_far_accumulated = data_acc[1][index_acc]\n",
    "    return so_far_dispensed, so_far_accumulated\n",
    "\n",
    "\n",
    "def source_dis_acc_data(row):\n",
    "    return get_dis_acc_data(row[\"source\"], dispensation_mapping, accumulation_mapping, row.name)\n",
    "\n",
    "\n",
    "def target_dis_acc_data(row):\n",
    "    return get_dis_acc_data(row[\"target\"], dispensation_mapping, accumulation_mapping, row.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0986161-3c83-4bd6-93cf-cd7708698a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.1 s, sys: 900 ms, total: 51 s\n",
      "Wall time: 51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_pd.loc[:, \"dis_acc_source\"] = data_pd.apply(source_dis_acc_data, axis=1)\n",
    "data_pd.loc[:, \"dis_acc_target\"] = data_pd.apply(target_dis_acc_data, axis=1)\n",
    "\n",
    "data_pd.loc[:, \"source_positive_balance\"] = data_pd.loc[:, \"dis_acc_source\"].apply(\n",
    "    lambda x: x[1] - x[0] if x[1] > x[0] else 0\n",
    ")\n",
    "data_pd.loc[:, \"source_negative_balance\"] = data_pd.loc[:, \"dis_acc_source\"].apply(\n",
    "    lambda x: x[0] - x[1] if x[0] > x[1] else 0\n",
    ")\n",
    "data_pd.loc[:, \"target_positive_balance\"] = data_pd.loc[:, \"dis_acc_target\"].apply(\n",
    "    lambda x: x[1] - x[0] if x[1] > x[0] else 0\n",
    ")\n",
    "data_pd.loc[:, \"target_negative_balance\"] = data_pd.loc[:, \"dis_acc_target\"].apply(\n",
    "    lambda x: x[0] - x[1] if x[0] > x[1] else 0\n",
    ")\n",
    "\n",
    "del data_pd[\"dis_acc_source\"]\n",
    "del data_pd[\"dis_acc_target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6b341ae-62a0-4238-a564-c159c4b77aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.5 s, sys: 1.01 s, total: 56.5 s\n",
      "Wall time: 56.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "source_firsts = data_pd.groupby(\"source\").agg(first_trx=(\"timestamp\", \"min\"))\n",
    "target_firsts = data_pd.groupby(\"target\").agg(first_trx=(\"timestamp\", \"min\"))\n",
    "active_since = source_firsts.join(target_firsts, lsuffix=\"_left\", how=\"outer\").fillna(datetime.now())\n",
    "active_since.loc[:, \"active_since\"] = active_since.apply(lambda x: min([x[\"first_trx_left\"], x[\"first_trx\"]]), axis=1)\n",
    "active_since = active_since.loc[:, [\"active_since\"]]\n",
    "active_since.sort_values(\"active_since\", inplace=True)\n",
    "\n",
    "active_since = active_since[\"active_since\"].to_dict()\n",
    "last_trx_ts = data_pd[\"timestamp\"].max() + timedelta(hours=1)\n",
    "first_trx_ts = data_pd[\"timestamp\"].min() - timedelta(hours=1)\n",
    "active_for = {k : (last_trx_ts - v).total_seconds() for k, v in active_since.items()}\n",
    "\n",
    "data_pd.loc[:, \"source_active_for\"] = data_pd.apply(\n",
    "    lambda x: (x[\"timestamp\"] - active_since[x[\"source\"]]).total_seconds(), axis=1\n",
    ")\n",
    "data_pd.loc[:, \"target_active_for\"] = data_pd.apply(\n",
    "    lambda x: (x[\"timestamp\"] - active_since[x[\"target\"]]).total_seconds(), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fc122b7-70a9-42b4-adab-8c3240df6c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/16 19:59:50 WARN TaskSetManager: Stage 44 contains a task of very large size (1921 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(data_pd).repartition(TRX_PARTITIONS).write.parquet(s.STAGED_DATA_LOCATION, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b05f001e-4e47-40a3-9de2-b4a534c05a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet(s.STAGED_DATA_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15609486-de3c-4470-928e-612aa35a08e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "assert data.count() == data.select(\"transaction_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4276821f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5072693, 3209, 3209, 370)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count(), cases_data.shape[0], cases_data[\"transaction_id\"].nunique(), cases_data[\"id\"].nunique()\n",
    "# (179504480, 137936, 137933, 16467)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102f290-c8e0-4064-86e8-a89a42e1628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score, recall_score, RocCurveDisplay\n",
    "\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "import settings as s\n",
    "\n",
    "assert s.FILE_SIZE == \"Small\"\n",
    "assert s.HIGH_ILLICIT == False\n",
    "\n",
    "os.environ[\"EXT_DATA_TYPE_FOLDER\"] = s.OUTPUT_POSTFIX.lstrip(\"-\")\n",
    "\n",
    "from common import get_weights, delete_large_vars, MULTI_PROC_STAGING_LOCATION\n",
    "from communities import get_communities_spark\n",
    "from features import (\n",
    "    generate_features_spark, generate_features_udf_wrapper, get_edge_features_udf,\n",
    "    SCHEMA_FEAT_UDF, CURRENCY_RATES\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01648199-82fc-4bec-8248-828135bd0af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = int(os.environ.get(\"EXSTRAQT_SEED\", 42))\n",
    "print(f\"{SEED=}\")\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cbd846-2921-4802-b8b8-38d64092cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    sys.version_info.major,\n",
    "    sys.version_info.minor,\n",
    "    sys.version_info.micro,\n",
    ") != (3, 11, 8):\n",
    "    raise EnvironmentError(\n",
    "        \"Only runs efficiently on Python 3.11.8\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb8939c-e2fc-4bfb-ae18-ef1eec4eafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_CONF = [\n",
    "    (\"spark.driver.memory\", \"32g\"),\n",
    "    (\"spark.worker.memory\", \"32g\"),\n",
    "    (\"spark.driver.maxResultSize\", \"32g\"),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"true\"),\n",
    "    (\"spark.network.timeout\", \"600s\"),\n",
    "    (\"spark.sql.autoBroadcastJoinThreshold\", -1),\n",
    "    (\"spark.local.dir\", f\".{os.sep}temp-spark\"),\n",
    "]\n",
    "\n",
    "if \"EXSTRAQT_SEED\" in os.environ:\n",
    "    SPARK_CONF.append((\"spark.log.level\", \"ERROR\"))\n",
    "\n",
    "shutil.rmtree(\"artifacts\", ignore_errors=True)\n",
    "shutil.rmtree(\"temp-spark\", ignore_errors=True)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"testing\")\n",
    "    .config(conf=SparkConf().setAll(SPARK_CONF))\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b1fa7-53c8-4dbb-b25f-795aaca78a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PERC = 0.64\n",
    "VALIDATION_PERC = 0.19\n",
    "TEST_PERC = 0.17\n",
    "\n",
    "KEEP_TOP_N = 100\n",
    "\n",
    "assert(sum([TRAIN_PERC, VALIDATION_PERC, TEST_PERC]) == 1)\n",
    "\n",
    "location_main = os.path.join(\"features\", os.environ[\"EXT_DATA_TYPE_FOLDER\"])\n",
    "# shutil.rmtree(location_main, ignore_errors=True)\n",
    "\n",
    "location_communities_leiden = f\"{location_main}{os.sep}communities_leiden.parquet\"\n",
    "\n",
    "location_features_leiden = f\"{location_main}{os.sep}features_leiden.parquet\"\n",
    "location_features_ego = f\"{location_main}{os.sep}features_ego.parquet\"\n",
    "location_features_2_hop = f\"{location_main}{os.sep}features_2_hop.parquet\"\n",
    "location_features_2_hop_out = f\"{location_main}{os.sep}features_2_hop_out.parquet\"\n",
    "location_features_2_hop_in = f\"{location_main}{os.sep}features_2_hop_in.parquet\"\n",
    "location_features_2_hop_combined = f\"{location_main}{os.sep}features_2_hop_combined.parquet\"\n",
    "location_features_source = f\"{location_main}{os.sep}features_source.parquet\"\n",
    "location_features_target = f\"{location_main}{os.sep}features_target.parquet\"\n",
    "\n",
    "location_flow_dispense = f\"{location_main}{os.sep}location_flow_dispense.parquet\"\n",
    "location_flow_passthrough = f\"{location_main}{os.sep}location_flow_passthrough.parquet\"\n",
    "location_flow_sink = f\"{location_main}{os.sep}location_flow_sink.parquet\"\n",
    "\n",
    "location_comm_as_source_features = f\"{location_main}{os.sep}comm_as_source_features.parquet\"\n",
    "location_comm_as_target_features = f\"{location_main}{os.sep}comm_as_target_features.parquet\"\n",
    "location_comm_as_passthrough_features = f\"{location_main}{os.sep}comm_as_passthrough_features.parquet\"\n",
    "location_comm_as_passthrough_features_reverse = f\"{location_main}{os.sep}comm_as_passthrough_features_reverse.parquet\"\n",
    "\n",
    "location_features_node_level = f\"{location_main}{os.sep}features_node_level.parquet\"\n",
    "location_features_edges = f\"{location_main}{os.sep}features_edges.parquet\"\n",
    "\n",
    "location_features_edges_train = f\"{location_main}{os.sep}features_edges_train.parquet\"\n",
    "location_features_edges_valid = f\"{location_main}{os.sep}features_edges_valid.parquet\"\n",
    "location_features_edges_test = f\"{location_main}{os.sep}features_edges_test.parquet\"\n",
    "\n",
    "location_train_trx_features = f\"{location_main}{os.sep}train_trx_features.parquet\"\n",
    "location_valid_trx_features = f\"{location_main}{os.sep}valid_trx_features.parquet\"\n",
    "location_test_trx_features = f\"{location_main}{os.sep}test_trx_features.parquet\"\n",
    "\n",
    "location_train_features = f\"{location_main}{os.sep}train_features.parquet\"\n",
    "location_valid_features = f\"{location_main}{os.sep}valid_features.parquet\"\n",
    "location_test_features = f\"{location_main}{os.sep}test_features.parquet\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(location_main)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82546745-6577-4e38-bf44-1a6114c3a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet(s.STAGED_DATA_LOCATION)\n",
    "data = data.withColumn(\"is_laundering\", sf.col(\"is_laundering\").cast(\"boolean\"))\n",
    "data_count_original = data.count()\n",
    "\n",
    "# Probably not used in the benchmarks\n",
    "data = data.drop(\"source_entity\", \"target_entity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc12ed-f4c5-4f29-90e8-cfa20886c41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.drop(\"source\", \"target\")\n",
    "# data = data.withColumnRenamed(\"source_entity\", \"source\")\n",
    "# data = data.withColumnRenamed(\"target_entity\", \"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5d7e4-1b66-4e93-b67b-a947d8c8e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "trx_ids_sorted = data.sort(\"timestamp\").select(\"transaction_id\").toPandas()[\"transaction_id\"].values\n",
    "trx_count = len(trx_ids_sorted)\n",
    "\n",
    "last_train_index = int(np.floor(trx_count * TRAIN_PERC))\n",
    "last_validation_index = last_train_index + int(np.floor(trx_count * VALIDATION_PERC))\n",
    "train_indexes = trx_ids_sorted[:last_train_index]\n",
    "validation_indexes = trx_ids_sorted[last_train_index:last_validation_index]\n",
    "test_indexes = trx_ids_sorted[last_validation_index:]\n",
    "\n",
    "train_indexes_loc = os.path.join(location_main, \"temp_train_indexes.parquet\")\n",
    "validation_indexes_loc = os.path.join(location_main, \"temp_validation_indexes.parquet\")\n",
    "test_indexes_loc = os.path.join(location_main, \"temp_test_indexes.parquet\")\n",
    "\n",
    "pd.DataFrame(train_indexes, columns=[\"transaction_id\"]).to_parquet(train_indexes_loc)\n",
    "pd.DataFrame(validation_indexes, columns=[\"transaction_id\"]).to_parquet(validation_indexes_loc)\n",
    "pd.DataFrame(test_indexes, columns=[\"transaction_id\"]).to_parquet(test_indexes_loc)\n",
    "\n",
    "train_indexes = spark.read.parquet(train_indexes_loc)\n",
    "validation_indexes = spark.read.parquet(validation_indexes_loc)\n",
    "test_indexes = spark.read.parquet(test_indexes_loc)\n",
    "\n",
    "train = train_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "validation = validation_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "test = test_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "train_count, validation_count, test_count = train.count(), validation.count(), test.count()\n",
    "print()\n",
    "print(trx_count, train_count, validation_count, test_count)\n",
    "print()\n",
    "\n",
    "os.remove(train_indexes_loc)\n",
    "os.remove(validation_indexes_loc)\n",
    "os.remove(test_indexes_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d1078e-cc6f-4574-a729-28d81fb4fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_edge_features(input_data):\n",
    "    print(f\"Generating edge features\")\n",
    "    to_select = [\"source\", \"target\", \"format\", \"source_currency\", \"source_amount\", \"amount\", \"timestamp\"]\n",
    "    edges_features_input = input_data.select(*to_select).groupby(\n",
    "        [\"source\", \"target\", \"format\", \"source_currency\"]\n",
    "    ).agg(\n",
    "        sf.sum(\"source_amount\").alias(\"source_amount\"), \n",
    "        sf.sum(\"amount\").alias(\"amount\"),\n",
    "        sf.unix_timestamp(sf.min(\"timestamp\")).alias(\"min_ts\"),\n",
    "        sf.unix_timestamp(sf.max(\"timestamp\")).alias(\"max_ts\"),\n",
    "    ).repartition(os.cpu_count() * 2, \"source\", \"target\").persist(StorageLevel.DISK_ONLY)\n",
    "    _ = edges_features_input.count()\n",
    "    edge_features = edges_features_input.groupby([\"source\", \"target\"]).applyInPandas(\n",
    "        get_edge_features_udf, schema=SCHEMA_FEAT_UDF\n",
    "    ).toPandas()\n",
    "    edge_features = pd.DataFrame(edge_features[\"features\"].apply(json.loads).tolist())\n",
    "    edge_features.to_parquet(location_features_edges)\n",
    "    del edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce68e646-67f2-4158-ba72-528c94207d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_node_features_to_edges(features_in, location):\n",
    "    features_in = features_in.set_index(\"target\").join(\n",
    "        pd.read_parquet(location_features_node_level), how=\"left\", rsuffix=\"_target\"\n",
    "    ).reset_index().set_index(\"source\").join(\n",
    "        pd.read_parquet(location_features_node_level), how=\"left\", rsuffix=\"_source\"\n",
    "    ).reset_index()\n",
    "\n",
    "    features_in.loc[:, \"anomaly_scores_diff\"] = features_in.loc[:, \"anomaly_score\"] - features_in.loc[:, \"anomaly_score_source\"]\n",
    "    features_in.loc[:, \"anomaly_scores_min\"] = np.array(\n",
    "        [\n",
    "            features_in.loc[:, \"anomaly_score\"].values, \n",
    "            features_in.loc[:, \"anomaly_score_source\"].values\n",
    "        ],\n",
    "    ).min(axis=0)\n",
    "    features_in.loc[:, \"anomaly_scores_max\"] = np.array(\n",
    "        [\n",
    "            features_in.loc[:, \"anomaly_score\"].values, \n",
    "            features_in.loc[:, \"anomaly_score_source\"].values\n",
    "        ],\n",
    "    ).max(axis=0)\n",
    "    features_in.loc[:, \"anomaly_scores_mean\"] = np.array(\n",
    "        [\n",
    "            features_in.loc[:, \"anomaly_score\"].values, \n",
    "            features_in.loc[:, \"anomaly_score_source\"].values\n",
    "        ],\n",
    "    ).mean(axis=0)\n",
    "\n",
    "    features_in.to_parquet(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4364bc-eb49-4d03-b85b-8ff293a88fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trx_features(data_in, location):\n",
    "    columns = [\n",
    "        \"source\", \"target\", \"source_currency\", \"target_currency\", \"format\", \"amount\", \n",
    "        \"source_dispensation\",\n",
    "        \"target_accumulation\",\n",
    "        \"source_positive_balance\",\n",
    "        \"source_negative_balance\",\n",
    "        \"target_positive_balance\",\n",
    "        \"target_negative_balance\",\n",
    "        \"source_active_for\",\n",
    "        \"target_active_for\",\n",
    "        \"is_laundering\"\n",
    "    ]\n",
    "    trx_features = data_in.select(*columns).toPandas()\n",
    "    trx_features.loc[:, \"source_balance_ratio\"] = (\n",
    "        trx_features[\"source_positive_balance\"] / trx_features[\"source_negative_balance\"]\n",
    "    ).fillna(0).replace(np.inf, 0)\n",
    "    trx_features.loc[:, \"target_balance_ratio\"] = (\n",
    "        trx_features[\"target_positive_balance\"] / trx_features[\"target_negative_balance\"]\n",
    "    ).fillna(0).replace(np.inf, 0)\n",
    "    trx_features.loc[:, \"inter_currency\"] = trx_features[\"source_currency\"] != trx_features[\"target_currency\"]\n",
    "    trx_features = pd.get_dummies(trx_features, columns=[\"source_currency\", \"target_currency\", \"format\"], drop_first=False)\n",
    "    trx_features.to_parquet(location)\n",
    "    del trx_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c145f2-5598-4551-b0c1-497cb1f1ae41",
   "metadata": {},
   "source": [
    "# [To prevent data leakage]\n",
    "\n",
    "### As the `train`, `validation`, and `test` sets are split in chronological order:\n",
    "* `train` features are constructed, based on a **graph** (containing data), up till the last training record\n",
    "* `validation` features are constructed, ..., up till the last validation record\n",
    "* `train` features are constructed, ..., up till the last test record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4ecbd-1487-4b07-b7c6-1fb22e3bf4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data = train.select(\"*\")\n",
    "print(f\"Constructing node-level features for `train` data: {data.count():,}\")\n",
    "\n",
    "%run node_level_features.ipynb\n",
    "\n",
    "generate_edge_features(data)\n",
    "train_edges = train.select(\"source\", \"target\").drop_duplicates().toPandas().set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "train_features = train_edges.join(\n",
    "    pd.read_parquet(location_features_edges).set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "add_node_features_to_edges(train_features, location_features_edges_train)\n",
    "save_trx_features(train, location_train_trx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5e1408-4d7a-4547-baaa-0cac6952106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data = train.union(validation).select(\"*\")\n",
    "print(f\"Constructing node-level features for `validation` data: {data.count():,}\")\n",
    "\n",
    "%run node_level_features.ipynb\n",
    "\n",
    "generate_edge_features(data)\n",
    "validation_edges = validation.select(\"source\", \"target\").drop_duplicates().toPandas().set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "validation_features = validation_edges.join(\n",
    "    pd.read_parquet(location_features_edges).set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "add_node_features_to_edges(validation_features, location_features_edges_valid)\n",
    "save_trx_features(validation, location_valid_trx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec78fa4-1bb2-4233-afe9-e7e445302c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data = train.union(validation).union(test).select(\"*\")\n",
    "print(f\"Constructing node-level features for `test` data: {data.count():,}\")\n",
    "\n",
    "%run node_level_features.ipynb\n",
    "\n",
    "generate_edge_features(data)\n",
    "test_edges = test.select(\"source\", \"target\").drop_duplicates().toPandas().set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "test_features = test_edges.join(\n",
    "    pd.read_parquet(location_features_edges).set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "add_node_features_to_edges(test_features, location_features_edges_test)\n",
    "save_trx_features(test, location_test_trx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6eaaec-8af4-43d7-8127-a306b855f430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To free up memory for training\n",
    "\n",
    "to_reset = %who_ls\n",
    "to_reset = list(to_reset)\n",
    "to_reset.remove(\"to_keep\")\n",
    "to_reset = set(to_reset) - set(to_keep)\n",
    "for var_to_reset in list(to_reset):\n",
    "    var_to_reset = f\"^{var_to_reset}$\"\n",
    "    %reset_selective -f {var_to_reset}\n",
    "\n",
    "delete_large_vars(globals(), locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c3a17d-14f9-4d34-9103-0c7e2e8c064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(location_features_trx, location_features_edges, location_features):\n",
    "    features_input = spark.read.parquet(location_features_edges)\n",
    "    trx_features_input = spark.read.parquet(location_features_trx)\n",
    "    features_input = trx_features_input.join(\n",
    "        features_input,\n",
    "        on=[\"source\", \"target\"],\n",
    "        how=\"left\"\n",
    "    ).drop(\"source\", \"target\")\n",
    "    features_input.write.parquet(location_features, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96867e2d-ceed-4176-b2f2-cb407c592ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "combine_features(location_train_trx_features, location_features_edges_train, location_train_features)\n",
    "combine_features(location_valid_trx_features, location_features_edges_valid, location_valid_features)\n",
    "combine_features(location_test_trx_features, location_features_edges_test, location_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8924af3-4f88-48dc-9e82-ab345e77f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(MULTI_PROC_STAGING_LOCATION, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c2ca47-4a63-4e85-aa7f-ff6746a2618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_parquet(location_train_features)\n",
    "validation_features = pd.read_parquet(location_valid_features)\n",
    "test_features = pd.read_parquet(location_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d123edbe-93c9-4357-8bd3-1a341a95a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = set(train_features.columns) | set(validation_features.columns) | set(test_features.columns)\n",
    "\n",
    "for missing in (\n",
    "    all_columns.symmetric_difference(train_features.columns) |\n",
    "    all_columns.symmetric_difference(validation_features.columns) |\n",
    "    all_columns.symmetric_difference(test_features.columns)\n",
    "):\n",
    "    if missing in train_features.columns:\n",
    "        print(f\"Deleting {missing} from train\")\n",
    "        del train_features[missing]\n",
    "    if missing in validation_features.columns:\n",
    "        print(f\"Deleting {missing} from validation\")\n",
    "        del validation_features[missing]\n",
    "    if missing in test_features.columns:\n",
    "        print(f\"Deleting {missing} from test\")\n",
    "        del test_features[missing]\n",
    "\n",
    "validation_features = validation_features.loc[:, list(train_features.columns)]\n",
    "test_features = test_features.loc[:, list(train_features.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7f7de4-2df8-490f-8daa-b95980fd9686",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_features.shape[0] == train_count\n",
    "assert validation_features.shape[0] == validation_count\n",
    "assert test_features.shape[0] == test_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb7276-f84a-4468-bd82-a2fcce9132c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_labels = train_features.loc[:, [\"is_laundering\"]].copy(deep=True)\n",
    "del train_features[\"is_laundering\"]\n",
    "\n",
    "validation_features_labels = validation_features.loc[:, [\"is_laundering\"]].copy(deep=True)\n",
    "del validation_features[\"is_laundering\"]\n",
    "\n",
    "test_features_labels = test_features.loc[:, [\"is_laundering\"]].copy(deep=True)\n",
    "del test_features[\"is_laundering\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2dbe51-59bf-4e8e-a132-c4c3121ae717",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_available = False\n",
    "try:\n",
    "    import torch\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "xgb_args = dict(\n",
    "    early_stopping_rounds=10, scale_pos_weight=3,\n",
    "    eval_metric=\"aucpr\", \n",
    "    disable_default_eval_metric=True, \n",
    "    num_parallel_tree=10, max_depth=6,\n",
    "    colsample_bytree=0.5, subsample=1, \n",
    "    eta=0.05,\n",
    "    device=\"cpu\", nthread=10,\n",
    "    n_estimators=100, seed=SEED,\n",
    ")\n",
    "if cuda_available:\n",
    "    xgb_args[\"device\"] = \"cuda\"\n",
    "    xgb_args[\"nthread\"] = 2\n",
    "\n",
    "xgb_fit_args = {\n",
    "    \"eval_set\": [(validation_features, validation_features_labels[\"is_laundering\"].values)],\n",
    "    \"verbose\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15622322-48f4-4051-8f49-d8ea9476feda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = xgb.XGBClassifier(**xgb_args)\n",
    "model.fit(\n",
    "    train_features, train_features_labels[\"is_laundering\"].values, \n",
    "    **xgb_fit_args\n",
    ")\n",
    "y_test_predicted = model.predict(test_features)\n",
    "f1_test = f1_score(test_features_labels[\"is_laundering\"], y_test_predicted) * 100\n",
    "print(\n",
    "    f\"{SEED=}\",\n",
    "    f\"{model.best_iteration=}\",\n",
    "    f\"f1={round(f1_test, 2)}\",\n",
    "    f\"recall={round(recall_score(test_features_labels['is_laundering'], y_test_predicted) * 100, 2)}\",\n",
    ")\n",
    "print(f1_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

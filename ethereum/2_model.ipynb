{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8f1259-7f64-4919-93ab-93357d171d49",
   "metadata": {},
   "source": [
    "### https://arxiv.org/pdf/2402.08593"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5102f290-c8e0-4064-86e8-a89a42e1628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta, datetime\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score, recall_score, RocCurveDisplay\n",
    "\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "import settings as s\n",
    "\n",
    "os.environ[\"EXT_DATA_TYPE_FOLDER\"] = \"ethereum\"\n",
    "\n",
    "from common import get_weights, delete_large_vars, MULTI_PROC_STAGING_LOCATION\n",
    "from communities import get_communities_spark\n",
    "from features import (\n",
    "    generate_features_spark, generate_features_udf_wrapper,\n",
    "    SCHEMA_FEAT_UDF\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdfc52ce-1f9d-49d2-81f7-366fcdab6f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_script = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3cbd846-2921-4802-b8b8-38d64092cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    sys.version_info.major,\n",
    "    sys.version_info.minor,\n",
    "    sys.version_info.micro,\n",
    ") != (3, 11, 3):\n",
    "    raise EnvironmentError(\n",
    "        \"Only runs efficiently on Python 3.11.3\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adb8939c-e2fc-4bfb-ae18-ef1eec4eafc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/17 22:25:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/17 22:25:02 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "SPARK_CONF = [\n",
    "    (\"spark.driver.memory\", \"32g\"),\n",
    "    (\"spark.worker.memory\", \"32g\"),\n",
    "    (\"spark.driver.maxResultSize\", \"32g\"),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"true\"),\n",
    "    (\"spark.network.timeout\", \"600s\"),\n",
    "    (\"spark.sql.autoBroadcastJoinThreshold\", -1),\n",
    "    (\"spark.local.dir\", f\".{os.sep}spark-temp\")\n",
    "]\n",
    "\n",
    "shutil.rmtree(\"artifacts\", ignore_errors=True)\n",
    "shutil.rmtree(\"spark-temp\", ignore_errors=True)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"testing\")\n",
    "    .config(conf=SparkConf().setAll(SPARK_CONF))\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "320b1fa7-53c8-4dbb-b25f-795aaca78a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PERC = 0.65\n",
    "VALIDATION_PERC = 0.15\n",
    "TEST_PERC = 0.2\n",
    "\n",
    "KEEP_TOP_N = 100\n",
    "\n",
    "assert(sum([TRAIN_PERC, VALIDATION_PERC, TEST_PERC]) == 1)\n",
    "\n",
    "location_main = os.path.join(\"features\", os.environ[\"EXT_DATA_TYPE_FOLDER\"])\n",
    "# shutil.rmtree(location_main, ignore_errors=True)\n",
    "\n",
    "location_communities_leiden = f\"{location_main}{os.sep}communities_leiden.parquet\"\n",
    "\n",
    "location_features_leiden = f\"{location_main}{os.sep}features_leiden.parquet\"\n",
    "location_features_ego = f\"{location_main}{os.sep}features_ego.parquet\"\n",
    "location_features_2_hop = f\"{location_main}{os.sep}features_2_hop.parquet\"\n",
    "location_features_2_hop_out = f\"{location_main}{os.sep}features_2_hop_out.parquet\"\n",
    "location_features_2_hop_in = f\"{location_main}{os.sep}features_2_hop_in.parquet\"\n",
    "location_features_2_hop_combined = f\"{location_main}{os.sep}features_2_hop_combined.parquet\"\n",
    "location_features_source = f\"{location_main}{os.sep}features_source.parquet\"\n",
    "location_features_target = f\"{location_main}{os.sep}features_target.parquet\"\n",
    "\n",
    "location_flow_dispense = f\"{location_main}{os.sep}location_flow_dispense.parquet\"\n",
    "location_flow_passthrough = f\"{location_main}{os.sep}location_flow_passthrough.parquet\"\n",
    "location_flow_sink = f\"{location_main}{os.sep}location_flow_sink.parquet\"\n",
    "\n",
    "location_comm_as_source_features = f\"{location_main}{os.sep}comm_as_source_features.parquet\"\n",
    "location_comm_as_target_features = f\"{location_main}{os.sep}comm_as_target_features.parquet\"\n",
    "location_comm_as_passthrough_features = f\"{location_main}{os.sep}comm_as_passthrough_features.parquet\"\n",
    "location_comm_as_passthrough_features_reverse = f\"{location_main}{os.sep}comm_as_passthrough_features_reverse.parquet\"\n",
    "\n",
    "location_features_node_level = f\"{location_main}{os.sep}features_node_level.parquet\"\n",
    "location_features_edges = f\"{location_main}{os.sep}features_edges.parquet\"\n",
    "\n",
    "location_features_edges_train = f\"{location_main}{os.sep}features_edges_train.parquet\"\n",
    "location_features_edges_valid = f\"{location_main}{os.sep}features_edges_valid.parquet\"\n",
    "location_features_edges_test = f\"{location_main}{os.sep}features_edges_test.parquet\"\n",
    "\n",
    "location_train_trx_features = f\"{location_main}{os.sep}train_trx_features.parquet\"\n",
    "location_valid_trx_features = f\"{location_main}{os.sep}valid_trx_features.parquet\"\n",
    "location_test_trx_features = f\"{location_main}{os.sep}test_trx_features.parquet\"\n",
    "\n",
    "location_train_features = f\"{location_main}{os.sep}train_features.parquet\"\n",
    "location_valid_features = f\"{location_main}{os.sep}valid_features.parquet\"\n",
    "location_test_features = f\"{location_main}{os.sep}test_features.parquet\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(location_main)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82546745-6577-4e38-bf44-1a6114c3a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(s.INPUT_DATA_FILE)\n",
    "# Only interested when \"target\" is phishing\n",
    "phishing_nodes = set(data.loc[data[\"is_phishing\"], \"target\"].unique())\n",
    "assert len(phishing_nodes) == 1164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57b5d7e4-1b66-4e93-b67b-a947d8c8e06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,932,767 | 446,023 | 594,699\n",
      "CPU times: user 1min 23s, sys: 793 ms, total: 1min 23s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "source_firsts = data.groupby(\"source\").agg(first_trx=(\"timestamp\", \"min\"))\n",
    "target_firsts = data.groupby(\"target\").agg(first_trx=(\"timestamp\", \"min\"))\n",
    "active_since = source_firsts.join(target_firsts, lsuffix=\"_left\", how=\"outer\").fillna(datetime.now())\n",
    "active_since.loc[:, \"active_since\"] = active_since.apply(lambda x: min([x[\"first_trx_left\"], x[\"first_trx\"]]), axis=1)\n",
    "active_since = active_since.loc[:, [\"active_since\"]]\n",
    "active_since.sort_values(\"active_since\", inplace=True)\n",
    "\n",
    "number_of_train_accounts = int(np.floor(active_since.shape[0] * TRAIN_PERC))\n",
    "number_of_validation_accounts = int(np.floor(active_since.shape[0] * VALIDATION_PERC))\n",
    "train_accounts = set(active_since.head(number_of_train_accounts).index.tolist())\n",
    "assert len(train_accounts) == number_of_train_accounts\n",
    "remaining = active_since.loc[~active_since.index.isin(train_accounts), :].sort_values(\"active_since\")\n",
    "validation_accounts = set(remaining.head(number_of_validation_accounts).index.tolist())\n",
    "assert len(validation_accounts) == number_of_validation_accounts\n",
    "test_accounts = set(active_since.index) - train_accounts - validation_accounts\n",
    "print(f\"{len(train_accounts):,} | {len(validation_accounts):,} | {len(test_accounts):,}\")\n",
    "assert sorted(train_accounts | validation_accounts | test_accounts) == sorted(active_since.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96f7e149-4992-4244-bc64-2058bd29829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74 0.05 0.03\n"
     ]
    }
   ],
   "source": [
    "train = data.loc[data[\"source\"].isin(train_accounts) & data[\"target\"].isin(train_accounts), :]\n",
    "validation = data.loc[data[\"source\"].isin(validation_accounts) & data[\"target\"].isin(validation_accounts), :]\n",
    "test = data.loc[data[\"source\"].isin(test_accounts) & data[\"target\"].isin(test_accounts), :]\n",
    "print(\n",
    "    round(train.shape[0] / data.shape[0], 2), \n",
    "    round(validation.shape[0] / data.shape[0], 2), \n",
    "    round(test.shape[0] / data.shape[0], 2)\n",
    ")\n",
    "train_count, validation_count, test_count = train.shape[0], validation.shape[0], test.shape[0]\n",
    "\n",
    "assert set(train.index).intersection(validation.index) == set()\n",
    "assert set(validation.index).intersection(test.index) == set()\n",
    "assert set(train.index).intersection(test.index) == set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6cc277c-cf84-487e-bcbd-0784a304cc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 2.19 s, total: 1min 24s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "edges = data.groupby([\"source\", \"target\"]).agg(\n",
    "    amount_usd=(\"amount_usd\", \"sum\")\n",
    ").reset_index()\n",
    "weights = get_weights(edges)\n",
    "edges_agg = edges.set_index([\"source\", \"target\"]).join(\n",
    "    weights.set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "edges_agg.loc[:, \"amount_usd_weighted\"] = (\n",
    "    edges_agg.loc[:, \"amount_usd\"] * \n",
    "    (edges_agg.loc[:, \"weight\"] / edges_agg.loc[:, \"weight\"].max())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "199fa227-2ea8-4639-a5ce-fe198af819ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later on, we will reset the variables (to free up memory), while still keeping these intact\n",
    "to_keep = %who_ls\n",
    "to_keep = list(to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a29ef0e-48b7-4b6f-9567-ea156739e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 50\n",
    "NUM_HOPS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26195fe6-5b17-4525-bf16-83c63098e7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_agg 5355155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing comm_as_source\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed hop #1 | 4,097,207 | 2,113,092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed hop #2 | 28,187,498 | 1,061,380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/17 12:23:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:23:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:23:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:23:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:23:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:23:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:23:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:23:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:23:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:23:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:23:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:23:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:23:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:23:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:23:34 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:23:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed hop #3 | 27,947,678 | 905,405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/17 12:26:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:26:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:26:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:26:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:26:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:26:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:26:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:26:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:26:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:26:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:26:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:26:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:26:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:26:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:26:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:26:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed hop #4 | 33,630,298 | 900,985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/17 12:31:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:31:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:31:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:31:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:31:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:31:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:31:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:31:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:31:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:31:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:31:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:31:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:31:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:31:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:31:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/08/17 12:31:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed hop #5 | 33,279,305 | 889,540\n",
      "\n",
      "Processing comm_as_target\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed hop #1 | 1,595,960 | 1,119,024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed hop #2 | 36,986,916 | 1,109,886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed hop #3 | 36,884,480 | 1,068,444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed hop #4 | 52,038,968 | 1,082,890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed hop #5 | 52,261,072 | 1,064,713\n",
      "\n",
      "Processing comm_as_passthrough\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed hop #1 | 1,214,666 | 258,627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed hop #2 | 5,402,821 | 182,194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed hop #3 | 5,449,649 | 159,934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed hop #4 | 6,521,836 | 157,566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed hop #5 | 6,354,889 | 156,225\n",
      "\n",
      "Processing comm_as_passthrough_reverse\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed hop #1 | 537,548 | 258,628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed hop #2 | 8,850,094 | 250,407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed hop #3 | 9,263,090 | 243,777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed hop #4 | 11,708,036 | 246,105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed hop #5 | 12,045,187 | 242,996\n",
      "\n",
      "\n",
      "comm_as_source_features\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 s, sys: 1.73 s, total: 12.8 s\n",
      "Wall time: 30.5 s\n",
      "\n",
      "comm_as_target_features\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.12 s, sys: 1 s, total: 7.12 s\n",
      "Wall time: 22.5 s\n",
      "\n",
      "comm_as_passthrough_features\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.55 s, sys: 531 ms, total: 4.08 s\n",
      "Wall time: 8.76 s\n",
      "\n",
      "comm_as_passthrough_features_reverse\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.56 s, sys: 1.26 s, total: 8.81 s\n",
      "Wall time: 14 s\n",
      "\n",
      "\n",
      "CPU times: user 50.8 s, sys: 8.26 s, total: 59.1 s\n",
      "Wall time: 31min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_input = spark.createDataFrame(edges_agg)\n",
    "nodes_source = set(edges_agg[\"source\"].unique())\n",
    "nodes_target = set(edges_agg[\"target\"].unique())\n",
    "nodes_passthrough = nodes_source.intersection(nodes_target)\n",
    "\n",
    "%run generate_flow_features.ipynb\n",
    "\n",
    "comm_as_source_features.to_parquet(location_comm_as_source_features)\n",
    "comm_as_target_features.to_parquet(location_comm_as_target_features)\n",
    "comm_as_passthrough_features.to_parquet(location_comm_as_passthrough_features)\n",
    "comm_as_passthrough_features_reverse.to_parquet(location_comm_as_passthrough_features_reverse)\n",
    "\n",
    "del comm_as_source_features\n",
    "del comm_as_target_features\n",
    "del comm_as_passthrough_features\n",
    "del comm_as_passthrough_features_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "986d0df2-1737-4f95-ad2b-a73c93bfd259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Leiden communities\n",
      "CPU times: user 2h 10min 31s, sys: 29.8 s, total: 2h 11min\n",
      "Wall time: 2h 10min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TODO: Use https://docs.rapids.ai/api/cugraph/legacy/api_docs/api/cugraph/cugraph.leiden/ ?\n",
    "\n",
    "print(\"Constructing Leiden communities\")\n",
    "\n",
    "graph = ig.Graph.DataFrame(edges_agg.loc[:, [\"source\", \"target\", \"amount_usd_weighted\"]], use_vids=False, directed=True)\n",
    "nodes_mapping = {x.index: x[\"name\"] for x in graph.vs()}\n",
    "communities_leiden = la.find_partition(\n",
    "    graph, la.ModularityVertexPartition, n_iterations=100, weights=\"amount_usd_weighted\"\n",
    ")\n",
    "communities_leiden = [[nodes_mapping[_] for _ in x] for x in communities_leiden]\n",
    "communities_leiden = [(str(uuid.uuid4()), set(x)) for x in communities_leiden]\n",
    "sizes_leiden = [len(x[1]) for x in communities_leiden]\n",
    "\n",
    "with open(location_communities_leiden, \"wb\") as fl:\n",
    "    pickle.dump(communities_leiden, fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26e3f6b2-1caa-4e11-b6ef-65d19f8cc156",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(location_communities_leiden, \"rb\") as fl:\n",
    "    communities_leiden = pickle.load(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90985833-5070-45bd-aa5e-b0f0846af1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0998de24-a007-40f3-9cad-0703940530c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2973489\n",
      "250000 2973489\n",
      "500000 2973489\n",
      "750000 2973489\n",
      "1000000 2973489\n",
      "1250000 2973489\n",
      "1500000 2973489\n",
      "1750000 2973489\n",
      "2000000 2973489\n",
      "2250000 2973489\n",
      "2500000 2973489\n",
      "2750000 2973489\n",
      "CPU times: user 6min 8s, sys: 14.4 s, total: 6min 23s\n",
      "Wall time: 14min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_agg_weights = get_weights(\n",
    "    data.groupby([\"source\", \"target\"])\n",
    "    .agg(\n",
    "        sf.sum(\"amount_usd\").alias(\"amount_usd\")\n",
    "    ).toPandas()\n",
    ")\n",
    "\n",
    "data_agg_weights_rev = data_agg_weights.rename(\n",
    "    columns={\"target\": \"source\", \"source\": \"target\"}\n",
    ").loc[:, [\"source\", \"target\", \"weight\"]]\n",
    "data_agg_weights_ud = pd.concat([data_agg_weights, data_agg_weights_rev], ignore_index=True)\n",
    "data_agg_weights_ud = data_agg_weights_ud.groupby([\"source\", \"target\"]).agg(weight=(\"weight\", \"sum\")).reset_index()\n",
    "\n",
    "data_agg_weights_ud.sort_values(\"weight\", ascending=False, inplace=True)\n",
    "grouped_ud = data_agg_weights_ud.groupby(\"source\").head(KEEP_TOP_N).reset_index(drop=True)\n",
    "grouped_ud = grouped_ud.groupby(\"source\").agg(targets=(\"target\", set))\n",
    "\n",
    "total = grouped_ud.index.nunique()\n",
    "nodes_neighborhoods = {}\n",
    "for index, (source, targets) in enumerate(grouped_ud.iterrows()):\n",
    "    community_candidates = {source}\n",
    "    for target in targets[\"targets\"]:\n",
    "        community_candidates |= (grouped_ud.loc[target, \"targets\"] | {target})\n",
    "    nodes_neighborhoods[source] = set(community_candidates)\n",
    "    if not (index % 250_000):\n",
    "        print(index, total)\n",
    "\n",
    "del data_agg_weights_rev\n",
    "del data_agg_weights_ud\n",
    "del grouped_ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e37941e-b6c3-478c-bd54-a16572719410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing 2-hop communities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 39s, sys: 5.17 s, total: 2min 44s\n",
      "Wall time: 21min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Constructing 2-hop communities\")\n",
    "\n",
    "communities_2_hop = get_communities_spark(\n",
    "    nodes_neighborhoods,\n",
    "    ig.Graph.DataFrame(edges_agg.loc[:, [\"source\", \"target\", \"amount_usd_weighted\"]], use_vids=False, directed=True), \n",
    "    os.cpu_count(), spark, 2, \"all\", 0.01, \"amount_usd_weighted\"\n",
    ")\n",
    "sizes_2_hop = [len(x[1]) for x in communities_2_hop]\n",
    "del nodes_neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d03955cf-3674-4fa6-bc72-90939be394e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5355155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.7 s, sys: 2.83 s, total: 34.5 s\n",
      "Wall time: 14min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ts_min = data.select(sf.min(\"timestamp\").alias(\"x\")).collect()[0][\"x\"] - timedelta(minutes=1)\n",
    "data_graph_agg = data.groupby([\"source\", \"target\"]).agg(\n",
    "    sf.sum(\"num_transactions\").alias(\"num_transactions\"),\n",
    "    sf.sum(\"amount_usd\").alias(\"amount_usd\"),\n",
    "    sf.collect_list(sf.array((sf.col(\"timestamp\") - ts_min).cast(\"long\"), sf.col(\"amount_usd\"))).alias(\"timestamps_amounts\"),\n",
    ")\n",
    "data_graph_agg_sdf = data_graph_agg.persist(StorageLevel.DISK_ONLY)\n",
    "print(data_graph_agg_sdf.count())\n",
    "data_graph_agg = data_graph_agg_sdf.toPandas()\n",
    "index = [\"source\", \"target\"]\n",
    "edges_agg.loc[:, index + [\"weight\"]].set_index(index)\n",
    "data_graph_agg = data_graph_agg.set_index(index).join(\n",
    "    edges_agg.loc[:, index + [\"weight\"]].set_index(index), how=\"left\"\n",
    ").reset_index()\n",
    "data_graph_agg.loc[:, \"amount_usd_weighted\"] = (\n",
    "    data_graph_agg.loc[:, \"amount_usd\"] * \n",
    "    (data_graph_agg.loc[:, \"weight\"] / data_graph_agg.loc[:, \"weight\"].max())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea905a1c-23e7-4f88-b340-59867f2b4bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = ig.Graph.DataFrame(data_graph_agg, use_vids=False, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74dce0c8-f8e0-446d-9bfe-e456911434f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leiden communitites features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/17 16:56:18 WARN TaskSetManager: Stage 20 contains a task of very large size (1031 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 24s, sys: 2.49 s, total: 1min 27s\n",
      "Wall time: 15min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Leiden communitites features creation\")\n",
    "\n",
    "communities_leiden = [(x, y) for x, y in communities_leiden if len(y) > 1]\n",
    "features_leiden = generate_features_spark(communities_leiden, data_graph_agg, spark)\n",
    "features_leiden = features_leiden.rename(columns={\"key\": \"key_fake\"})\n",
    "communities_leiden_dict = dict(communities_leiden)\n",
    "features_leiden.loc[:, \"key\"] = features_leiden.loc[:, \"key_fake\"].apply(lambda x: communities_leiden_dict[x])\n",
    "features_leiden = features_leiden.explode(\"key\")\n",
    "del features_leiden[\"key_fake\"]\n",
    "features_leiden.set_index(\"key\").to_parquet(location_features_leiden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66b1390f-e28d-4def-ae4a-f57766bcf83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-hop communitites features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/17 17:36:49 WARN TaskSetManager: Stage 31 contains a task of very large size (1031 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 9s, sys: 10.8 s, total: 3min 19s\n",
      "Wall time: 1h 8min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"2-hop communitites features creation\")\n",
    "\n",
    "features_2_hop = generate_features_spark(communities_2_hop, data_graph_agg, spark)\n",
    "features_2_hop.set_index(\"key\").to_parquet(location_features_2_hop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d51ba7de-7131-43c7-990e-03a97f4318a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4c4b527-dab6-4dee-9e19-64c9a74dc15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal flows features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/17 18:25:53 WARN TaskSetManager: Stage 42 contains a task of very large size (6438 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7874361 11725385\n",
      "dispense\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passthrough\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sink\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.9 s, sys: 658 ms, total: 18.5 s\n",
      "Wall time: 1h 6min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Temporal flows features creation\")\n",
    "\n",
    "edges_totals = data.select(\"source\", \"target\", \"amount_usd\").groupby(\n",
    "    [\"source\", \"target\"]\n",
    ").agg(sf.count(\"amount_usd\").alias(\"amount_usd\")).toPandas()\n",
    "edges_totals = edges_totals.sort_values(\"amount_usd\", ascending=False).reset_index(drop=True)\n",
    "left_edges = spark.createDataFrame(edges_totals.groupby(\"target\").head(TOP_N).loc[:, [\"source\", \"target\"]])\n",
    "right_edges = spark.createDataFrame(edges_totals.groupby(\"source\").head(TOP_N).loc[:, [\"source\", \"target\"]])\n",
    "\n",
    "columns = [\"source\", \"target\", \"timestamp\", \"amount_usd\"]\n",
    "\n",
    "left = left_edges.select(sf.col(\"source\").alias(\"src\"), sf.col(\"target\").alias(\"tgt\")).join(\n",
    "    data.select(*columns),\n",
    "    on=(sf.col(\"src\") == sf.col(\"source\")) & (sf.col(\"tgt\") == sf.col(\"target\")),\n",
    "    how=\"left\"\n",
    ").drop(\"src\", \"tgt\").persist(StorageLevel.DISK_ONLY)\n",
    "select = []\n",
    "for column in left.columns:\n",
    "    select.append(sf.col(column).alias(f\"left_{column}\"))\n",
    "left = left.select(*select)\n",
    "right = right_edges.select(sf.col(\"source\").alias(\"src\"), sf.col(\"target\").alias(\"tgt\")).join(\n",
    "    data.select(*columns),\n",
    "    on=(sf.col(\"src\") == sf.col(\"source\")) & (sf.col(\"tgt\") == sf.col(\"target\")),\n",
    "    how=\"left\"\n",
    ").drop(\"src\", \"tgt\").persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "print(left.count(), right.count())\n",
    "\n",
    "flows_temporal = left.join(\n",
    "    right,\n",
    "    (left[\"left_target\"] == right[\"source\"]) &\n",
    "    (left[\"left_timestamp\"] <= right[\"timestamp\"]),\n",
    "    how=\"inner\"\n",
    ").groupby([\"left_source\", \"left_target\", \"source\", \"target\"]).agg(\n",
    "    sf.sum(\"left_amount_usd\").alias(\"left_amount_usd\"),\n",
    "    sf.sum(\"amount_usd\").alias(\"amount_usd\"),\n",
    ").drop(\"left_target\").select(\n",
    "    sf.col(\"left_source\").alias(\"dispense\"),\n",
    "    sf.col(\"source\").alias(\"passthrough\"),\n",
    "    sf.col(\"target\").alias(\"sink\"),\n",
    "    sf.least(\"left_amount_usd\", \"amount_usd\").alias(\"amount_usd\"),\n",
    ")\n",
    "\n",
    "aggregate = [\n",
    "    sf.sum(\"amount_usd\").alias(\"amount_sum\"),\n",
    "    sf.mean(\"amount_usd\").alias(\"amount_mean\"),\n",
    "    sf.median(\"amount_usd\").alias(\"amount_median\"),\n",
    "    sf.max(\"amount_usd\").alias(\"amount_max\"),\n",
    "    sf.stddev(\"amount_usd\").alias(\"amount_std\"),\n",
    "    sf.countDistinct(\"dispense\").alias(\"dispense_count\"),\n",
    "    sf.countDistinct(\"passthrough\").alias(\"passthrough_count\"),\n",
    "    sf.countDistinct(\"sink\").alias(\"sink_count\"),\n",
    "]\n",
    "for flow_location, flow_type in [\n",
    "    (location_flow_dispense, \"dispense\"), \n",
    "    (location_flow_passthrough, \"passthrough\"), \n",
    "    (location_flow_sink, \"sink\")\n",
    "]:\n",
    "    print(flow_type)\n",
    "    flows_temporal_stats = flows_temporal.groupby(flow_type).agg(*aggregate).toPandas()\n",
    "    flows_temporal_cyclic_stats = flows_temporal.where(\n",
    "        (sf.col(\"dispense\") == sf.col(\"sink\"))\n",
    "    ).groupby(flow_type).agg(*aggregate).toPandas()\n",
    "    flows_temporal_stats = flows_temporal_stats.set_index(flow_type).join(\n",
    "        flows_temporal_cyclic_stats.set_index(flow_type),\n",
    "        how=\"left\", rsuffix=\"_cycle\"\n",
    "    )\n",
    "    flows_temporal_stats.index.name = \"key\"\n",
    "    flows_temporal_stats.to_parquet(flow_location)\n",
    "    del flows_temporal_stats\n",
    "    del flows_temporal_cyclic_stats\n",
    "\n",
    "left.unpersist()\n",
    "right.unpersist()\n",
    "\n",
    "del edges_totals\n",
    "del left_edges\n",
    "del right_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6839964f-cb70-4f50-97be-96b0acdd8355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-hop-source features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/17 19:24:58 WARN TaskSetManager: Stage 246 contains a task of very large size (1031 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 5s, sys: 2.43 s, total: 2min 8s\n",
      "Wall time: 20min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"1-hop-source features creation\")\n",
    "\n",
    "features_source = spark.createDataFrame(data_graph_agg).withColumn(\n",
    "    \"key\", sf.col(\"source\")\n",
    ").repartition(os.cpu_count() * 5, \"key\").groupby(\"key\").applyInPandas(\n",
    "    generate_features_udf_wrapper(False), schema=SCHEMA_FEAT_UDF\n",
    ").toPandas()\n",
    "features_source = pd.DataFrame(features_source[\"features\"].apply(json.loads).tolist())\n",
    "features_source.columns = [f\"{s.G_1HOP_PREFIX}{x}\" if x != \"key\" else x for x in features_source.columns]\n",
    "features_source.set_index(\"key\").to_parquet(location_features_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb81ceb5-dcdd-4c9e-bb47-1429de489490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-hop-target features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/17 19:45:39 WARN TaskSetManager: Stage 249 contains a task of very large size (1031 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 36s, sys: 1.49 s, total: 1min 38s\n",
      "Wall time: 12min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"1-hop-target features creation\")\n",
    "\n",
    "features_target = spark.createDataFrame(data_graph_agg).withColumn(\n",
    "    \"key\", sf.col(\"target\")\n",
    ").repartition(os.cpu_count() * 5, \"key\").groupby(\"key\").applyInPandas(\n",
    "    generate_features_udf_wrapper(False), schema=SCHEMA_FEAT_UDF\n",
    ").toPandas()\n",
    "features_target = pd.DataFrame(features_target[\"features\"].apply(json.loads).tolist())\n",
    "features_target.columns = [f\"{s.G_1HOP_PREFIX}{x}\" if x != \"key\" else x for x in features_target.columns]\n",
    "features_target.set_index(\"key\").to_parquet(location_features_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f335ce88-2ddb-4a5e-82ef-1f2295494bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_graph_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5be6ab9-3a2f-4d27-ad4b-3be73d99f71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLED_FEATURES = [\n",
    "    (\"leiden\", location_features_leiden),\n",
    "    (\"2_hop\", location_features_2_hop),\n",
    "    (\"as_source\", location_features_source),\n",
    "    (\"as_target\", location_features_target),\n",
    "    (\"comm_as_source_features\", location_comm_as_source_features),\n",
    "    (\"comm_as_target_features\", location_comm_as_target_features),\n",
    "    (\"comm_as_passthrough_features\", location_comm_as_passthrough_features),\n",
    "    (\"comm_as_passthrough_features_reverse\", location_comm_as_passthrough_features_reverse),\n",
    "    (\"flow_dispense\", location_flow_dispense),\n",
    "    (\"flow_passthrough\", location_flow_passthrough),\n",
    "    (\"flow_sink\", location_flow_sink),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74b28c47-b23c-44e2-b53e-a80b07eb971f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: (2973489, 288)\n"
     ]
    }
   ],
   "source": [
    "all_features = pd.DataFrame()\n",
    "all_features.index.name = \"key\"\n",
    "\n",
    "for feature_group, location in ENABLED_FEATURES:\n",
    "    all_features = all_features.join(\n",
    "        pd.read_parquet(location), how=\"outer\", rsuffix=f\"_{feature_group}\"\n",
    "    )\n",
    "\n",
    "all_features.to_parquet(location_features_node_level)\n",
    "print(\"Features:\", all_features.shape)\n",
    "del all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85f3470e-c691-4aa7-b273-b4b0041f1dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.read_parquet(location_features_node_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09a76fe3-abb0-44b4-ac8a-b3b60eb55f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 14 constant columns\n"
     ]
    }
   ],
   "source": [
    "constants = []\n",
    "for column in all_features.columns:\n",
    "    if all_features[column].nunique(dropna=True) <= 1:\n",
    "        del all_features[column]\n",
    "        constants.append(column)\n",
    "print(f\"Deleted {len(constants)} constant columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1963559f-3817-41f4-8ea8-2869faf99e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = {}\n",
    "for column in all_features.columns:\n",
    "    medians[column] = np.nanmedian(all_features[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "322714b7-7063-4485-81b1-6d541d25bb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script executed in 0:03:18\n"
     ]
    }
   ],
   "source": [
    "delta = round(time.time() - start_script)\n",
    "print(f\"Script executed in {timedelta(seconds=delta)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6da3dca-7750-4520-9c78-4fa51c674804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the anomaly detection model\n",
      "CPU times: user 1min 59s, sys: 18.3 s, total: 2min 18s\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Training the anomaly detection model\")\n",
    "\n",
    "anomalies = all_features.loc[:, []]\n",
    "model_ad = IsolationForest(n_estimators=1_000)\n",
    "anomalies.loc[:, \"anomaly_score\"] = -model_ad.fit(\n",
    "    all_features.fillna(medians)\n",
    ").decision_function(all_features.fillna(medians))\n",
    "anomalies.loc[:, \"anomaly_score\"] += abs(anomalies.loc[:, \"anomaly_score\"].min()) + 1e-10\n",
    "anomalies.loc[:, \"anomaly_score\"] /= anomalies.loc[:, \"anomaly_score\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e30b7013-254c-4f97-9894-71af0b6c4587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating edge features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.85 s, sys: 540 ms, total: 5.39 s\n",
      "Wall time: 17min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(f\"Generating edge features\")\n",
    "\n",
    "to_select = [\"source\", \"target\", \"timestamp\", \"num_transactions\", \"amount\", \"amount_usd\", \"is_zero_transaction\"]\n",
    "\n",
    "edges_features_input = data.select(to_select).groupby(\n",
    "    [\"source\", \"target\"]\n",
    ").agg(\n",
    "    sf.sum(\"num_transactions\").alias(\"num_transactions\"), \n",
    "    sf.sum(\"amount\").alias(\"amount\"),\n",
    "    sf.sum(\"amount_usd\").alias(\"amount_usd\"),\n",
    "    sf.count(sf.when(sf.col(\"is_zero_transaction\"), 1).otherwise(0)).alias(\"count_zero_transactions\"),\n",
    "    sf.count(sf.when(sf.col(\"is_zero_transaction\"), 0).otherwise(1)).alias(\"count_non_zero_transactions\"),\n",
    "    (sf.unix_timestamp(sf.max(\"timestamp\")) - sf.unix_timestamp(sf.min(\"timestamp\"))).alias(\"related_for\"),\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "_ = edges_features_input.count()\n",
    "\n",
    "edge_features = edges_features_input.toPandas()\n",
    "edge_features.to_parquet(location_features_edges)\n",
    "del edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b80a20e-5859-4af4-93b5-09c47c2e3516",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_features = pd.read_parquet(location_features_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de5e1408-4d7a-4547-baaa-0cac6952106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.7 s, sys: 1.31 s, total: 36 s\n",
      "Wall time: 36.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_edges = train.loc[:, [\"source\", \"target\"]].drop_duplicates().set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "valid_edges = validation.loc[:, [\"source\", \"target\"]].drop_duplicates().set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "test_edges = test.loc[:, [\"source\", \"target\"]].drop_duplicates().set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "\n",
    "train_features = train_edges.join(\n",
    "    edge_features.set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "validation_features = valid_edges.join(\n",
    "    edge_features.set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "test_features = test_edges.join(\n",
    "    edge_features.set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce68e646-67f2-4158-ba72-528c94207d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_edge_features(features_in, location):\n",
    "    features_in = features_in.set_index(\"target\").join(\n",
    "        anomalies, how=\"left\"\n",
    "    ).reset_index()\n",
    "    features_in = features_in.set_index(\"source\").join(\n",
    "        anomalies, how=\"left\", rsuffix=\"_source\"\n",
    "    ).reset_index()\n",
    "    features_in = features_in.set_index(\"target\").join(\n",
    "        all_features, how=\"left\", rsuffix=\"_target\"\n",
    "    ).reset_index()\n",
    "    features_in = features_in.set_index(\"source\").join(\n",
    "        all_features, how=\"left\", rsuffix=\"_source\"\n",
    "    ).reset_index()\n",
    "    features_in.loc[:, \"anom_scores_diff\"] = features_in.loc[:, \"anomaly_score\"] - features_in.loc[:, \"anomaly_score_source\"]\n",
    "    features_in.loc[:, \"anom_scores_min\"] = np.array(\n",
    "        [\n",
    "            features_in.loc[:, \"anomaly_score\"].values, \n",
    "            features_in.loc[:, \"anomaly_score_source\"].values\n",
    "        ],\n",
    "    ).min(axis=0)\n",
    "    features_in.loc[:, \"anom_scores_max\"] = np.array(\n",
    "        [\n",
    "            features_in.loc[:, \"anomaly_score\"].values, \n",
    "            features_in.loc[:, \"anomaly_score_source\"].values\n",
    "        ],\n",
    "    ).max(axis=0)\n",
    "    features_in.loc[:, \"anom_scores_mean\"] = np.array(\n",
    "        [\n",
    "            features_in.loc[:, \"anomaly_score\"].values, \n",
    "            features_in.loc[:, \"anomaly_score_source\"].values\n",
    "        ],\n",
    "    ).mean(axis=0)\n",
    "    features_in.to_parquet(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5af6d41c-660c-4bc2-87a8-73660fe48a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 59s, sys: 23.8 s, total: 2min 23s\n",
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "save_edge_features(train_features, location_features_edges_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7adbd114-df75-4bca-b697-7fcd08ae36c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.9 s, sys: 2.5 s, total: 29.4 s\n",
      "Wall time: 29.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "save_edge_features(validation_features, location_features_edges_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40d4fed8-cc76-44e0-809a-5d6ae82539c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.5 s, sys: 1.55 s, total: 26 s\n",
      "Wall time: 25.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "save_edge_features(test_features, location_features_edges_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f4364bc-eb49-4d03-b85b-8ff293a88fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trx_features(data_in, location):\n",
    "    columns = [\n",
    "        \"source\",\n",
    "        \"target\",\n",
    "        \"amount\",\n",
    "        \"amount_usd\",\n",
    "        \"is_zero_transaction\",\n",
    "        \"source_dispensation\",\n",
    "        \"target_accumulation\",\n",
    "        \"source_positive_balance\",\n",
    "        \"source_negative_balance\",\n",
    "        \"target_positive_balance\",\n",
    "        \"target_negative_balance\",\n",
    "        \"source_active_for\",\n",
    "        \"target_active_for\",\n",
    "        \"is_phishing\",\n",
    "    ]\n",
    "    trx_features = data_in.loc[:, columns]\n",
    "    trx_features.to_parquet(location)\n",
    "    del trx_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c783f612-4c76-4ea5-aed2-68eeaec1191d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.3 s, sys: 382 ms, total: 7.68 s\n",
      "Wall time: 7.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "save_trx_features(train, location_train_trx_features)\n",
    "save_trx_features(validation, location_valid_trx_features)\n",
    "save_trx_features(test, location_test_trx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e6eaaec-8af4-43d7-8127-a306b855f430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To free up memory for training\n",
    "\n",
    "to_reset = %who_ls\n",
    "to_reset = list(to_reset)\n",
    "to_reset.remove(\"to_keep\")\n",
    "to_reset = set(to_reset) - set(to_keep)\n",
    "for var_to_reset in list(to_reset):\n",
    "    var_to_reset = f\"^{var_to_reset}$\"\n",
    "    %reset_selective -f {var_to_reset}\n",
    "\n",
    "delete_large_vars(globals(), locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13c3a17d-14f9-4d34-9103-0c7e2e8c064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(location_features_trx, location_features_edges, location_features):\n",
    "    features_input = spark.read.parquet(location_features_edges)\n",
    "    trx_features_input = spark.read.parquet(location_features_trx).withColumnRenamed(\n",
    "        \"source\", \"source_trx\"\n",
    "    ).withColumnRenamed(\n",
    "        \"target\", \"target_trx\"\n",
    "    ).withColumnRenamed(\n",
    "        \"amount\", \"amount_trx\"\n",
    "    ).withColumnRenamed(\n",
    "        \"amount_usd\", \"amount_usd_trx\"\n",
    "    )\n",
    "    features_input = trx_features_input.join(\n",
    "        features_input,\n",
    "        (trx_features_input[\"source_trx\"] == features_input[\"source\"]) &\n",
    "        (trx_features_input[\"target_trx\"] == features_input[\"target\"]),\n",
    "        how=\"left\"\n",
    "    ).drop(\"source_trx\", \"target_trx\", \"source\", \"target\")\n",
    "    features_input.write.parquet(location_features, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96867e2d-ceed-4176-b2f2-cb407c592ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/17 21:57:31 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 84.8 ms, sys: 19.2 ms, total: 104 ms\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "combine_features(location_train_trx_features, location_features_edges_train, location_train_features)\n",
    "combine_features(location_valid_trx_features, location_features_edges_valid, location_valid_features)\n",
    "combine_features(location_test_trx_features, location_features_edges_test, location_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8924af3-4f88-48dc-9e82-ab345e77f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(MULTI_PROC_STAGING_LOCATION, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4994538f-cc63-4a41-b494-43d016f0f6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 36.9 s, total: 1min 39s\n",
      "Wall time: 9.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_features = pd.read_parquet(location_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb0af34c-0428-4f00-9e02-799f8fb844ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.99 s, sys: 2min 46s, total: 2min 50s\n",
      "Wall time: 5.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "validation_features = pd.read_parquet(location_valid_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4374b121-2d18-4f3c-b769-5e05bbcbd03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.79 s, sys: 4.21 s, total: 7.01 s\n",
      "Wall time: 416 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_features = pd.read_parquet(location_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a7d7c2f-cc45-48a0-a928-08c4f67ec0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = set(train_features.columns) | set(validation_features.columns) | set(test_features.columns)\n",
    "\n",
    "for missing in (\n",
    "    all_columns.symmetric_difference(train_features.columns) |\n",
    "    all_columns.symmetric_difference(validation_features.columns) |\n",
    "    all_columns.symmetric_difference(test_features.columns)\n",
    "):\n",
    "    if missing in train_features.columns:\n",
    "        print(f\"Deleting {missing} from train\")\n",
    "        del train_features[missing]\n",
    "    if missing in validation_features.columns:\n",
    "        print(f\"Deleting {missing} from validation\")\n",
    "        del validation_features[missing]\n",
    "    if missing in test_features.columns:\n",
    "        print(f\"Deleting {missing} from test\")\n",
    "        del test_features[missing]\n",
    "\n",
    "validation_features = validation_features.loc[:, list(train_features.columns)]\n",
    "test_features = test_features.loc[:, list(train_features.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f310f90b-93a2-455f-a0b3-020c87fec357",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_features.shape[0] == train_count\n",
    "assert validation_features.shape[0] == validation_count\n",
    "assert test_features.shape[0] == test_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2ea56f4-4dba-4cb4-a95e-b8989ca7b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_labels = train_features.loc[:, [\"is_phishing\"]].copy(deep=True)\n",
    "del train_features[\"is_phishing\"]\n",
    "\n",
    "validation_features_labels = validation_features.loc[:, [\"is_phishing\"]].copy(deep=True)\n",
    "validation_features = validation_features.loc[:, train_features.columns]\n",
    "\n",
    "test_features_labels = test_features.loc[:, [\"is_phishing\"]].copy(deep=True)\n",
    "test_features = test_features.loc[:, train_features.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db2dbe51-59bf-4e8e-a132-c4c3121ae717",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_available = False\n",
    "try:\n",
    "    import torch\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "\n",
    "def f1_eval(y, y_):\n",
    "    return 1 - f1_score(y, np.round(y_))\n",
    "\n",
    "\n",
    "xgb_args = dict(\n",
    "    early_stopping_rounds=10, scale_pos_weight=1000,\n",
    "    eval_metric=f1_eval, disable_default_eval_metric=True, \n",
    "    num_parallel_tree=10, max_depth=6,\n",
    "    colsample_bytree=1, subsample=0.5,\n",
    "    device=\"cpu\", nthread=16,\n",
    "    n_estimators=100, seed=0,\n",
    ")\n",
    "if cuda_available:\n",
    "    xgb_args[\"device\"] = \"cuda\"\n",
    "    xgb_args[\"nthread\"] = 2\n",
    "\n",
    "xgb_fit_args = {\n",
    "    \"eval_set\": [(validation_features, validation_features_labels[\"is_phishing\"].values)],\n",
    "    \"verbose\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de1642f1-ce3f-4a20-8b99-f0362047c8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-f1_eval:0.86539\n",
      "[1]\tvalidation_0-f1_eval:0.82877\n",
      "[2]\tvalidation_0-f1_eval:0.80248\n",
      "[3]\tvalidation_0-f1_eval:0.79393\n",
      "[4]\tvalidation_0-f1_eval:0.77311\n",
      "[5]\tvalidation_0-f1_eval:0.75992\n",
      "[6]\tvalidation_0-f1_eval:0.74926\n",
      "[7]\tvalidation_0-f1_eval:0.73140\n",
      "[8]\tvalidation_0-f1_eval:0.71620\n",
      "[9]\tvalidation_0-f1_eval:0.70261\n",
      "[10]\tvalidation_0-f1_eval:0.68067\n",
      "[11]\tvalidation_0-f1_eval:0.66696\n",
      "[12]\tvalidation_0-f1_eval:0.65585\n",
      "[13]\tvalidation_0-f1_eval:0.64675\n",
      "[14]\tvalidation_0-f1_eval:0.63479\n",
      "[15]\tvalidation_0-f1_eval:0.60401\n",
      "[16]\tvalidation_0-f1_eval:0.59719\n",
      "[17]\tvalidation_0-f1_eval:0.59574\n",
      "[18]\tvalidation_0-f1_eval:0.58282\n",
      "[19]\tvalidation_0-f1_eval:0.57521\n",
      "[20]\tvalidation_0-f1_eval:0.57050\n",
      "[21]\tvalidation_0-f1_eval:0.56473\n",
      "[22]\tvalidation_0-f1_eval:0.55605\n",
      "[23]\tvalidation_0-f1_eval:0.54864\n",
      "[24]\tvalidation_0-f1_eval:0.54100\n",
      "[25]\tvalidation_0-f1_eval:0.52017\n",
      "[26]\tvalidation_0-f1_eval:0.50567\n",
      "[27]\tvalidation_0-f1_eval:0.49363\n",
      "[28]\tvalidation_0-f1_eval:0.48584\n",
      "[29]\tvalidation_0-f1_eval:0.47191\n",
      "[30]\tvalidation_0-f1_eval:0.46131\n",
      "[31]\tvalidation_0-f1_eval:0.45366\n",
      "[32]\tvalidation_0-f1_eval:0.44123\n",
      "[33]\tvalidation_0-f1_eval:0.43195\n",
      "[34]\tvalidation_0-f1_eval:0.42498\n",
      "[35]\tvalidation_0-f1_eval:0.42092\n",
      "[36]\tvalidation_0-f1_eval:0.41373\n",
      "[37]\tvalidation_0-f1_eval:0.41001\n",
      "[38]\tvalidation_0-f1_eval:0.40214\n",
      "[39]\tvalidation_0-f1_eval:0.39546\n",
      "[40]\tvalidation_0-f1_eval:0.39169\n",
      "[41]\tvalidation_0-f1_eval:0.39211\n",
      "[42]\tvalidation_0-f1_eval:0.38874\n",
      "[43]\tvalidation_0-f1_eval:0.38474\n",
      "[44]\tvalidation_0-f1_eval:0.38028\n",
      "[45]\tvalidation_0-f1_eval:0.37542\n",
      "[46]\tvalidation_0-f1_eval:0.37256\n",
      "[47]\tvalidation_0-f1_eval:0.36822\n",
      "[48]\tvalidation_0-f1_eval:0.36519\n",
      "[49]\tvalidation_0-f1_eval:0.36377\n",
      "[50]\tvalidation_0-f1_eval:0.36172\n",
      "[51]\tvalidation_0-f1_eval:0.35926\n",
      "[52]\tvalidation_0-f1_eval:0.35816\n",
      "[53]\tvalidation_0-f1_eval:0.36986\n",
      "[54]\tvalidation_0-f1_eval:0.36758\n",
      "[55]\tvalidation_0-f1_eval:0.36660\n",
      "[56]\tvalidation_0-f1_eval:0.36384\n",
      "[57]\tvalidation_0-f1_eval:0.36058\n",
      "[58]\tvalidation_0-f1_eval:0.36612\n",
      "[59]\tvalidation_0-f1_eval:0.36517\n",
      "[60]\tvalidation_0-f1_eval:0.36282\n",
      "[61]\tvalidation_0-f1_eval:0.36370\n",
      "[62]\tvalidation_0-f1_eval:0.36532\n",
      "63.24 61.06\n",
      "\n",
      "CPU times: user 5h 3min 6s, sys: 21 s, total: 5h 3min 27s\n",
      "Wall time: 23min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = xgb.XGBClassifier(**xgb_args)\n",
    "model.fit(train_features, train_features_labels[\"is_phishing\"].values, **xgb_fit_args)\n",
    "y_test_predicted = model.predict(test_features)\n",
    "f1_final = f1_score(test_features_labels[\"is_phishing\"], y_test_predicted) * 100\n",
    "print(\n",
    "    round(f1_final, 2),\n",
    "    round(recall_score(test_features_labels[\"is_phishing\"], y_test_predicted) * 100, 2)\n",
    ")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e178c2-8cf6-4edb-ae31-320420efc8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed 10\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "f1_scores = [f1_final]\n",
    "xgb_fit_args[\"verbose\"] = False\n",
    "for seed in [10, 20, 30, 40]:\n",
    "    xgb_args[\"seed\"] = seed\n",
    "    print(\"seed\", seed)\n",
    "    model = xgb.XGBClassifier(**xgb_args)\n",
    "    model.fit(train_features, train_features_labels[\"is_phishing\"].values, **xgb_fit_args)\n",
    "    f1_scores.append(f1_score(test_features_labels[\"is_phishing\"], model.predict(test_features)) * 100)\n",
    "    print(round(f1_scores[-1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18819c8-a15e-40da-b154-01c7c52cd7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gfp_best = 51.49\n",
    "gfp_std = 4.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998bda30-ec09-4839-84b3-bebdbb79bea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"GFP best: {gfp_best} ± {gfp_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6bbb2c-8383-43e7-91f2-447c9b013683",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{round(max(f1_scores), 2)} ±{round(np.std(f1_scores), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8243a3-75d5-46e5-81c7-a5418ec30623",
   "metadata": {},
   "outputs": [],
   "source": [
    "uplift = round(((max(f1_scores) - gfp_best) / gfp_best) * 100, 2)\n",
    "print(f\"Uplift of {uplift}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8f1259-7f64-4919-93ab-93357d171d49",
   "metadata": {},
   "source": [
    "### https://dl.acm.org/doi/pdf/10.1145/3677052.3698648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102f290-c8e0-4064-86e8-a89a42e1628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta, datetime\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score, recall_score, RocCurveDisplay\n",
    "\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "import settings as s\n",
    "\n",
    "os.environ[\"EXT_DATA_TYPE_FOLDER\"] = \"ethereum\"\n",
    "\n",
    "from common import get_weights, delete_large_vars, MULTI_PROC_STAGING_LOCATION\n",
    "from communities import get_communities_spark\n",
    "from features import (\n",
    "    generate_features_spark, generate_features_udf_wrapper,\n",
    "    SCHEMA_FEAT_UDF\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa41d126-7f49-40e0-acc8-78556f3c39fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = int(os.environ.get(\"EXSTRAQT_SEED\", 42))\n",
    "print(f\"{SEED=}\")\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cbd846-2921-4802-b8b8-38d64092cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    sys.version_info.major,\n",
    "    sys.version_info.minor,\n",
    "    sys.version_info.micro,\n",
    ") != (3, 11, 8):\n",
    "    raise EnvironmentError(\n",
    "        \"Only runs efficiently on Python 3.11.8\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb8939c-e2fc-4bfb-ae18-ef1eec4eafc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_CONF = [\n",
    "    (\"spark.driver.memory\", \"32g\"),\n",
    "    (\"spark.worker.memory\", \"32g\"),\n",
    "    (\"spark.driver.maxResultSize\", \"32g\"),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"true\"),\n",
    "    (\"spark.network.timeout\", \"600s\"),\n",
    "    (\"spark.sql.autoBroadcastJoinThreshold\", -1),\n",
    "    (\"spark.local.dir\", f\".{os.sep}temp-spark\"),\n",
    "]\n",
    "\n",
    "if \"EXSTRAQT_SEED\" in os.environ:\n",
    "    SPARK_CONF.append((\"spark.log.level\", \"ERROR\"))\n",
    "\n",
    "shutil.rmtree(\"artifacts\", ignore_errors=True)\n",
    "shutil.rmtree(\"temp-spark\", ignore_errors=True)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"testing\")\n",
    "    .config(conf=SparkConf().setAll(SPARK_CONF))\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b1fa7-53c8-4dbb-b25f-795aaca78a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PERC = 0.65\n",
    "VALIDATION_PERC = 0.15\n",
    "TEST_PERC = 0.2\n",
    "\n",
    "KEEP_TOP_N = 100\n",
    "\n",
    "assert(sum([TRAIN_PERC, VALIDATION_PERC, TEST_PERC]) == 1)\n",
    "\n",
    "location_main = os.path.join(\"features\", os.environ[\"EXT_DATA_TYPE_FOLDER\"])\n",
    "# shutil.rmtree(location_main, ignore_errors=True)\n",
    "\n",
    "location_communities_leiden = f\"{location_main}{os.sep}communities_leiden.parquet\"\n",
    "\n",
    "location_features_leiden = f\"{location_main}{os.sep}features_leiden.parquet\"\n",
    "location_features_ego = f\"{location_main}{os.sep}features_ego.parquet\"\n",
    "location_features_2_hop = f\"{location_main}{os.sep}features_2_hop.parquet\"\n",
    "location_features_2_hop_out = f\"{location_main}{os.sep}features_2_hop_out.parquet\"\n",
    "location_features_2_hop_in = f\"{location_main}{os.sep}features_2_hop_in.parquet\"\n",
    "location_features_2_hop_combined = f\"{location_main}{os.sep}features_2_hop_combined.parquet\"\n",
    "location_features_source = f\"{location_main}{os.sep}features_source.parquet\"\n",
    "location_features_target = f\"{location_main}{os.sep}features_target.parquet\"\n",
    "\n",
    "location_flow_dispense = f\"{location_main}{os.sep}location_flow_dispense.parquet\"\n",
    "location_flow_passthrough = f\"{location_main}{os.sep}location_flow_passthrough.parquet\"\n",
    "location_flow_sink = f\"{location_main}{os.sep}location_flow_sink.parquet\"\n",
    "\n",
    "location_comm_as_source_features = f\"{location_main}{os.sep}comm_as_source_features.parquet\"\n",
    "location_comm_as_target_features = f\"{location_main}{os.sep}comm_as_target_features.parquet\"\n",
    "location_comm_as_passthrough_features = f\"{location_main}{os.sep}comm_as_passthrough_features.parquet\"\n",
    "location_comm_as_passthrough_features_reverse = f\"{location_main}{os.sep}comm_as_passthrough_features_reverse.parquet\"\n",
    "\n",
    "location_features_node_level = f\"{location_main}{os.sep}features_node_level.parquet\"\n",
    "location_features_edges = f\"{location_main}{os.sep}features_edges.parquet\"\n",
    "\n",
    "location_features_edges_train = f\"{location_main}{os.sep}features_edges_train.parquet\"\n",
    "location_features_edges_valid = f\"{location_main}{os.sep}features_edges_valid.parquet\"\n",
    "location_features_edges_test = f\"{location_main}{os.sep}features_edges_test.parquet\"\n",
    "\n",
    "location_train_trx_features = f\"{location_main}{os.sep}train_trx_features.parquet\"\n",
    "location_valid_trx_features = f\"{location_main}{os.sep}valid_trx_features.parquet\"\n",
    "location_test_trx_features = f\"{location_main}{os.sep}test_trx_features.parquet\"\n",
    "\n",
    "location_train_features = f\"{location_main}{os.sep}train_features.parquet\"\n",
    "location_valid_features = f\"{location_main}{os.sep}valid_features.parquet\"\n",
    "location_test_features = f\"{location_main}{os.sep}test_features.parquet\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(location_main)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82546745-6577-4e38-bf44-1a6114c3a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(s.INPUT_DATA_FILE)\n",
    "# Only interested when \"target\" is phishing\n",
    "phishing_nodes = set(data.loc[data[\"is_phishing\"], \"target\"].unique())\n",
    "assert len(phishing_nodes) == 1164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5d7e4-1b66-4e93-b67b-a947d8c8e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "source_firsts = data.groupby(\"source\").agg(first_trx=(\"timestamp\", \"min\"))\n",
    "target_firsts = data.groupby(\"target\").agg(first_trx=(\"timestamp\", \"min\"))\n",
    "active_since = source_firsts.join(target_firsts, lsuffix=\"_left\", how=\"outer\").fillna(datetime.now())\n",
    "active_since.loc[:, \"active_since\"] = active_since.apply(lambda x: min([x[\"first_trx_left\"], x[\"first_trx\"]]), axis=1)\n",
    "active_since = active_since.loc[:, [\"active_since\"]]\n",
    "active_since.sort_values(\"active_since\", inplace=True)\n",
    "\n",
    "number_of_train_accounts = int(np.floor(active_since.shape[0] * TRAIN_PERC))\n",
    "number_of_validation_accounts = int(np.floor(active_since.shape[0] * VALIDATION_PERC))\n",
    "train_accounts = set(active_since.head(number_of_train_accounts).index.tolist())\n",
    "assert len(train_accounts) == number_of_train_accounts\n",
    "remaining = active_since.loc[~active_since.index.isin(train_accounts), :].sort_values(\"active_since\")\n",
    "validation_accounts = set(remaining.head(number_of_validation_accounts).index.tolist())\n",
    "assert len(validation_accounts) == number_of_validation_accounts\n",
    "test_accounts = set(active_since.index) - train_accounts - validation_accounts\n",
    "print(f\"{len(train_accounts):,} | {len(validation_accounts):,} | {len(test_accounts):,}\")\n",
    "assert sorted(train_accounts | validation_accounts | test_accounts) == sorted(active_since.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac24d73c-ef15-42c9-bf93-8de463982b3d",
   "metadata": {},
   "source": [
    "# [To prevent data leakage]\n",
    "### Each accounts set is _exclusive_ for `train`, `validation`, and `test` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f7e149-4992-4244-bc64-2058bd29829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.loc[data[\"source\"].isin(train_accounts) & data[\"target\"].isin(train_accounts), :]\n",
    "validation = data.loc[data[\"source\"].isin(validation_accounts) & data[\"target\"].isin(validation_accounts), :]\n",
    "test = data.loc[data[\"source\"].isin(test_accounts) & data[\"target\"].isin(test_accounts), :]\n",
    "print(\n",
    "    round(train.shape[0] / data.shape[0], 2), \n",
    "    round(validation.shape[0] / data.shape[0], 2), \n",
    "    round(test.shape[0] / data.shape[0], 2)\n",
    ")\n",
    "train_count, validation_count, test_count = train.shape[0], validation.shape[0], test.shape[0]\n",
    "\n",
    "assert set(train.index).intersection(validation.index) == set()\n",
    "assert set(validation.index).intersection(test.index) == set()\n",
    "assert set(train.index).intersection(test.index) == set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30b7013-254c-4f97-9894-71af0b6c4587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_edge_features(input_data):\n",
    "    print(f\"Generating edge features\")\n",
    "    to_select = [\"source\", \"target\", \"timestamp\", \"num_transactions\", \"amount\", \"amount_usd\", \"is_zero_transaction\"]    \n",
    "    edges_features_input = input_data.select(*to_select).groupby(\n",
    "        [\"source\", \"target\"]\n",
    "    ).agg(\n",
    "        sf.sum(\"num_transactions\").alias(\"num_transactions\"), \n",
    "        sf.sum(\"amount\").alias(\"amount\"),\n",
    "        sf.sum(\"amount_usd\").alias(\"amount_usd\"),\n",
    "        sf.count(sf.when(sf.col(\"is_zero_transaction\"), 1).otherwise(0)).alias(\"count_zero_transactions\"),\n",
    "        sf.count(sf.when(sf.col(\"is_zero_transaction\"), 0).otherwise(1)).alias(\"count_non_zero_transactions\"),\n",
    "        (sf.unix_timestamp(sf.max(\"timestamp\")) - sf.unix_timestamp(sf.min(\"timestamp\"))).alias(\"related_for\"),\n",
    "    ).persist(StorageLevel.DISK_ONLY)\n",
    "    _ = edges_features_input.count()\n",
    "    edge_features = edges_features_input.toPandas()\n",
    "    edge_features.to_parquet(location_features_edges)\n",
    "    del edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce68e646-67f2-4158-ba72-528c94207d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_node_features_to_edges(features_in, location):\n",
    "    features_in = features_in.set_index(\"target\").join(\n",
    "        pd.read_parquet(location_features_node_level), how=\"left\", rsuffix=\"_target\"\n",
    "    ).reset_index().set_index(\"source\").join(\n",
    "        pd.read_parquet(location_features_node_level), how=\"left\", rsuffix=\"_source\"\n",
    "    ).reset_index()\n",
    "\n",
    "    features_in.loc[:, \"anomaly_scores_diff\"] = features_in.loc[:, \"anomaly_score\"] - features_in.loc[:, \"anomaly_score_source\"]\n",
    "    features_in.loc[:, \"anomaly_scores_min\"] = np.array(\n",
    "        [\n",
    "            features_in.loc[:, \"anomaly_score\"].values, \n",
    "            features_in.loc[:, \"anomaly_score_source\"].values\n",
    "        ],\n",
    "    ).min(axis=0)\n",
    "    features_in.loc[:, \"anomaly_scores_max\"] = np.array(\n",
    "        [\n",
    "            features_in.loc[:, \"anomaly_score\"].values, \n",
    "            features_in.loc[:, \"anomaly_score_source\"].values\n",
    "        ],\n",
    "    ).max(axis=0)\n",
    "    features_in.loc[:, \"anomaly_scores_mean\"] = np.array(\n",
    "        [\n",
    "            features_in.loc[:, \"anomaly_score\"].values, \n",
    "            features_in.loc[:, \"anomaly_score_source\"].values\n",
    "        ],\n",
    "    ).mean(axis=0)\n",
    "\n",
    "    features_in.to_parquet(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4364bc-eb49-4d03-b85b-8ff293a88fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trx_features(data_in, location):\n",
    "    columns = [\n",
    "        \"source\",\n",
    "        \"target\",\n",
    "        \"amount\",\n",
    "        \"amount_usd\",\n",
    "        \"is_zero_transaction\",\n",
    "        \"source_dispensation\",\n",
    "        \"target_accumulation\",\n",
    "        \"source_positive_balance\",\n",
    "        \"source_negative_balance\",\n",
    "        \"target_positive_balance\",\n",
    "        \"target_negative_balance\",\n",
    "        \"source_active_for\",\n",
    "        \"target_active_for\",\n",
    "        \"is_phishing\",\n",
    "    ]\n",
    "    trx_features = data_in.loc[:, columns]\n",
    "    trx_features.loc[:, \"source_balance_ratio\"] = (\n",
    "        trx_features[\"source_positive_balance\"] / trx_features[\"source_negative_balance\"]\n",
    "    ).fillna(0).replace(np.inf, 0)\n",
    "    trx_features.loc[:, \"target_balance_ratio\"] = (\n",
    "        trx_features[\"target_positive_balance\"] / trx_features[\"target_negative_balance\"]\n",
    "    ).fillna(0).replace(np.inf, 0)\n",
    "    trx_features.to_parquet(location)\n",
    "    del trx_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c783f612-4c76-4ea5-aed2-68eeaec1191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(f\"Constructing node-level features: {data.shape[0]:,}\")\n",
    "\n",
    "%run node_level_features.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad7975d-7924-42c5-ba35-3174f6b794d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "generate_edge_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b790a55-6ca4-4946-a66c-7d9515c284b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_edges = train.loc[:, [\"source\", \"target\"]].drop_duplicates().set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "train_features = train_edges.join(\n",
    "    pd.read_parquet(location_features_edges).set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "add_node_features_to_edges(train_features, location_features_edges_train)\n",
    "save_trx_features(train, location_train_trx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232b2e5f-d1b1-4f2c-be82-58ad56b8959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "validation_edges = validation.loc[:, [\"source\", \"target\"]].drop_duplicates().set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "validation_features = validation_edges.join(\n",
    "    pd.read_parquet(location_features_edges).set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "add_node_features_to_edges(validation_features, location_features_edges_valid)\n",
    "save_trx_features(validation, location_valid_trx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d3d068-70df-4201-a0e2-1e4e4ace8b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test_edges = test.loc[:, [\"source\", \"target\"]].drop_duplicates().set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "test_features = test_edges.join(\n",
    "    pd.read_parquet(location_features_edges).set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "add_node_features_to_edges(test_features, location_features_edges_test)\n",
    "save_trx_features(test, location_test_trx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6eaaec-8af4-43d7-8127-a306b855f430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To free up memory for training\n",
    "\n",
    "to_reset = %who_ls\n",
    "to_reset = list(to_reset)\n",
    "to_reset.remove(\"to_keep\")\n",
    "to_reset = set(to_reset) - set(to_keep)\n",
    "for var_to_reset in list(to_reset):\n",
    "    var_to_reset = f\"^{var_to_reset}$\"\n",
    "    %reset_selective -f {var_to_reset}\n",
    "\n",
    "delete_large_vars(globals(), locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c3a17d-14f9-4d34-9103-0c7e2e8c064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(location_features_trx, location_features_edges, location_features):\n",
    "    features_input = spark.read.parquet(location_features_edges)\n",
    "    trx_features_input = spark.read.parquet(location_features_trx).withColumnRenamed(\n",
    "        \"amount\", \"amount_trx\"\n",
    "    ).withColumnRenamed(\n",
    "        \"amount_usd\", \"amount_usd_trx\"\n",
    "    )\n",
    "    features_input = trx_features_input.join(\n",
    "        features_input,\n",
    "        on=[\"source\", \"target\"],\n",
    "        how=\"left\"\n",
    "    ).drop(\"source\", \"target\")\n",
    "    features_input.write.parquet(location_features, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96867e2d-ceed-4176-b2f2-cb407c592ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "combine_features(location_train_trx_features, location_features_edges_train, location_train_features)\n",
    "combine_features(location_valid_trx_features, location_features_edges_valid, location_valid_features)\n",
    "combine_features(location_test_trx_features, location_features_edges_test, location_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8924af3-4f88-48dc-9e82-ab345e77f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(MULTI_PROC_STAGING_LOCATION, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4994538f-cc63-4a41-b494-43d016f0f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_features = pd.read_parquet(location_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0af34c-0428-4f00-9e02-799f8fb844ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "validation_features = pd.read_parquet(location_valid_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4374b121-2d18-4f3c-b769-5e05bbcbd03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test_features = pd.read_parquet(location_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d7c2f-cc45-48a0-a928-08c4f67ec0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = set(train_features.columns) | set(validation_features.columns) | set(test_features.columns)\n",
    "\n",
    "for missing in (\n",
    "    all_columns.symmetric_difference(train_features.columns) |\n",
    "    all_columns.symmetric_difference(validation_features.columns) |\n",
    "    all_columns.symmetric_difference(test_features.columns)\n",
    "):\n",
    "    if missing in train_features.columns:\n",
    "        print(f\"Deleting {missing} from train\")\n",
    "        del train_features[missing]\n",
    "    if missing in validation_features.columns:\n",
    "        print(f\"Deleting {missing} from validation\")\n",
    "        del validation_features[missing]\n",
    "    if missing in test_features.columns:\n",
    "        print(f\"Deleting {missing} from test\")\n",
    "        del test_features[missing]\n",
    "\n",
    "validation_features = validation_features.loc[:, list(train_features.columns)]\n",
    "test_features = test_features.loc[:, list(train_features.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f310f90b-93a2-455f-a0b3-020c87fec357",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_features.shape[0] == train_count\n",
    "assert validation_features.shape[0] == validation_count\n",
    "assert test_features.shape[0] == test_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea56f4-4dba-4cb4-a95e-b8989ca7b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_labels = train_features.loc[:, [\"is_phishing\"]].copy(deep=True)\n",
    "del train_features[\"is_phishing\"]\n",
    "\n",
    "validation_features_labels = validation_features.loc[:, [\"is_phishing\"]].copy(deep=True)\n",
    "validation_features = validation_features.loc[:, train_features.columns]\n",
    "\n",
    "test_features_labels = test_features.loc[:, [\"is_phishing\"]].copy(deep=True)\n",
    "test_features = test_features.loc[:, train_features.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2dbe51-59bf-4e8e-a132-c4c3121ae717",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_available = False\n",
    "try:\n",
    "    import torch\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "xgb_args = dict(\n",
    "    seed=SEED,\n",
    "    max_depth=6,\n",
    "    scale_pos_weight=35,\n",
    "    eta=0.025,\n",
    "    subsample=0.5,\n",
    "    colsample_bytree=0.9, \n",
    "    num_parallel_tree=10, \n",
    "    n_estimators=100, \n",
    "    early_stopping_rounds=10, \n",
    "    eval_metric=\"aucpr\", \n",
    "    disable_default_eval_metric=True, \n",
    "    nthread=10,\n",
    "    device=\"cpu\", \n",
    ")\n",
    "if cuda_available:\n",
    "    xgb_args[\"device\"] = \"cuda\"\n",
    "    xgb_args[\"nthread\"] = 2\n",
    "\n",
    "xgb_fit_args = {\n",
    "    \"eval_set\": [(validation_features, validation_features_labels[\"is_phishing\"].values)],\n",
    "    \"verbose\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1642f1-ce3f-4a20-8b99-f0362047c8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = xgb.XGBClassifier(**xgb_args)\n",
    "model.fit(\n",
    "    train_features, train_features_labels[\"is_phishing\"].values, \n",
    "    **xgb_fit_args\n",
    ")\n",
    "y_test_predicted = model.predict(test_features)\n",
    "f1_test = f1_score(test_features_labels[\"is_phishing\"], y_test_predicted) * 100\n",
    "print(\n",
    "    f\"{SEED=}\",\n",
    "    f\"f1={round(f1_test, 2)}\",\n",
    "    f\"recall={round(recall_score(test_features_labels['is_phishing'], y_test_predicted) * 100, 2)}\",\n",
    ")\n",
    "print(f1_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

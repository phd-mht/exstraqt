{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5102f290-c8e0-4064-86e8-a89a42e1628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score, recall_score, RocCurveDisplay\n",
    "\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "import settings as s\n",
    "\n",
    "assert s.FILE_SIZE == \"Small\"\n",
    "assert s.HIGH_ILLICIT == False\n",
    "\n",
    "os.environ[\"EXT_DATA_TYPE_FOLDER\"] = s.OUTPUT_POSTFIX.lstrip(\"-\")\n",
    "\n",
    "from common import get_weights, delete_large_vars, MULTI_PROC_STAGING_LOCATION\n",
    "from communities import get_communities_spark\n",
    "from features import (\n",
    "    generate_features_spark, generate_features_udf_wrapper, get_edge_features_udf,\n",
    "    SCHEMA_FEAT_UDF, CURRENCY_RATES\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01648199-82fc-4bec-8248-828135bd0af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED=42\n"
     ]
    }
   ],
   "source": [
    "SEED = int(os.environ.get(\"EXSTRAQT_SEED\", 42))\n",
    "print(f\"{SEED=}\")\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3cbd846-2921-4802-b8b8-38d64092cfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    sys.version_info.major,\n",
    "    sys.version_info.minor,\n",
    "    sys.version_info.micro,\n",
    ") != (3, 11, 8):\n",
    "    raise EnvironmentError(\n",
    "        \"Only runs efficiently on Python 3.11.8\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adb8939c-e2fc-4bfb-ae18-ef1eec4eafc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/28 19:17:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/10/28 19:17:56 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "SPARK_CONF = [\n",
    "    (\"spark.driver.memory\", \"32g\"),\n",
    "    (\"spark.worker.memory\", \"32g\"),\n",
    "    (\"spark.driver.maxResultSize\", \"32g\"),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"true\"),\n",
    "    (\"spark.network.timeout\", \"600s\"),\n",
    "    (\"spark.sql.autoBroadcastJoinThreshold\", -1),\n",
    "    (\"spark.local.dir\", f\".{os.sep}temp-spark\"),\n",
    "]\n",
    "\n",
    "if \"EXSTRAQT_SEED\" in os.environ:\n",
    "    SPARK_CONF.append((\"spark.log.level\", \"ERROR\"))\n",
    "\n",
    "shutil.rmtree(\"artifacts\", ignore_errors=True)\n",
    "shutil.rmtree(\"temp-spark\", ignore_errors=True)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"testing\")\n",
    "    .config(conf=SparkConf().setAll(SPARK_CONF))\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "320b1fa7-53c8-4dbb-b25f-795aaca78a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PERC = 0.64\n",
    "VALIDATION_PERC = 0.19\n",
    "TEST_PERC = 0.17\n",
    "\n",
    "KEEP_TOP_N = 100\n",
    "\n",
    "assert(sum([TRAIN_PERC, VALIDATION_PERC, TEST_PERC]) == 1)\n",
    "\n",
    "location_main = os.path.join(\"features\", os.environ[\"EXT_DATA_TYPE_FOLDER\"])\n",
    "# shutil.rmtree(location_main, ignore_errors=True)\n",
    "\n",
    "location_communities_leiden = f\"{location_main}{os.sep}communities_leiden.parquet\"\n",
    "\n",
    "location_features_leiden = f\"{location_main}{os.sep}features_leiden.parquet\"\n",
    "location_features_ego = f\"{location_main}{os.sep}features_ego.parquet\"\n",
    "location_features_2_hop = f\"{location_main}{os.sep}features_2_hop.parquet\"\n",
    "location_features_2_hop_out = f\"{location_main}{os.sep}features_2_hop_out.parquet\"\n",
    "location_features_2_hop_in = f\"{location_main}{os.sep}features_2_hop_in.parquet\"\n",
    "location_features_2_hop_combined = f\"{location_main}{os.sep}features_2_hop_combined.parquet\"\n",
    "location_features_source = f\"{location_main}{os.sep}features_source.parquet\"\n",
    "location_features_target = f\"{location_main}{os.sep}features_target.parquet\"\n",
    "\n",
    "location_flow_dispense = f\"{location_main}{os.sep}location_flow_dispense.parquet\"\n",
    "location_flow_passthrough = f\"{location_main}{os.sep}location_flow_passthrough.parquet\"\n",
    "location_flow_sink = f\"{location_main}{os.sep}location_flow_sink.parquet\"\n",
    "\n",
    "location_comm_as_source_features = f\"{location_main}{os.sep}comm_as_source_features.parquet\"\n",
    "location_comm_as_target_features = f\"{location_main}{os.sep}comm_as_target_features.parquet\"\n",
    "location_comm_as_passthrough_features = f\"{location_main}{os.sep}comm_as_passthrough_features.parquet\"\n",
    "location_comm_as_passthrough_features_reverse = f\"{location_main}{os.sep}comm_as_passthrough_features_reverse.parquet\"\n",
    "\n",
    "location_features_node_level = f\"{location_main}{os.sep}features_node_level.parquet\"\n",
    "location_features_edges = f\"{location_main}{os.sep}features_edges.parquet\"\n",
    "\n",
    "location_features_edges_train = f\"{location_main}{os.sep}features_edges_train.parquet\"\n",
    "location_features_edges_valid = f\"{location_main}{os.sep}features_edges_valid.parquet\"\n",
    "location_features_edges_test = f\"{location_main}{os.sep}features_edges_test.parquet\"\n",
    "\n",
    "location_train_trx_features = f\"{location_main}{os.sep}train_trx_features.parquet\"\n",
    "location_valid_trx_features = f\"{location_main}{os.sep}valid_trx_features.parquet\"\n",
    "location_test_trx_features = f\"{location_main}{os.sep}test_trx_features.parquet\"\n",
    "\n",
    "location_train_features = f\"{location_main}{os.sep}train_features.parquet\"\n",
    "location_valid_features = f\"{location_main}{os.sep}valid_features.parquet\"\n",
    "location_test_features = f\"{location_main}{os.sep}test_features.parquet\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(location_main)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82546745-6577-4e38-bf44-1a6114c3a398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "data = spark.read.parquet(s.STAGED_DATA_LOCATION)\n",
    "data = data.withColumn(\"is_laundering\", sf.col(\"is_laundering\").cast(\"boolean\"))\n",
    "data_count_original = data.count()\n",
    "\n",
    "# Probably not used in the benchmarks\n",
    "data = data.drop(\"source_entity\", \"target_entity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30fc12ed-f4c5-4f29-90e8-cfa20886c41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.drop(\"source\", \"target\")\n",
    "# data = data.withColumnRenamed(\"source_entity\", \"source\")\n",
    "# data = data.withColumnRenamed(\"target_entity\", \"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57b5d7e4-1b66-4e93-b67b-a947d8c8e06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:===============================================>     (181 + 12) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6914124 4425039 1313683 1175402\n",
      "\n",
      "CPU times: user 165 ms, sys: 118 ms, total: 283 ms\n",
      "Wall time: 37.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trx_ids_sorted = data.sort(\"timestamp\").select(\"transaction_id\").toPandas()[\"transaction_id\"].values\n",
    "trx_count = len(trx_ids_sorted)\n",
    "\n",
    "last_train_index = int(np.floor(trx_count * TRAIN_PERC))\n",
    "last_validation_index = last_train_index + int(np.floor(trx_count * VALIDATION_PERC))\n",
    "train_indexes = trx_ids_sorted[:last_train_index]\n",
    "validation_indexes = trx_ids_sorted[last_train_index:last_validation_index]\n",
    "test_indexes = trx_ids_sorted[last_validation_index:]\n",
    "\n",
    "train_indexes_loc = os.path.join(location_main, \"temp_train_indexes.parquet\")\n",
    "validation_indexes_loc = os.path.join(location_main, \"temp_validation_indexes.parquet\")\n",
    "test_indexes_loc = os.path.join(location_main, \"temp_test_indexes.parquet\")\n",
    "\n",
    "pd.DataFrame(train_indexes, columns=[\"transaction_id\"]).to_parquet(train_indexes_loc)\n",
    "pd.DataFrame(validation_indexes, columns=[\"transaction_id\"]).to_parquet(validation_indexes_loc)\n",
    "pd.DataFrame(test_indexes, columns=[\"transaction_id\"]).to_parquet(test_indexes_loc)\n",
    "\n",
    "train_indexes = spark.read.parquet(train_indexes_loc)\n",
    "validation_indexes = spark.read.parquet(validation_indexes_loc)\n",
    "test_indexes = spark.read.parquet(test_indexes_loc)\n",
    "\n",
    "train = train_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "validation = validation_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "test = test_indexes.join(\n",
    "    data, on=\"transaction_id\", how=\"left\"\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "train_count, validation_count, test_count = train.count(), validation.count(), test.count()\n",
    "print()\n",
    "print(trx_count, train_count, validation_count, test_count)\n",
    "print()\n",
    "\n",
    "os.remove(train_indexes_loc)\n",
    "os.remove(validation_indexes_loc)\n",
    "os.remove(test_indexes_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2d1078e-cc6f-4574-a729-28d81fb4fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_edge_features(input_data):\n",
    "    print(f\"Generating edge features\")\n",
    "    to_select = [\"source\", \"target\", \"format\", \"source_currency\", \"source_amount\", \"amount\", \"timestamp\"]\n",
    "    edges_features_input = input_data.select(*to_select).groupby(\n",
    "        [\"source\", \"target\", \"format\", \"source_currency\"]\n",
    "    ).agg(\n",
    "        sf.sum(\"source_amount\").alias(\"source_amount\"), \n",
    "        sf.sum(\"amount\").alias(\"amount\"),\n",
    "        sf.unix_timestamp(sf.min(\"timestamp\")).alias(\"min_ts\"),\n",
    "        sf.unix_timestamp(sf.max(\"timestamp\")).alias(\"max_ts\"),\n",
    "    ).repartition(os.cpu_count() * 2, \"source\", \"target\").persist(StorageLevel.DISK_ONLY)\n",
    "    _ = edges_features_input.count()\n",
    "    edge_features = edges_features_input.groupby([\"source\", \"target\"]).applyInPandas(\n",
    "        get_edge_features_udf, schema=SCHEMA_FEAT_UDF\n",
    "    ).toPandas()\n",
    "    edge_features = pd.DataFrame(edge_features[\"features\"].apply(json.loads).tolist())\n",
    "    edge_features.to_parquet(location_features_edges)\n",
    "    del edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce68e646-67f2-4158-ba72-528c94207d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_node_features_to_edges(features_in, location):\n",
    "    features_in = features_in.set_index(\"target\").join(\n",
    "        pd.read_parquet(location_features_node_level), how=\"left\", rsuffix=\"_target\"\n",
    "    ).reset_index().set_index(\"source\").join(\n",
    "        pd.read_parquet(location_features_node_level), how=\"left\", rsuffix=\"_source\"\n",
    "    ).reset_index()\n",
    "\n",
    "    features_in.loc[:, \"anomaly_scores_diff\"] = features_in.loc[:, \"anomaly_score\"] - features_in.loc[:, \"anomaly_score_source\"]\n",
    "    features_in.loc[:, \"anomaly_scores_min\"] = np.array(\n",
    "        [\n",
    "            features_in.loc[:, \"anomaly_score\"].values, \n",
    "            features_in.loc[:, \"anomaly_score_source\"].values\n",
    "        ],\n",
    "    ).min(axis=0)\n",
    "    features_in.loc[:, \"anomaly_scores_max\"] = np.array(\n",
    "        [\n",
    "            features_in.loc[:, \"anomaly_score\"].values, \n",
    "            features_in.loc[:, \"anomaly_score_source\"].values\n",
    "        ],\n",
    "    ).max(axis=0)\n",
    "    features_in.loc[:, \"anomaly_scores_mean\"] = np.array(\n",
    "        [\n",
    "            features_in.loc[:, \"anomaly_score\"].values, \n",
    "            features_in.loc[:, \"anomaly_score_source\"].values\n",
    "        ],\n",
    "    ).mean(axis=0)\n",
    "\n",
    "    features_in.to_parquet(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f4364bc-eb49-4d03-b85b-8ff293a88fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trx_features(data_in, location):\n",
    "    columns = [\n",
    "        \"source\", \"target\", \"source_currency\", \"target_currency\", \"format\", \"amount\", \n",
    "        \"source_dispensation\",\n",
    "        \"target_accumulation\",\n",
    "        \"source_positive_balance\",\n",
    "        \"source_negative_balance\",\n",
    "        \"target_positive_balance\",\n",
    "        \"target_negative_balance\",\n",
    "        \"source_active_for\",\n",
    "        \"target_active_for\",\n",
    "        \"is_laundering\"\n",
    "    ]\n",
    "    trx_features = data_in.select(*columns).toPandas()\n",
    "    trx_features.loc[:, \"source_balance_ratio\"] = (\n",
    "        trx_features[\"source_positive_balance\"] / trx_features[\"source_negative_balance\"]\n",
    "    ).fillna(0).replace(np.inf, 0)\n",
    "    trx_features.loc[:, \"target_balance_ratio\"] = (\n",
    "        trx_features[\"target_positive_balance\"] / trx_features[\"target_negative_balance\"]\n",
    "    ).fillna(0).replace(np.inf, 0)\n",
    "    trx_features.loc[:, \"inter_currency\"] = trx_features[\"source_currency\"] != trx_features[\"target_currency\"]\n",
    "    trx_features = pd.get_dummies(trx_features, columns=[\"source_currency\", \"target_currency\", \"format\"], drop_first=False)\n",
    "    trx_features.to_parquet(location)\n",
    "    del trx_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c145f2-5598-4551-b0c1-497cb1f1ae41",
   "metadata": {},
   "source": [
    "# [To prevent data leakage]\n",
    "\n",
    "### As the `train`, `validation`, and `test` sets are split in chronological order:\n",
    "* `train` features are constructed, based on a **graph** (containing data), up till the last training record\n",
    "* `validation` features are constructed, ..., up till the last validation record\n",
    "* `train` features are constructed, ..., up till the last test record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19d4ecbd-1487-4b07-b7c6-1fb22e3bf4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing node-level features for `train` data: 4,425,039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.4 s, sys: 229 ms, total: 8.63 s\n",
      "Wall time: 38.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOWS ET: 169\n",
      "\n",
      "\n",
      "CPU times: user 7.3 s, sys: 2.14 s, total: 9.44 s\n",
      "Wall time: 2min 59s\n",
      "Constructing Leiden communities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 16s, sys: 43.2 s, total: 22min 59s\n",
      "Wall time: 25min 4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 700624\n",
      "250000 700624\n",
      "500000 700624\n",
      "CPU times: user 31.2 s, sys: 759 ms, total: 32 s\n",
      "Wall time: 1min 17s\n",
      "Constructing 2-hop communities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.22 s, sys: 651 ms, total: 6.87 s\n",
      "Wall time: 1min 25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1285259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.28 s, sys: 1.31 s, total: 10.6 s\n",
      "Wall time: 53.2 s\n",
      "Leiden communitites features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/28 19:52:36 WARN TaskSetManager: Stage 843 contains a task of very large size (1647 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.6 s, sys: 8.12 s, total: 31.7 s\n",
      "Wall time: 8min 3s\n",
      "2-hop communitites features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/28 20:02:50 WARN TaskSetManager: Stage 854 contains a task of very large size (1647 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.8 s, sys: 24.2 s, total: 1min 1s\n",
      "Wall time: 25min 20s\n",
      "Temporal flows features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/28 20:27:08 WARN TaskSetManager: Stage 873 contains a task of very large size (5947 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/10/28 20:27:39 WARN TaskSetManager: Stage 893 contains a task of very large size (5729 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4420818 4073341\n",
      "dispense\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passthrough\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sink\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.28 s, sys: 1.67 s, total: 6.95 s\n",
      "Wall time: 11min 16s\n",
      "1-hop-source features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/28 20:36:18 WARN TaskSetManager: Stage 1138 contains a task of very large size (1647 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.1 s, sys: 4.18 s, total: 23.3 s\n",
      "Wall time: 12min 29s\n",
      "1-hop-target features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/28 20:48:49 WARN TaskSetManager: Stage 1141 contains a task of very large size (1647 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.1 s, sys: 4.98 s, total: 26.1 s\n",
      "Wall time: 12min 42s\n",
      "Features: (700624, 388)\n",
      "Deleted 14 constant columns\n",
      "Training the anomaly detection model\n",
      "CPU times: user 14.6 s, sys: 3.09 s, total: 17.7 s\n",
      "Wall time: 18.5 s\n",
      "Generating edge features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26min 25s, sys: 2min 18s, total: 28min 44s\n",
      "Wall time: 1h 53min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data = train.select(\"*\")\n",
    "print(f\"Constructing node-level features for `train` data: {data.count():,}\")\n",
    "\n",
    "%run node_level_features.ipynb\n",
    "\n",
    "generate_edge_features(data)\n",
    "train_edges = train.select(\"source\", \"target\").drop_duplicates().toPandas().set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "train_features = train_edges.join(\n",
    "    pd.read_parquet(location_features_edges).set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "add_node_features_to_edges(train_features, location_features_edges_train)\n",
    "save_trx_features(train, location_train_trx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de5e1408-4d7a-4547-baaa-0cac6952106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing node-level features for `validation` data: 5,738,722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 s, sys: 508 ms, total: 10.8 s\n",
      "Wall time: 1min 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOWS ET: 422\n",
      "\n",
      "\n",
      "CPU times: user 12.1 s, sys: 5.36 s, total: 17.4 s\n",
      "Wall time: 7min 14s\n",
      "Constructing Leiden communities\n",
      "CPU times: user 22min 43s, sys: 52.7 s, total: 23min 36s\n",
      "Wall time: 25min 54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 702264\n",
      "250000 702264\n",
      "500000 702264\n",
      "CPU times: user 32.2 s, sys: 2.2 s, total: 34.3 s\n",
      "Wall time: 3min 12s\n",
      "Constructing 2-hop communities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.54 s, sys: 1.44 s, total: 8.98 s\n",
      "Wall time: 1min 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1336641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 s, sys: 6.52 s, total: 21.9 s\n",
      "Wall time: 1min 43s\n",
      "Leiden communitites features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/28 21:54:37 WARN TaskSetManager: Stage 2004 contains a task of very large size (1836 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.3 s, sys: 17.6 s, total: 46.9 s\n",
      "Wall time: 8min 29s\n",
      "2-hop communitites features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/28 22:05:42 WARN TaskSetManager: Stage 2015 contains a task of very large size (1836 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.5 s, sys: 11.5 s, total: 49 s\n",
      "Wall time: 26min 16s\n",
      "Temporal flows features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/28 22:28:46 WARN TaskSetManager: Stage 2040 contains a task of very large size (6181 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/10/28 22:29:48 WARN TaskSetManager: Stage 2068 contains a task of very large size (5967 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5733476 5257730\n",
      "dispense\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passthrough\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sink\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.99 s, sys: 2.44 s, total: 7.43 s\n",
      "Wall time: 10min 45s\n",
      "1-hop-source features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/28 22:38:43 WARN TaskSetManager: Stage 2367 contains a task of very large size (1836 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.5 s, sys: 7.77 s, total: 33.3 s\n",
      "Wall time: 14min 15s\n",
      "1-hop-target features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/28 22:53:25 WARN TaskSetManager: Stage 2370 contains a task of very large size (1836 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.9 s, sys: 15.3 s, total: 43.2 s\n",
      "Wall time: 16min 17s\n",
      "Features: (702264, 388)\n",
      "Deleted 14 constant columns\n",
      "Training the anomaly detection model\n",
      "CPU times: user 14.9 s, sys: 4.63 s, total: 19.5 s\n",
      "Wall time: 21.6 s\n",
      "Generating edge features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27min 11s, sys: 2min 44s, total: 29min 56s\n",
      "Wall time: 2h 9min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data = train.union(validation).select(\"*\")\n",
    "print(f\"Constructing node-level features for `validation` data: {data.count():,}\")\n",
    "\n",
    "%run node_level_features.ipynb\n",
    "\n",
    "generate_edge_features(data)\n",
    "validation_edges = validation.select(\"source\", \"target\").drop_duplicates().toPandas().set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "validation_features = validation_edges.join(\n",
    "    pd.read_parquet(location_features_edges).set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "add_node_features_to_edges(validation_features, location_features_edges_valid)\n",
    "save_trx_features(validation, location_valid_trx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ec78fa4-1bb2-4233-afe9-e7e445302c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing node-level features for `test` data: 6,914,124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 s, sys: 609 ms, total: 11.5 s\n",
      "Wall time: 1min 42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOWS ET: 270\n",
      "\n",
      "\n",
      "CPU times: user 13.9 s, sys: 7.08 s, total: 21 s\n",
      "Wall time: 4min 48s\n",
      "Constructing Leiden communities\n",
      "CPU times: user 23min 35s, sys: 1min 4s, total: 24min 39s\n",
      "Wall time: 27min 15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 703589\n",
      "250000 703589\n",
      "500000 703589\n",
      "CPU times: user 44.3 s, sys: 3.66 s, total: 47.9 s\n",
      "Wall time: 4min 7s\n",
      "Constructing 2-hop communities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.8 s, sys: 9.91 s, total: 24.8 s\n",
      "Wall time: 1min 54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1384576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 s, sys: 2.54 s, total: 13.4 s\n",
      "Wall time: 4min 9s\n",
      "Leiden communitites features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/29 00:07:34 WARN TaskSetManager: Stage 3271 contains a task of very large size (2001 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.1 s, sys: 11.1 s, total: 40.2 s\n",
      "Wall time: 9min 17s\n",
      "2-hop communitites features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/29 00:19:46 WARN TaskSetManager: Stage 3282 contains a task of very large size (2001 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.1 s, sys: 32.3 s, total: 1min 20s\n",
      "Wall time: 32min 44s\n",
      "Temporal flows features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/29 00:53:36 WARN TaskSetManager: Stage 3313 contains a task of very large size (6399 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/10/29 00:54:59 WARN TaskSetManager: Stage 3349 contains a task of very large size (6190 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6906470 6329177\n",
      "dispense\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passthrough\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sink\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.05 s, sys: 3.26 s, total: 10.3 s\n",
      "Wall time: 17min 2s\n",
      "1-hop-source features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/29 01:05:22 WARN TaskSetManager: Stage 3702 contains a task of very large size (2001 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.8 s, sys: 20.2 s, total: 50 s\n",
      "Wall time: 17min 49s\n",
      "1-hop-target features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/29 01:23:26 WARN TaskSetManager: Stage 3705 contains a task of very large size (2001 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.5 s, sys: 10.5 s, total: 42 s\n",
      "Wall time: 13min 57s\n",
      "Features: (703589, 388)\n",
      "Deleted 14 constant columns\n",
      "Training the anomaly detection model\n",
      "CPU times: user 14.7 s, sys: 2.9 s, total: 17.6 s\n",
      "Wall time: 18.5 s\n",
      "Generating edge features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28min 36s, sys: 3min 12s, total: 31min 48s\n",
      "Wall time: 2h 27min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data = train.union(validation).union(test).select(\"*\")\n",
    "print(f\"Constructing node-level features for `test` data: {data.count():,}\")\n",
    "\n",
    "%run node_level_features.ipynb\n",
    "\n",
    "generate_edge_features(data)\n",
    "test_edges = test.select(\"source\", \"target\").drop_duplicates().toPandas().set_index(\n",
    "    [\"source\", \"target\"]\n",
    ")\n",
    "test_features = test_edges.join(\n",
    "    pd.read_parquet(location_features_edges).set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "add_node_features_to_edges(test_features, location_features_edges_test)\n",
    "save_trx_features(test, location_test_trx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e6eaaec-8af4-43d7-8127-a306b855f430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted `global` DataFrame: edges\n",
      "Deleted `global` DataFrame: weights\n",
      "Deleted `global` DataFrame: edges_agg\n",
      "Deleted `global` large object: nodes_source\n",
      "Deleted `global` large object: nodes_target\n",
      "Deleted `global` large object: nodes_passthrough\n",
      "Deleted `global` large object: totals_sent\n",
      "Deleted `global` large object: totals_received\n",
      "Deleted `global` large object: nodes_mapping\n",
      "Deleted `global` large object: communities_leiden\n",
      "Deleted `global` large object: sizes_leiden\n",
      "Deleted `global` DataFrame: data_agg_weights\n",
      "Deleted `global` large object: nodes_neighborhoods\n",
      "Deleted `global` large object: communities_2_hop\n",
      "Deleted `global` large object: sizes_2_hop\n",
      "Deleted `global` DataFrame: features_leiden\n",
      "Deleted `global` large object: communities_leiden_dict\n",
      "Deleted `global` DataFrame: features_2_hop\n",
      "Deleted `global` DataFrame: features_source\n",
      "Deleted `global` DataFrame: features_target\n",
      "Deleted `global` DataFrame: anomalies\n",
      "Deleted `global` DataFrame: train_edges\n",
      "Deleted `global` DataFrame: train_features\n",
      "Deleted `global` DataFrame: validation_edges\n",
      "Deleted `global` DataFrame: validation_features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To free up memory for training\n",
    "\n",
    "to_reset = %who_ls\n",
    "to_reset = list(to_reset)\n",
    "to_reset.remove(\"to_keep\")\n",
    "to_reset = set(to_reset) - set(to_keep)\n",
    "for var_to_reset in list(to_reset):\n",
    "    var_to_reset = f\"^{var_to_reset}$\"\n",
    "    %reset_selective -f {var_to_reset}\n",
    "\n",
    "delete_large_vars(globals(), locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13c3a17d-14f9-4d34-9103-0c7e2e8c064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(location_features_trx, location_features_edges, location_features):\n",
    "    features_input = spark.read.parquet(location_features_edges)\n",
    "    trx_features_input = spark.read.parquet(location_features_trx)\n",
    "    features_input = trx_features_input.join(\n",
    "        features_input,\n",
    "        on=[\"source\", \"target\"],\n",
    "        how=\"left\"\n",
    "    ).drop(\"source\", \"target\")\n",
    "    features_input.write.parquet(location_features, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96867e2d-ceed-4176-b2f2-cb407c592ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/29 01:49:04 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 82.7 ms, sys: 207 ms, total: 289 ms\n",
      "Wall time: 4min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "combine_features(location_train_trx_features, location_features_edges_train, location_train_features)\n",
    "combine_features(location_valid_trx_features, location_features_edges_valid, location_valid_features)\n",
    "combine_features(location_test_trx_features, location_features_edges_test, location_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8924af3-4f88-48dc-9e82-ab345e77f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(MULTI_PROC_STAGING_LOCATION, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79c2ca47-4a63-4e85-aa7f-ff6746a2618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_parquet(location_train_features)\n",
    "validation_features = pd.read_parquet(location_valid_features)\n",
    "test_features = pd.read_parquet(location_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d123edbe-93c9-4357-8bd3-1a341a95a8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting format_Reinvestment from train\n"
     ]
    }
   ],
   "source": [
    "all_columns = set(train_features.columns) | set(validation_features.columns) | set(test_features.columns)\n",
    "\n",
    "for missing in (\n",
    "    all_columns.symmetric_difference(train_features.columns) |\n",
    "    all_columns.symmetric_difference(validation_features.columns) |\n",
    "    all_columns.symmetric_difference(test_features.columns)\n",
    "):\n",
    "    if missing in train_features.columns:\n",
    "        print(f\"Deleting {missing} from train\")\n",
    "        del train_features[missing]\n",
    "    if missing in validation_features.columns:\n",
    "        print(f\"Deleting {missing} from validation\")\n",
    "        del validation_features[missing]\n",
    "    if missing in test_features.columns:\n",
    "        print(f\"Deleting {missing} from test\")\n",
    "        del test_features[missing]\n",
    "\n",
    "validation_features = validation_features.loc[:, list(train_features.columns)]\n",
    "test_features = test_features.loc[:, list(train_features.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a7f7de4-2df8-490f-8daa-b95980fd9686",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_features.shape[0] == train_count\n",
    "assert validation_features.shape[0] == validation_count\n",
    "assert test_features.shape[0] == test_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eefb7276-f84a-4468-bd82-a2fcce9132c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_labels = train_features.loc[:, [\"is_laundering\"]].copy(deep=True)\n",
    "del train_features[\"is_laundering\"]\n",
    "\n",
    "validation_features_labels = validation_features.loc[:, [\"is_laundering\"]].copy(deep=True)\n",
    "del validation_features[\"is_laundering\"]\n",
    "\n",
    "test_features_labels = test_features.loc[:, [\"is_laundering\"]].copy(deep=True)\n",
    "del test_features[\"is_laundering\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db2dbe51-59bf-4e8e-a132-c4c3121ae717",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_available = False\n",
    "try:\n",
    "    import torch\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "\n",
    "xgb_args = dict(\n",
    "    early_stopping_rounds=10, scale_pos_weight=5,\n",
    "    eval_metric=\"aucpr\", \n",
    "    disable_default_eval_metric=True, \n",
    "    num_parallel_tree=10, max_depth=6,\n",
    "    colsample_bytree=0.5, subsample=1, \n",
    "    eta=0.05,\n",
    "    device=\"cpu\", nthread=10,\n",
    "    n_estimators=500, seed=SEED,\n",
    ")\n",
    "if cuda_available:\n",
    "    xgb_args[\"device\"] = \"cuda\"\n",
    "    xgb_args[\"nthread\"] = 2\n",
    "\n",
    "xgb_fit_args = {\n",
    "    \"eval_set\": [(validation_features, validation_features_labels[\"is_laundering\"].values)],\n",
    "    \"verbose\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15622322-48f4-4051-8f49-d8ea9476feda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED=42 model.best_iteration=163 f1=45.49 recall=32.04\n",
      "45.48672566371682\n",
      "CPU times: user 5h 9min 53s, sys: 10min 47s, total: 5h 20min 41s\n",
      "Wall time: 36min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = xgb.XGBClassifier(**xgb_args)\n",
    "model.fit(\n",
    "    train_features, train_features_labels[\"is_laundering\"].values, \n",
    "    **xgb_fit_args\n",
    ")\n",
    "y_test_predicted = model.predict(test_features)\n",
    "f1_test = f1_score(test_features_labels[\"is_laundering\"], y_test_predicted) * 100\n",
    "print(\n",
    "    f\"{SEED=}\",\n",
    "    f\"{model.best_iteration=}\",\n",
    "    f\"f1={round(f1_test, 2)}\",\n",
    "    f\"recall={round(recall_score(test_features_labels['is_laundering'], y_test_predicted) * 100, 2)}\",\n",
    ")\n",
    "print(f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b14cc0-5e85-4857-b4b5-f2061e8c5ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting `xgb_model`\n",
      "[0]\tvalidation_0-aucpr:0.21038\n",
      "[1]\tvalidation_0-aucpr:0.21668\n",
      "[2]\tvalidation_0-aucpr:0.22498\n",
      "[3]\tvalidation_0-aucpr:0.23252\n",
      "[4]\tvalidation_0-aucpr:0.23682\n",
      "5 f1=1.98 recall=1.0\n",
      "\n",
      "[0]\tvalidation_0-aucpr:0.23873\n",
      "[1]\tvalidation_0-aucpr:0.23862\n",
      "[2]\tvalidation_0-aucpr:0.24144\n",
      "[3]\tvalidation_0-aucpr:0.24429\n",
      "[4]\tvalidation_0-aucpr:0.24760\n",
      "10 f1=26.0 recall=14.96\n",
      "\n",
      "[0]\tvalidation_0-aucpr:0.25057\n",
      "[1]\tvalidation_0-aucpr:0.25145\n",
      "[2]\tvalidation_0-aucpr:0.25289\n",
      "[3]\tvalidation_0-aucpr:0.25388\n",
      "[4]\tvalidation_0-aucpr:0.25477\n",
      "15 f1=36.35 recall=22.32\n",
      "\n",
      "[0]\tvalidation_0-aucpr:0.25587\n",
      "[1]\tvalidation_0-aucpr:0.25765\n",
      "[2]\tvalidation_0-aucpr:0.25844\n",
      "[3]\tvalidation_0-aucpr:0.26037\n",
      "[4]\tvalidation_0-aucpr:0.26146\n",
      "20 f1=40.2 recall=25.44\n",
      "\n",
      "[0]\tvalidation_0-aucpr:0.26280\n",
      "[1]\tvalidation_0-aucpr:0.26449\n",
      "[2]\tvalidation_0-aucpr:0.26591\n",
      "[3]\tvalidation_0-aucpr:0.26723\n",
      "[4]\tvalidation_0-aucpr:0.26902\n",
      "25 f1=42.13 recall=27.18\n",
      "\n",
      "[0]\tvalidation_0-aucpr:0.26998\n",
      "[1]\tvalidation_0-aucpr:0.27075\n",
      "[2]\tvalidation_0-aucpr:0.27090\n",
      "[3]\tvalidation_0-aucpr:0.27184\n",
      "[4]\tvalidation_0-aucpr:0.27274\n",
      "30 f1=42.87 recall=27.93\n",
      "\n",
      "[0]\tvalidation_0-aucpr:0.27342\n",
      "[1]\tvalidation_0-aucpr:0.27401\n",
      "[2]\tvalidation_0-aucpr:0.27488\n",
      "[3]\tvalidation_0-aucpr:0.27563\n",
      "[4]\tvalidation_0-aucpr:0.27618\n",
      "35 f1=43.79 recall=28.8\n",
      "\n",
      "[0]\tvalidation_0-aucpr:0.27670\n",
      "[1]\tvalidation_0-aucpr:0.27730\n",
      "[2]\tvalidation_0-aucpr:0.27802\n",
      "[3]\tvalidation_0-aucpr:0.27883\n",
      "[4]\tvalidation_0-aucpr:0.27907\n",
      "40 f1=43.73 recall=28.93\n",
      "\n",
      "[0]\tvalidation_0-aucpr:0.27979\n",
      "[1]\tvalidation_0-aucpr:0.28022\n",
      "[2]\tvalidation_0-aucpr:0.28115\n",
      "[3]\tvalidation_0-aucpr:0.28156\n",
      "[4]\tvalidation_0-aucpr:0.28283\n",
      "45 f1=44.49 recall=29.68\n",
      "\n",
      "[0]\tvalidation_0-aucpr:0.28380\n",
      "[1]\tvalidation_0-aucpr:0.28453\n",
      "[2]\tvalidation_0-aucpr:0.28520\n",
      "[3]\tvalidation_0-aucpr:0.28549\n",
      "[4]\tvalidation_0-aucpr:0.28631\n",
      "50 f1=45.23 recall=30.42\n",
      "\n",
      "[0]\tvalidation_0-aucpr:0.28671\n",
      "[1]\tvalidation_0-aucpr:0.28721\n",
      "[2]\tvalidation_0-aucpr:0.28759\n",
      "[3]\tvalidation_0-aucpr:0.28817\n",
      "[4]\tvalidation_0-aucpr:0.28888\n",
      "55 f1=45.57 recall=30.8\n",
      "\n",
      "[0]\tvalidation_0-aucpr:0.28914\n",
      "[1]\tvalidation_0-aucpr:0.28996\n",
      "[2]\tvalidation_0-aucpr:0.29033\n",
      "[3]\tvalidation_0-aucpr:0.29074\n",
      "[4]\tvalidation_0-aucpr:0.29139\n",
      "60 f1=45.28 recall=30.8\n",
      "\n",
      "[0]\tvalidation_0-aucpr:0.29204\n",
      "[1]\tvalidation_0-aucpr:0.29235\n",
      "[2]\tvalidation_0-aucpr:0.29261\n",
      "[3]\tvalidation_0-aucpr:0.29285\n",
      "[4]\tvalidation_0-aucpr:0.29297\n",
      "65 f1=45.4 recall=31.05\n",
      "\n",
      "[0]\tvalidation_0-aucpr:0.29311\n",
      "[1]\tvalidation_0-aucpr:0.29335\n",
      "[2]\tvalidation_0-aucpr:0.29338\n",
      "[3]\tvalidation_0-aucpr:0.29351\n",
      "[4]\tvalidation_0-aucpr:0.29356\n",
      "70 f1=45.45 recall=31.17\n",
      "\n",
      "[0]\tvalidation_0-aucpr:0.29386\n",
      "[1]\tvalidation_0-aucpr:0.29395\n",
      "[2]\tvalidation_0-aucpr:0.29399\n",
      "[3]\tvalidation_0-aucpr:0.29423\n",
      "[4]\tvalidation_0-aucpr:0.29440\n",
      "75 f1=45.36 recall=31.05\n",
      "\n",
      "[0]\tvalidation_0-aucpr:0.29447\n",
      "[1]\tvalidation_0-aucpr:0.29453\n",
      "[2]\tvalidation_0-aucpr:0.29452\n",
      "[3]\tvalidation_0-aucpr:0.29452\n",
      "[4]\tvalidation_0-aucpr:0.29442\n",
      "80 f1=45.11 recall=30.8\n",
      "\n",
      "[0]\tvalidation_0-aucpr:0.29448\n",
      "[1]\tvalidation_0-aucpr:0.29441\n",
      "[2]\tvalidation_0-aucpr:0.29449\n",
      "[3]\tvalidation_0-aucpr:0.29462\n",
      "[4]\tvalidation_0-aucpr:0.29464\n",
      "85 f1=45.01 recall=30.67\n",
      "\n",
      "[0]\tvalidation_0-aucpr:0.29461\n",
      "[1]\tvalidation_0-aucpr:0.29463\n",
      "[2]\tvalidation_0-aucpr:0.29455\n",
      "[3]\tvalidation_0-aucpr:0.29466\n",
      "[4]\tvalidation_0-aucpr:0.29474\n",
      "90 f1=44.89 recall=30.67\n",
      "\n",
      "[0]\tvalidation_0-aucpr:0.29480\n",
      "[1]\tvalidation_0-aucpr:0.29474\n",
      "[2]\tvalidation_0-aucpr:0.29487\n",
      "[3]\tvalidation_0-aucpr:0.29491\n",
      "[4]\tvalidation_0-aucpr:0.29472\n",
      "95 f1=44.97 recall=30.67\n",
      "\n",
      "[0]\tvalidation_0-aucpr:0.29471\n",
      "[1]\tvalidation_0-aucpr:0.29488\n",
      "[2]\tvalidation_0-aucpr:0.29500\n",
      "[3]\tvalidation_0-aucpr:0.29502\n",
      "[4]\tvalidation_0-aucpr:0.29527\n",
      "100 f1=44.81 recall=30.42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_args = dict(\n",
    "    scale_pos_weight=3,\n",
    "    eval_metric=\"aucpr\", \n",
    "    disable_default_eval_metric=True, \n",
    "    num_parallel_tree=10, max_depth=6,\n",
    "    colsample_bytree=0.5, subsample=1, \n",
    "    eta=0.05,\n",
    "    device=\"cpu\", nthread=10,\n",
    "    n_estimators=5, seed=SEED,\n",
    ")\n",
    "model = xgb.XGBClassifier(**xgb_args)\n",
    "if \"xgb_model\" in xgb_fit_args:\n",
    "    del xgb_fit_args[\"xgb_model\"]\n",
    "    print(\"Deleting `xgb_model`\")\n",
    "for i in range(200):\n",
    "    model = model.fit(\n",
    "        train_features, train_features_labels[\"is_laundering\"].values, \n",
    "        **xgb_fit_args\n",
    "    )\n",
    "    xgb_fit_args[\"xgb_model\"] = model\n",
    "    y_test_predicted = model.predict(test_features)\n",
    "    f1_test = f1_score(test_features_labels[\"is_laundering\"], y_test_predicted) * 100\n",
    "    print(\n",
    "        (i + 1) * xgb_args[\"n_estimators\"],\n",
    "        f\"f1={round(f1_test, 2)}\",\n",
    "        f\"recall={round(recall_score(test_features_labels['is_laundering'], y_test_predicted) * 100, 2)}\",\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ba047-ef87-4c8c-b9fe-cd3ff3ebb6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 f1=45.7 recall=31.17\n",
    "# 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
